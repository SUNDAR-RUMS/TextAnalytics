Text,Category
"
",Cricket
"Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 20-metre (22-yard) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The batting side scores runs by striking the ball bowled at the wicket with the bat, while the bowling and fielding side tries to prevent this and dismiss each player (so they are ""out""). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side catching the ball after it is hit by the bat, but before it hits the ground. When ten players have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches. They communicate with two off-field scorers who record the match's statistical information.
",Cricket
"There are various formats ranging from Twenty20, played over a few hours with each team batting for a single innings of 20 overs, to Test matches, played over five days with unlimited overs and the teams each batting for two innings of unlimited length. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid spheroid made of compressed leather with a slightly raised sewn seam enclosing a cork core which is layered with tightly wound string.
",Cricket
"Historically, cricket's origins are uncertain and the earliest definite reference is in south-east England in the middle of the 16th century. It spread globally with the expansion of the British Empire, leading to the first international matches in the second half of the 19th century. The game's governing body is the International Cricket Council (ICC), which has over 100 members, twelve of which are full members who play Test matches. The game's rules are held in a code called the Laws of Cricket which is owned and maintained by Marylebone Cricket Club (MCC) in London. The sport is followed primarily in the Indian subcontinent, Australasia, the United Kingdom, Ireland, southern Africa and the West Indies, its globalisation occurring during the expansion of the British Empire and remaining popular into the 21st century.[1] Women's cricket, which is organised and played separately, has also achieved international standard. The most successful side playing international cricket is Australia, having won seven One Day International trophies, including five World Cups, more than any other country, and having been the top-rated Test side more than any other country.
",Cricket
"
",Cricket
"Cricket is one of many games in the ""club ball"" sphere that basically involve hitting a ball with a hand-held implement; others are baseball, golf, hockey, tennis, squash, and table tennis.[2] In cricket's case, a key difference is the existence of a solid target structure, the wicket (originally, it is thought, a ""wicket gate"" through which sheep were herded), that the batsman must defend.[3] The cricket historian Harry Altham identified three ""groups"" of ""club ball"" games: the ""hockey group"", in which the ball is driven to and fro between two targets (the goals); the ""golf group"", in which the ball is driven towards an undefended target (the hole); and the ""cricket group"", in which ""the ball is aimed at a mark (the wicket) and driven away from it"".[4]
",Cricket
"It is generally believed that cricket originated as a children's game in the south-eastern counties of England, sometime during the medieval period.[3] Although there are claims for prior dates, the earliest definite reference to cricket being played comes from evidence given at a court case in Guildford on Monday, 17 January 1597 (Julian calendar; equating to 30 January 1598 in the Gregorian calendar). The case concerned ownership of a certain plot of land and the court heard the testimony of a 59-year-old coroner, John Derrick, who gave witness that:[5][6][7]
",Cricket
"""Being a scholler in the ffree schoole of Guldeford hee and diverse of his fellows did runne and play there at creckett and other plaies"".",Cricket
"Given Derrick's age, it was about half a century earlier when he was at school and so it is certain that cricket was being played c. 1550 by boys in Surrey.[7] The view that it was originally a children's game is reinforced by Randle Cotgrave's 1611 English-French dictionary in which he defined the noun ""crosse"" as ""the crooked staff wherewith boys play at cricket"" and the verb form ""crosser"" as ""to play at cricket"".[8][9]
",Cricket
"One possible source for the sport's name is the Old English word ""cryce"" (or ""cricc"") meaning a crutch or staff. In Samuel Johnson's Dictionary, he derived cricket from ""cryce, Saxon, a stick"".[5] In Old French, the word ""criquet"" seems to have meant a kind of club or stick.[10] Given the strong medieval trade connections between south-east England and the County of Flanders when the latter belonged to the Duchy of Burgundy, the name may have been derived from the Middle Dutch (in use in Flanders at the time) ""krick""(-e), meaning a stick (crook).[10] Another possible source is the Middle Dutch word ""krickstoel"", meaning a long low stool used for kneeling in church and which resembled the long low wicket with two stumps used in early cricket.[11] According to Heiner Gillmeister, a European language expert of Bonn University, ""cricket"" derives from the Middle Dutch phrase for hockey, met de (krik ket)sen (i.e., ""with the stick chase"").[12] Gillmeister has suggested that not only the name but also the sport itself may be of Flemish origin.[12]
",Cricket
"Although the main object of the game has always been to score the most runs, the early form of cricket differed from the modern game in certain key technical aspects. The ball was bowled underarm by the bowler and all along the ground towards a batsman armed with a bat that, in shape, resembled a hockey stick; the batsman defended a low, two-stump wicket; and runs were called ""notches"" because the scorers recorded them by notching tally sticks.[13][14][15]
",Cricket
"In 1611, the year Cotgrave's dictionary was published, ecclesiastical court records at Sidlesham in Sussex state that two parishioners, Bartholomew Wyatt and Richard Latter, failed to attend church on Easter Sunday because they were playing cricket. They were fined 12d each and ordered to do penance.[16] This is the earliest mention of adult participation in cricket and it was around the same time that the earliest known organised inter-parish or village match was played – at Chevening, Kent.[5][17] In 1624, a player called Jasper Vinall died after he was accidentally struck on the head during a match between two parish teams in Sussex.[18]
",Cricket
"Cricket remained a low-key local pursuit for much of the century.[9] It is known, through numerous references found in the records of ecclesiastical court cases, to have been proscribed at times by the Puritans before and during the Commonwealth.[19][20] The problem was nearly always the issue of Sunday play as the Puritans considered cricket to be ""profane"" if played on the Sabbath, especially if large crowds and/or gambling were involved.[21][22]
",Cricket
"According to the social historian Derek Birley, there was a ""great upsurge of sport after the Restoration"" in 1660.[23] Gambling on sport became a problem significant enough for Parliament to pass the 1664 Gambling Act, limiting stakes to £100 which was in any case a colossal sum exceeding the annual income of 99% of the population.[23] Along with prizefighting, horse racing and blood sports, cricket was perceived to be a gambling sport.[24] Rich patrons made matches for high stakes, forming teams in which they engaged the first professional players.[25] By the end of the century, cricket had developed into a major sport which was spreading throughout England and was already being taken abroad by English mariners and colonisers – the earliest reference to cricket overseas is dated 1676.[26] A 1697 newspaper report survives of ""a great cricket match"" played in Sussex ""for fifty guineas apiece"" – this is the earliest known match that is generally considered top-class.[27][28]
",Cricket
"The patrons, and other players from the social class known as the ""gentry"", began to classify themselves as ""amateurs""[fn 1] to establish a clear distinction vis-à-vis the professionals, who were invariably members of the working class, even to the point of having separate changing and dining facilities.[29] The gentry, including such high-ranking nobles as the Dukes of Richmond, exerted their honour code of noblesse oblige to claim rights of leadership in any sporting contests they took part in, especially as it was necessary for them to play alongside their ""social inferiors"" if they were to win their bets.[30] In time, a perception took hold that the typical amateur who played in first-class cricket, until 1962 when amateurism was abolished, was someone with a public school education who had then gone to one of Cambridge or Oxford University – society insisted that such people were ""officers and gentlemen"" whose destiny was to provide leadership.[31] In a purely financial sense, the cricketing amateur would theoretically claim expenses for playing while his professional counterpart played under contract and was paid a wage or match fee; in practice, many amateurs claimed somewhat more than actual expenditure and the derisive term ""shamateur"" was coined to describe the syndrome.[32][33]
",Cricket
"The game underwent major development in the 18th century to become England's national sport.[citation needed] Its success was underwritten by the twin necessities of patronage and betting.[34] Cricket was prominent in London as early as 1707 and, in the middle years of the century, large crowds flocked to matches on the Artillery Ground in Finsbury.[citation needed] The single wicket form of the sport attracted huge crowds and wagers to match, its popularity peaking in the 1748 season.[35] Bowling underwent an evolution around 1760 when bowlers began to pitch the ball instead of rolling or skimming it towards the batsman. This caused a revolution in bat design because, to deal with the bouncing ball, it was necessary to introduce the modern straight bat in place of the old ""hockey stick"" shape.[36][citation needed]
",Cricket
"The Hambledon Club was founded in the 1760s and, for the next twenty years until the formation of Marylebone Cricket Club (MCC) and the opening of Lord's Old Ground in 1787, Hambledon was both the game's greatest club and its focal point.[citation needed] MCC quickly became the sport's premier club and the custodian of the Laws of Cricket. New Laws introduced in the latter part of the 18th century included the three stump wicket and leg before wicket (lbw).[37]
",Cricket
"The 19th century saw underarm bowling superseded by first roundarm and then overarm bowling. Both developments were controversial.[38] Organisation of the game at county level led to the creation of the county clubs, starting with Sussex in 1839.[39] In December 1889, the eight leading county clubs formed the official County Championship, which began in 1890.[40]
",Cricket
"The most famous player of the 19th century was W. G. Grace, who started his long and influential career in 1865. It was especially during the career of Grace that the distinction between amateurs and professionals became blurred by the existence of players like him who were nominally amateur but, in terms of their financial gain, de facto professional. Grace himself was said to have been paid more money for playing cricket than any professional.[citation needed]
",Cricket
"The last two decades before the First World War have been called the ""Golden Age of cricket"". It is a nostalgic name prompted by the collective sense of loss resulting from the war, but the period did produce some great players and memorable matches, especially as organised competition at county and Test level developed.[41]
",Cricket
"Meanwhile, the British Empire had been instrumental in spreading the game overseas and by the middle of the 19th century it had become well established in Australia, the Caribbean, India, New Zealand, North America and South Africa.[42] In 1844, the first-ever international match took place between the United States and Canada.[43] In 1859, a team of English players went to North America on the first overseas tour.[44]
",Cricket
"In 1862, an English team made the first tour of Australia.[45] The first Australian team to travel overseas consisted of Aboriginal stockmen who toured England in 1868.[46]
",Cricket
"In 1876–77, an England team took part in what was retrospectively recognised as the first-ever Test match at the Melbourne Cricket Ground against Australia.[47] The rivalry between England and Australia gave birth to The Ashes in 1882 and this has remained Test cricket's most famous contest.[48] Test cricket began to expand in 1888–89 when South Africa played England.[citation needed]
",Cricket
"The inter-war years were dominated by Australia's Don Bradman, statistically the greatest Test batsman of all time. Test cricket continued to expand during the 20th century with the addition of the West Indies (1928), New Zealand (1930) and India (1932) before the Second World War and then Pakistan (1952), Sri Lanka (1982), Zimbabwe (1992) and Bangladesh (2000) in the post-war period.[49][50] South Africa was banned from international cricket from 1970 to 1992 as part of the apartheid boycott.[51]
",Cricket
"Cricket entered a new era in 1963 when English counties introduced the limited overs variant.[52] As it was sure to produce a result, limited overs cricket was lucrative and the number of matches increased.[53] The first Limited Overs International was played in 1971 and the governing International Cricket Council (ICC), seeing its potential, staged the first limited overs Cricket World Cup in 1975.[54] In the 21st century, a new limited overs form, Twenty20, made an immediate impact.[citation needed] On 22 June 2017, Afghanistan and Ireland became the 11th and 12th ICC full members, enabling them to play Test cricket.[55][56]
",Cricket
"In cricket, the rules of the game are specified in a code called The Laws of Cricket (hereinafter called ""the Laws"") which has a global remit. There are 42 Laws (always written with a capital ""L""). The earliest known version of the code was drafted in 1744 and, since 1788, it has been owned and maintained by its custodian, the Marylebone Cricket Club (MCC) in London.[57]
",Cricket
"Cricket is a bat-and-ball game played on a cricket field (see image, right) between two teams of eleven players each.[58] The field is usually circular or oval in shape and the edge of the playing area is marked by a boundary, which may be a fence, part of the stands, a rope, a painted line or a combination of these; the boundary must if possible be marked along its entire length.[59]
",Cricket
"In the approximate centre of the field is a rectangular pitch (see image, below) on which a wooden target called a wicket is sited at each end; the wickets are placed 22 yards (20 m) apart.[60] The pitch is a flat surface 3 metres (9.8 ft) wide, with very short grass that tends to be worn away as the game progresses (cricket can also be played on artificial surfaces, notably matting). Each wicket is made of three wooden stumps topped by two bails.[61]
",Cricket
"As illustrated above, the pitch is marked at each end with four white painted lines: a bowling crease, a popping crease and two return creases. The three stumps are aligned centrally on the bowling crease, which is eight feet eight inches long. The popping crease is drawn four feet in front of the bowling crease and parallel to it; although it is drawn as a twelve-foot line (six feet either side of the wicket), it is in fact unlimited in length. The return creases are drawn at right angles to the popping crease so that they intersect the ends of the bowling crease; each return crease is drawn as an eight-foot line, so that it extends four feet behind the bowling crease, but is also in fact unlimited in length.[62]
",Cricket
"Before a match begins, the team captains (who are also players) toss a coin to decide which team will bat first and so take the first innings.[63] Innings is the term used for each phase of play in the match.[63] In each innings, one team bats, attempting to score runs, while the other team bowls and fields the ball, attempting to restrict the scoring and dismiss the batsmen.[64][65] When the first innings ends, the teams change roles; there can be two to four innings depending upon the type of match. A match with four scheduled innings is played over three to five days; a match with two scheduled innings is usually completed in a single day.[63] During an innings, all eleven members of the fielding team take the field, but only two members of the batting team are on the field at any given time.[63] The order of batsmen is usually announced just before the match, but it can be varied.[58]
",Cricket
"The main objective of each team is to score more runs than their opponents but, in some forms of cricket, it is also necessary to dismiss all of the opposition batsmen in their final innings in order to win the match, which would otherwise be drawn.[66] If the team batting last is all out having scored fewer runs than their opponents, they are said to have ""lost by n runs"" (where n is the difference between the aggregate number of runs scored by the teams). If the team that bats last scores enough runs to win, it is said to have ""won by n wickets"", where n is the number of wickets left to fall. For example, a team that passes its opponents' total having lost six wickets (i.e., six of their batsmen have been dismissed) have won the match ""by four wickets"".[66]
",Cricket
"In a two-innings-a-side match, one team's combined first and second innings total may be less than the other side's first innings total. The team with the greater score is then said to have ""won by an innings and n runs"", and does not need to bat again: n is the difference between the two teams' aggregate scores. If the team batting last is all out, and both sides have scored the same number of runs, then the match is a tie; this result is quite rare in matches of two innings a side with only 62 happening in first-class matches from the earliest known instance in 1741 until January 2017. In the traditional form of the game, if the time allotted for the match expires before either side can win, then the game is declared a draw.[66]
",Cricket
"If the match has only a single innings per side, then a maximum number of overs applies to each innings. Such a match is called a ""limited overs"" or ""one-day"" match, and the side scoring more runs wins regardless of the number of wickets lost, so that a draw cannot occur. If this kind of match is temporarily interrupted by bad weather, then a complex mathematical formula, known as the Duckworth-Lewis method after its developers, is often used to recalculate a new target score. A one-day match can also be declared a ""no-result"" if fewer than a previously agreed number of overs have been bowled by either team, in circumstances that make normal resumption of play impossible; for example, wet weather.[66]
",Cricket
"In all forms of cricket, the umpires can abandon the match if bad light or rain makes it impossible to continue.[67] There have been instances of entire matches, even Test matches scheduled to be played over five days, being lost to bad weather without a ball being bowled: for example, the third Test of the 1970/71 series in Australia.[68]
",Cricket
"i) A used white ball. White balls are mainly used in limited overs cricket, especially in matches played at night, under floodlights (left).
",Cricket
"The essence of the sport is that a bowler delivers (i.e., bowls) the ball from his end of the pitch towards the batsman who, armed with a bat is ""on strike"" at the other end (see next sub-section: Basic gameplay).
",Cricket
"The bat is made of wood, usually salix alba (white willow), and has the shape of a blade topped by a cylindrical handle. The blade must not be more than four and one quarter inches (108 mm) wide and the total length of the bat not more than 38 inches (965 mm). There is no standard for the weight which is usually between 2 lb 7 oz and 3 lb (1.1 and 1.4 kg).[69][70]
",Cricket
"The ball is a hard leather-seamed spheroid, with a circumference of 22.9 centimetres (9.0 in). The ball has a ""seam"": six rows of stitches attaching the leather shell of the ball to the string and cork interior. The seam on a new ball is prominent, and helps the bowler propel it in a less predictable manner. During matches, the quality of the ball deteriorates to a point where it is no longer usable, and during the course of this deterioration its behaviour in flight will change and can influence the outcome of the match. Players will therefore attempt to modify the ball's behaviour by modifying its physical properties. Polishing the ball and wetting it with sweat or saliva is legal, even when the polishing is deliberately done on one side only to increase the ball's swing through the air, but the acts of rubbing other substances into the ball, scratching the surface or picking at the seam is illegal ball tampering.[71]
",Cricket
"During normal play, thirteen players and two umpires are on the field. Two of the players are batsmen and the rest are all eleven members of the fielding team. The other nine players in the batting team are off the field in the pavilion. The image with overlay below shows what is happening when a ball is being bowled and which of the personnel are on or close to the pitch. The photo was taken during an international match between Australia and Sri Lanka; Muttiah Muralitharan of Sri Lanka is bowling to Australian batsman Adam Gilchrist.
",Cricket
"In the photo, the two batsmen (3 & 8; wearing yellow) have taken position at each end of the pitch (6). Three members of the fielding team (4, 10 & 11; wearing dark blue) are in shot. One of the two umpires (1; wearing white hat) is stationed behind the wicket (2) at the bowler's (4) end of the pitch. The bowler (4) is bowling the ball (5) from his end of the pitch to the batsman (8) at the other end who is called the ""striker"". The other batsman (3) at the bowling end is called the ""non-striker"". The wicket-keeper (10), who is a specialist, is positioned behind the striker's wicket (9) and behind him stands one of the fielders in a position called ""first slip"" (11). While the bowler and the first slip are wearing conventional kit only, the two batsmen and the wicket-keeper are wearing protective gear including safety helmets, padded gloves and leg guards (pads).
",Cricket
"While the umpire (1) in shot stands at the bowler's end of the pitch, his colleague stands in the outfield, usually in or near the fielding position called ""square leg"", so that he is in line with the popping crease (7) at the striker's end of the pitch. The bowling crease (not numbered) is the one on which the wicket is located between the return creases (12). The bowler (4) intends to hit the wicket (9) with the ball (5) or, at least, to prevent the striker (8) from scoring runs. The striker (8) intends, by using his bat, to defend his wicket and, if possible, to hit the ball away from the pitch in order to score runs.
",Cricket
"Some players are skilled in both batting and bowling so are termed all-rounders. Adam Gilchrist, pictured above, was a wicket-keeper/batsman, another type of all-rounder. Bowlers are also classified according to their style, generally as fast bowlers, medium pace seam bowlers or, like Muttiah Muralitharan pictured above, spinners. Batsmen are classified according to whether they are right-handed or left-handed.
",Cricket
"Of the eleven fielders, three are in shot in the image above. The other eight are elsewhere on the field, their positions determined on a tactical basis by the captain or the bowler. Fielders often change position between deliveries, again as directed by the captain or bowler.[72]
",Cricket
"If a fielder is injured or becomes ill during a match, a substitute is allowed to field instead of him, but the substitute cannot bowl or act as a captain. The substitute leaves the field when the injured player is fit to return.[73] The Laws of Cricket were updated in 2017 to allow substitutes to act as wicket-keepers,[74] a situation that first occurred when Mumbai Indians' wicket-keeper Ishan Kishan was injured in a match on 18 April 2018.[75]
",Cricket
"The captain is often the most experienced player in the team, certainly the most tactically astute, and can possess any of the main skillsets as a batsman, a bowler or a wicket-keeper. Within the Laws, the captain has certain responsibilities in terms of nominating his players to the umpires before the match and ensuring that his players conduct themselves ""within the spirit and traditions of the game as well as within the Laws"".[58]
",Cricket
"The wicket-keeper (sometimes called simply the ""keeper"") is a specialist fielder subject to various rules within the Laws about his equipment and demeanour. He is the only member of the fielding side who can effect a stumping and is the only one permitted to wear gloves and external leg guards.[76] Depending on their primary skills, the other ten players in the team tend to be classified as specialist batsmen or specialist bowlers. Generally, a team will include five or six specialist batsmen and four or five specialist bowlers, plus the wicket-keeper.[77][78]
",Cricket
"The wicket-keeper and the batsmen wear protective gear because of the hardness of the ball, which can be delivered at speeds of more than 145 kilometres per hour (90 mph) and presents a major health and safety concern. Protective clothing includes pads (designed to protect the knees and shins), batting gloves or wicket-keeper's gloves for the hands, a safety helmet for the head and a box inside the trousers (to protect the crotch area).[79] Some batsmen wear additional padding inside their shirts and trousers such as thigh pads, arm pads, rib protectors and shoulder pads. The only fielders allowed to wear protective gear are those in positions very close to the batsman (i.e., if they are alongside or in front of him), but they cannot wear gloves or external leg guards.[72]
",Cricket
"Subject to certain variations, on-field clothing generally includes a collared shirt with short or long sleeves; long trousers; woollen pullover (if needed); cricket cap (for fielding) or a safety helmet; and spiked shoes or boots to increase traction. The kit is traditionally all white and this remains the case in Test and first-class cricket but, in limited overs cricket, team colours are worn instead.[80]
",Cricket
"The innings (ending with 's' in both singular and plural form) is the term used for each phase of play during a match. Depending on the type of match being played, each team has either one or two innings. Sometimes all eleven members of the batting side take a turn to bat but, for various reasons, an innings can end before they have all done so. The innings terminates if the batting team is ""all out"", a term defined by the Laws: ""at the fall of a wicket or the retirement of a batsman, further balls remain to be bowled but no further batsman is available to come in"".[63] In this situation, one of the batsman has not been dismissed and is termed not out; this is because he has no partners left and there must always be two active batsmen while the innings is in progress.
",Cricket
"An innings may end early while there are still two not out batsmen:[63]
",Cricket
"The Laws state that, throughout an innings, ""the ball shall be bowled from each end alternately in overs of 6 balls"".[81] The name ""over"" came about because the umpire calls ""Over!"" when six balls have been bowled. At this point, another bowler is deployed at the other end, and the fielding side changes ends while the batsmen do not. A bowler cannot bowl two successive overs, although a bowler can (and usually does) bowl alternate overs, from the same end, for several overs which are termed a ""spell"". The batsmen do not change ends at the end of the over, and so the one who was non-striker is now the striker and vice-versa. The umpires also change positions so that the one who was at ""square leg"" now stands behind the wicket at the non-striker's end and vice-versa.[81]
",Cricket
"The game on the field is regulated by the two umpires, one of whom stands behind the wicket at the bowler's end, the other in a position called ""square leg"" which is about 15–20 metres away from the batsman on strike and in line with the popping crease on which he is taking guard. The umpires have several responsibilities including adjudication on whether a ball has been correctly bowled (i.e., not a no-ball or a wide); when a run is scored; whether a batsman is out (the fielding side must first appeal to the umpire, usually with the phrase ""How's that?"" or ""Owzat?""); when intervals start and end; and the suitability of the pitch, field and weather for playing the game. The umpires are authorised to interrupt or even abandon a match due to circumstances likely to endanger the players, such as a damp pitch or deterioration of the light.[67]
",Cricket
"Off the field in televised matches, there is usually a third umpire who can make decisions on certain incidents with the aid of video evidence. The third umpire is mandatory under the playing conditions for Test and Limited Overs International matches played between two ICC full member countries. These matches also have a match referee whose job is to ensure that play is within the Laws and the spirit of the game.[67]
",Cricket
"The match details, including runs and dismissals, are recorded by two official scorers, one representing each team. The scorers are directed by the hand signals of an umpire (see image, right). For example, the umpire raises a forefinger to signal that the batsman is out (has been dismissed); he raises both arms above his head if the batsman has hit the ball for six runs. The scorers are required by the Laws to record all runs scored, wickets taken and overs bowled; in practice, they also note significant amounts of additional data relating to the game.[82]
",Cricket
"A match's statistics are summarised on a scorecard. Prior to the popularisation of scorecards, most scoring was done by men sitting on vantage points cuttings notches on tally sticks and runs were originally called notches.[83] According to Rowland Bowen, the earliest known scorecard templates were introduced in 1776 by T. Pratt of Sevenoaks and soon came into general use.[84] It is believed that scorecards were printed and sold at Lord's for the first time in 1846.[85]
",Cricket
"Besides observing the Laws, cricketers must respect the ""Spirit of Cricket,"" which is the ""Preamble to the Laws,"" first published in the 2000 code, and updated in 2017, and now opens with this statement:[86]
",Cricket
"""Cricket owes much of its appeal and enjoyment to the fact that it should be played not only according to the Laws, but also within the Spirit of Cricket"".",Cricket
"The Preamble is a short statement that emphasises the ""Positive behaviours that make cricket an exciting game that encourages leadership,friendship and teamwork.""[87]
",Cricket
"The major responsibility for ensuring fair play is placed firmly on the captains, but extends to all players, umpires, teachers, coaches and parents involved.
",Cricket
"The umpires are the sole judges of fair and unfair play. They are required under the Laws to intervene in case of dangerous or unfair play or in cases of unacceptable conduct by a player.
",Cricket
"Previous versions of the Spirit identified actions that were deemed contrary (for example, appealing knowing that the batsman is not out) but all specifics are now covered in the Laws of Cricket, the relevant governing playing regulations and disciplinary codes, or left to the judgement of the umpires, captains, their clubs and governing bodies. The terse expression of the Spirit of Cricket now avoids the diversity of cultural conventions that exist on the detail of sportsmanship – or its absence.
",Cricket
"Most bowlers are considered specialists in that they are selected for the team because of their skill as a bowler, although some are all-rounders and even specialist batsmen bowl occasionally. The specialist bowlers are active multiple times during an innings, but may not bowl two overs consecutively. If the captain wants a bowler to ""change ends"", another bowler must temporarily fill in so that the change is not immediate.[81]
",Cricket
"A bowler reaches his delivery stride by means of a ""run-up"" and an over is deemed to have begun when the bowler starts his run-up for the first delivery of that over, the ball then being ""in play"".[81] Fast bowlers, needing momentum, take a lengthy run up while bowlers with a slow delivery take no more than a couple of steps before bowling. The fastest bowlers can deliver the ball at a speed of over 145 kilometres per hour (90 mph) and they sometimes rely on sheer speed to try and defeat the batsman, who is forced to react very quickly.[89] Other fast bowlers rely on a mixture of speed and guile by making the ball seam or swing (i.e. curve) in flight. This type of delivery can deceive a batsman into miscuing his shot, for example, so that the ball just touches the edge of the bat and can then be ""caught behind"" by the wicket-keeper or a slip fielder.[89] At the other end of the bowling scale is the spin bowler who bowls at a relatively slow pace and relies entirely on guile to deceive the batsman. A spinner will often ""buy his wicket"" by ""tossing one up"" (in a slower, steeper parabolic path) to lure the batsman into making a poor shot. The batsman has to be very wary of such deliveries as they are often ""flighted"" or spun so that the ball will not behave quite as he expects and he could be ""trapped"" into getting himself out.[90] In between the pacemen and the spinners are the medium paced seamers who rely on persistent accuracy to try and contain the rate of scoring and wear down the batsman's concentration.[89]
",Cricket
"There are ten ways in which a batsman can be dismissed: five relatively common and five extremely rare. The common forms of dismissal are bowled,[91] caught,[92] leg before wicket (lbw),[93] run out[94] and stumped.[95] Rare methods are hit wicket,[96] hit the ball twice,[97] obstructing the field,[98] handled the ball[99] and timed out.[100] The Laws state that the fielding team, usually the bowler in practice, must appeal for a dismissal before the umpire can give his decision. If the batsman is out, the umpire raises a forefinger and says ""Out!""; otherwise, he will shake his head and say ""Not out"".[101] There is, effectively, an eleventh method of dismissal, retired out, which is not an on-field dismissal as such but rather a retrospective one for which no fielder is credited.[102]
",Cricket
"Batsmen take turns to bat via a batting order which is decided beforehand by the team captain and presented to the umpires, though the order remains flexible when the captain officially nominates the team.[58] Substitute batsmen are not allowed.[73]
",Cricket
"A skilled batsman can use a wide array of ""shots"" or ""strokes"" in both defensive and attacking mode. The idea is to hit the ball to best effect with the flat surface of the bat's blade. If the ball touches the side of the bat it is called an ""edge"". The batsman does not have to play a shot and can allow the ball to go through to the wicketkeeper. Equally, he does not have to attempt a run when he hits the ball with his bat. Batsmen do not always seek to hit the ball as hard as possible, and a good player can score runs just by making a deft stroke with a turn of the wrists or by simply ""blocking"" the ball but directing it away from fielders so that he has time to take a run. A wide variety of shots are played, the batsman's repertoire including strokes named according to the style of swing and the direction aimed: e.g., ""cut"", ""drive"", ""hook"", ""pull"".[103]
",Cricket
"The batsman on strike (i.e. the ""striker"") must prevent the ball hitting the wicket, and try to score runs by hitting the ball with his bat so that he and his partner have time to run from one end of the pitch to the other before the fielding side can return the ball. To register a run, both runners must touch the ground behind the popping crease with either their bats or their bodies (the batsmen carry their bats as they run). Each completed run increments the score of both the team and the striker.[104]
",Cricket
"The decision to attempt a run is ideally made by the batsman who has the better view of the ball's progress, and this is communicated by calling: usually ""yes"", ""no"" or ""wait"". More than one run can be scored from a single hit: hits worth one to three runs are common, but the size of the field is such that it is usually difficult to run four or more.[104] To compensate for this, hits that reach the boundary of the field are automatically awarded four runs if the ball touches the ground en route to the boundary or six runs if the ball clears the boundary without touching the ground within the boundary. In these cases the batsmen do not need to run.[105] Hits for five are unusual and generally rely on the help of ""overthrows"" by a fielder returning the ball. If an odd number of runs is scored by the striker, the two batsmen have changed ends, and the one who was non-striker is now the striker. Only the striker can score individual runs, but all runs are added to the team's total.[104]
",Cricket
"Additional runs can be gained by the batting team as extras (called ""sundries"" in Australia) due to errors made by the fielding side. This is achieved in four ways: no-ball, a penalty of one extra conceded by the bowler if he breaks the rules;[106] wide, a penalty of one extra conceded by the bowler if he bowls so that the ball is out of the batsman's reach;[107] bye, an extra awarded if the batsman misses the ball and it goes past the wicket-keeper and gives the batsmen time to run in the conventional way;[108] leg bye, as for a bye except that the ball has hit the batsman's body, though not his bat.[108] If the bowler has conceded a no-ball or a wide, his team incurs an additional penalty because that ball (i.e., delivery) has to be bowled again and hence the batting side has the opportunity to score more runs from this extra ball.[106][107]
",Cricket
"Women's cricket was first recorded in Surrey in 1745.[109] International development began at the start of the 20th century and the first Test Match was played between Australia and England in December 1934.[110] The following year, New Zealand women joined them, and in 2007 Netherlands women became the tenth women's Test nation when they made their debut against South Africa women. In 1958, the International Women's Cricket Council was founded (it merged with the ICC in 2005).[110] In 1973, the first Cricket World Cup of any kind took place when a Women's World Cup was held in England.[110]  In 2005, the International Women's Cricket Council was merged with the International Cricket Council (ICC) to form one unified body to help manage and develop cricket. The ICC Women's Rankings were launched on 1 October 2015 covering all three formats of women's cricket. In October 2018 following the ICC's decision to award T20 International status to all members, the Women's rankings were split into separate ODI (for Full Members) and T20I lists.[111]
",Cricket
"The International Cricket Council (ICC), which has its headquarters in Dubai, is the global governing body of cricket. It was founded as the Imperial Cricket Conference in 1909 by representatives from England, Australia and South Africa, renamed the International Cricket Conference in 1965, and took up its current name in 1989.[110] The ICC in 2017 has 105 member nations, twelve of which hold full membership and can play Test cricket.[112] The ICC is responsible for the organisation and governance of cricket's major international tournaments, notably the men's and women's versions of the Cricket World Cup. It also appoints the umpires and referees that officiate at all sanctioned Test matches, Limited Overs Internationals and Twenty20 Internationals.
",Cricket
"Each member nation has a national cricket board which regulates cricket matches played in its country, selects the national squad, and organises home and away tours for the national team.[113] In the West Indies, which for cricket purposes is a federation of nations, these matters are addressed by Cricket West Indies.[114]
",Cricket
"The table below lists the ICC full members and their national cricket boards:[115]
",Cricket
"Cricket is a multi-faceted sport with multiple formats that can effectively be divided into first-class cricket, limited overs cricket and, historically, single wicket cricket. The highest standard is Test cricket (always written with a capital ""T"") which is in effect the international version of first-class cricket and is restricted to teams representing the twelve countries that are full members of the ICC (see above). Although the term ""Test match"" was not coined until much later, Test cricket is deemed to have begun with two matches between Australia and England in the 1876–77 Australian season; since 1882, most Test series between England and Australia have been played for a trophy known as The Ashes. The term ""first-class"", in general usage, is applied to top-level domestic cricket. Test matches are played over five days and first-class over three to four days; in all of these matches, the teams are allotted two innings each and the draw is a valid result.[117]
",Cricket
"Limited overs cricket is always scheduled for completion in a single day. There are two types: List A which normally allows fifty overs per team; and Twenty20 in which the teams have twenty overs each. Both of the limited overs forms are played internationally as Limited Overs Internationals (LOI) and Twenty20 Internationals (T20I). List A was introduced in England in the 1963 season as a knockout cup contested by the first-class county clubs. In 1969, a national league competition was established. The concept was gradually introduced to the other leading cricket countries and the first limited overs international was played in 1971. In 1975, the first Cricket World Cup took place in England. Twenty20 is a new variant of limited overs itself with the purpose being to complete the match within about three hours, usually in an evening session. The first Twenty20 World Championship was held in 2007. Limited overs matches cannot be drawn, although a tie is possible and an unfinished match is a ""no result"".[118][119]
",Cricket
"Single wicket was popular in the 18th and 19th centuries and its matches were generally considered top-class. In this form, although each team may have from one to six players, there is only one batsman in at a time and he must face every delivery bowled while his innings lasts. Single wicket has rarely been played since limited overs cricket began. Matches tended to have two innings per team like a full first-class one and they could end in a draw.[120]
",Cricket
"Most international matches are played as parts of 'tours', when one nation travels to another for a number of weeks or months, and plays a number of matches of various sorts against the host nation. Sometimes a perpetual trophy is awarded to the winner of the Test series, the most famous of which is The Ashes.
",Cricket
"The ICC also organises competitions that are for several countries at once, including the Cricket World Cup, ICC T20 World Cup and ICC Champions Trophy. A league competition for Test matches played as part of normal tours, the ICC World Test Championship, has been proposed several times, and is currently planned to begin in 2019. The ICC maintains Test rankings, ODI rankings and T20 rankings systems for the countries which play these forms of cricket.
",Cricket
"Competitions for member nations of the ICC with Associate status include the ICC Intercontinental Cup, for first-class cricket matches, and the World Cricket League for one-day matches, the final matches of which now also serve as the ICC World Cup Qualifier.
",Cricket
"First-class cricket in England is played for the most part by the 18 county clubs which contest the County Championship. The concept of a champion county has existed since the 18th century but the official competition was not established until 1890.[40] The most successful club has been Yorkshire, who had won 32 official titles (plus one shared) to 2017.[121]
",Cricket
"Australia established its national first-class championship in 1892–93 when the Sheffield Shield was introduced. In Australia, the first-class teams represent the various states.[122] New South Wales has the highest number of titles.
",Cricket
"The other ICC full members have national championship trophies called the Ahmad Shah Abdali 4-day Tournament (Afghanistan); the National Cricket League (Bangladesh); the Ranji Trophy (India); the Inter-Provincial Championship (Ireland); the Plunket Shield (New Zealand); the Quaid-e-Azam Trophy (Pakistan); the Currie Cup (South Africa); the Premier Trophy (Sri Lanka); the Shell Shield (West Indies); and the Logan Cup (Zimbabwe).
",Cricket
"The world's earliest known cricket match was a village cricket meeting in Kent which has been deduced from a 1640 court case recording a ""cricketing"" of ""the Weald and the Upland"" versus ""the Chalk Hill"" at Chevening ""about thirty years since"" (i.e., c. 1611). Inter-parish contests became popular in the first half of the 17th century and continued to develop through the 18th with the first local leagues being founded in the second half of the 19th.[17]
",Cricket
"At the grassroots level, local club cricket is essentially an amateur pastime for those involved but still usually involves teams playing in competitions at weekends or in the evening. Schools cricket, first known in southern England in the 17th century, has a similar scenario and both are widely played in the countries where cricket is popular.[123] Although there can be variations in game format, compared with professional cricket, the Laws are always observed and club/school matches are therefore formal and competitive events.[124] The sport has numerous informal variants such as French cricket.[125]
",Cricket
"Cricket has had a broad impact on popular culture, both in the Commonwealth of Nations and elsewhere. It has, for example, influenced the lexicon of these nations, especially the English language, with various phrases such as ""that's not cricket"" (that's unfair), ""had a good innings"" (lived a long life) and ""sticky wicket"". ""On a sticky wicket"" (aka ""sticky dog"" or ""glue pot"")[126] is a metaphor[127] used to describe a difficult circumstance. It originated as a term for difficult batting conditions in cricket, caused by a damp and soft pitch.[128]
",Cricket
"Cricket is the subject of works by noted English poets, including William Blake and Lord Byron.[129] Beyond a Boundary (1963), written by Trinidadian C. L. R. James, is often named the best book on any sport ever written.[130]
",Cricket
"In the visual arts, notable cricket paintings include Albert Chevallier Tayler's Kent vs Lancashire at Canterbury (1907) and Russell Drysdale's The Cricketers (1948), which has been called ""possibly the most famous Australian painting of the 20th century.""[131] French impressionist Camille Pissarro painted cricket on a visit to England in the 1890s.[129] Francis Bacon, an avid cricket fan, captured a batsman in motion.[129] Caribbean artist Wendy Nanan's cricket images[132] are featured in a limited edition first day cover for Royal Mail's ""World of Invention"" stamp issue, which celebrated the London Cricket Conference 1–3 March 2007, first international workshop of its kind and part of the celebrations leading up to the 2007 Cricket World Cup.[133]
",Cricket
"Cricket has close historical ties with Australian rules football and many players have competed at top levels in both sports.[134] In 1858, prominent Australian cricketer Tom Wills called for the formation of a ""foot-ball club"" with ""a code of laws"" to keep cricketers fit during the off-season. The Melbourne Football Club was founded the following year, and Wills and three other members codified the first laws of the game.[135] It is typically played on modified cricket fields.[136]
",Cricket
"In England, a number of association football clubs owe their origins to cricketers who sought to play football as a means of keeping fit during the winter months. Derby County was founded as a branch of the Derbyshire County Cricket Club in 1884;[137] Aston Villa (1874) and Everton (1876) were both founded by members of church cricket teams.[138] Sheffield United's Bramall Lane ground was, from 1854, the home of the Sheffield Cricket Club, and then of Yorkshire; it was not used for football until 1862 and was shared by Yorkshire and Sheffield United from 1889 to 1973.[139]
",Cricket
"In the late 19th century, a former cricketer, English-born Henry Chadwick of Brooklyn, New York, was credited with devising the baseball box score[140] (which he adapted from the cricket scorecard) for reporting game events. The first box score appeared in an 1859 issue of the Clipper.[141] The statistical record is so central to the game's ""historical essence"" that Chadwick is sometimes referred to as ""the Father of Baseball"" because he facilitated the popularity of the sport in its early days.[142]
",Cricket
"Related sports
",Cricket
"Organisations and competitions
",Cricket
"Statistics and records
",Cricket
"News and other resources
",Cricket
"
",Cricket
"Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 20-metre (22-yard) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The batting side scores runs by striking the ball bowled at the wicket with the bat, while the bowling and fielding side tries to prevent this and dismiss each player (so they are ""out""). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side catching the ball after it is hit by the bat, but before it hits the ground. When ten players have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches. They communicate with two off-field scorers who record the match's statistical information.
",Cricket
"There are various formats ranging from Twenty20, played over a few hours with each team batting for a single innings of 20 overs, to Test matches, played over five days with unlimited overs and the teams each batting for two innings of unlimited length. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid spheroid made of compressed leather with a slightly raised sewn seam enclosing a cork core which is layered with tightly wound string.
",Cricket
"Historically, cricket's origins are uncertain and the earliest definite reference is in south-east England in the middle of the 16th century. It spread globally with the expansion of the British Empire, leading to the first international matches in the second half of the 19th century. The game's governing body is the International Cricket Council (ICC), which has over 100 members, twelve of which are full members who play Test matches. The game's rules are held in a code called the Laws of Cricket which is owned and maintained by Marylebone Cricket Club (MCC) in London. The sport is followed primarily in the Indian subcontinent, Australasia, the United Kingdom, Ireland, southern Africa and the West Indies, its globalisation occurring during the expansion of the British Empire and remaining popular into the 21st century.[1] Women's cricket, which is organised and played separately, has also achieved international standard. The most successful side playing international cricket is Australia, having won seven One Day International trophies, including five World Cups, more than any other country, and having been the top-rated Test side more than any other country.
",Cricket
"
",Cricket
"Cricket is one of many games in the ""club ball"" sphere that basically involve hitting a ball with a hand-held implement; others are baseball, golf, hockey, tennis, squash, and table tennis.[2] In cricket's case, a key difference is the existence of a solid target structure, the wicket (originally, it is thought, a ""wicket gate"" through which sheep were herded), that the batsman must defend.[3] The cricket historian Harry Altham identified three ""groups"" of ""club ball"" games: the ""hockey group"", in which the ball is driven to and fro between two targets (the goals); the ""golf group"", in which the ball is driven towards an undefended target (the hole); and the ""cricket group"", in which ""the ball is aimed at a mark (the wicket) and driven away from it"".[4]
",Cricket
"It is generally believed that cricket originated as a children's game in the south-eastern counties of England, sometime during the medieval period.[3] Although there are claims for prior dates, the earliest definite reference to cricket being played comes from evidence given at a court case in Guildford on Monday, 17 January 1597 (Julian calendar; equating to 30 January 1598 in the Gregorian calendar). The case concerned ownership of a certain plot of land and the court heard the testimony of a 59-year-old coroner, John Derrick, who gave witness that:[5][6][7]
",Cricket
"""Being a scholler in the ffree schoole of Guldeford hee and diverse of his fellows did runne and play there at creckett and other plaies"".",Cricket
"Given Derrick's age, it was about half a century earlier when he was at school and so it is certain that cricket was being played c. 1550 by boys in Surrey.[7] The view that it was originally a children's game is reinforced by Randle Cotgrave's 1611 English-French dictionary in which he defined the noun ""crosse"" as ""the crooked staff wherewith boys play at cricket"" and the verb form ""crosser"" as ""to play at cricket"".[8][9]
",Cricket
"One possible source for the sport's name is the Old English word ""cryce"" (or ""cricc"") meaning a crutch or staff. In Samuel Johnson's Dictionary, he derived cricket from ""cryce, Saxon, a stick"".[5] In Old French, the word ""criquet"" seems to have meant a kind of club or stick.[10] Given the strong medieval trade connections between south-east England and the County of Flanders when the latter belonged to the Duchy of Burgundy, the name may have been derived from the Middle Dutch (in use in Flanders at the time) ""krick""(-e), meaning a stick (crook).[10] Another possible source is the Middle Dutch word ""krickstoel"", meaning a long low stool used for kneeling in church and which resembled the long low wicket with two stumps used in early cricket.[11] According to Heiner Gillmeister, a European language expert of Bonn University, ""cricket"" derives from the Middle Dutch phrase for hockey, met de (krik ket)sen (i.e., ""with the stick chase"").[12] Gillmeister has suggested that not only the name but also the sport itself may be of Flemish origin.[12]
",Cricket
"Although the main object of the game has always been to score the most runs, the early form of cricket differed from the modern game in certain key technical aspects. The ball was bowled underarm by the bowler and all along the ground towards a batsman armed with a bat that, in shape, resembled a hockey stick; the batsman defended a low, two-stump wicket; and runs were called ""notches"" because the scorers recorded them by notching tally sticks.[13][14][15]
",Cricket
"In 1611, the year Cotgrave's dictionary was published, ecclesiastical court records at Sidlesham in Sussex state that two parishioners, Bartholomew Wyatt and Richard Latter, failed to attend church on Easter Sunday because they were playing cricket. They were fined 12d each and ordered to do penance.[16] This is the earliest mention of adult participation in cricket and it was around the same time that the earliest known organised inter-parish or village match was played – at Chevening, Kent.[5][17] In 1624, a player called Jasper Vinall died after he was accidentally struck on the head during a match between two parish teams in Sussex.[18]
",Cricket
"Cricket remained a low-key local pursuit for much of the century.[9] It is known, through numerous references found in the records of ecclesiastical court cases, to have been proscribed at times by the Puritans before and during the Commonwealth.[19][20] The problem was nearly always the issue of Sunday play as the Puritans considered cricket to be ""profane"" if played on the Sabbath, especially if large crowds and/or gambling were involved.[21][22]
",Cricket
"According to the social historian Derek Birley, there was a ""great upsurge of sport after the Restoration"" in 1660.[23] Gambling on sport became a problem significant enough for Parliament to pass the 1664 Gambling Act, limiting stakes to £100 which was in any case a colossal sum exceeding the annual income of 99% of the population.[23] Along with prizefighting, horse racing and blood sports, cricket was perceived to be a gambling sport.[24] Rich patrons made matches for high stakes, forming teams in which they engaged the first professional players.[25] By the end of the century, cricket had developed into a major sport which was spreading throughout England and was already being taken abroad by English mariners and colonisers – the earliest reference to cricket overseas is dated 1676.[26] A 1697 newspaper report survives of ""a great cricket match"" played in Sussex ""for fifty guineas apiece"" – this is the earliest known match that is generally considered top-class.[27][28]
",Cricket
"The patrons, and other players from the social class known as the ""gentry"", began to classify themselves as ""amateurs""[fn 1] to establish a clear distinction vis-à-vis the professionals, who were invariably members of the working class, even to the point of having separate changing and dining facilities.[29] The gentry, including such high-ranking nobles as the Dukes of Richmond, exerted their honour code of noblesse oblige to claim rights of leadership in any sporting contests they took part in, especially as it was necessary for them to play alongside their ""social inferiors"" if they were to win their bets.[30] In time, a perception took hold that the typical amateur who played in first-class cricket, until 1962 when amateurism was abolished, was someone with a public school education who had then gone to one of Cambridge or Oxford University – society insisted that such people were ""officers and gentlemen"" whose destiny was to provide leadership.[31] In a purely financial sense, the cricketing amateur would theoretically claim expenses for playing while his professional counterpart played under contract and was paid a wage or match fee; in practice, many amateurs claimed somewhat more than actual expenditure and the derisive term ""shamateur"" was coined to describe the syndrome.[32][33]
",Cricket
"The game underwent major development in the 18th century to become England's national sport.[citation needed] Its success was underwritten by the twin necessities of patronage and betting.[34] Cricket was prominent in London as early as 1707 and, in the middle years of the century, large crowds flocked to matches on the Artillery Ground in Finsbury.[citation needed] The single wicket form of the sport attracted huge crowds and wagers to match, its popularity peaking in the 1748 season.[35] Bowling underwent an evolution around 1760 when bowlers began to pitch the ball instead of rolling or skimming it towards the batsman. This caused a revolution in bat design because, to deal with the bouncing ball, it was necessary to introduce the modern straight bat in place of the old ""hockey stick"" shape.[36][citation needed]
",Cricket
"The Hambledon Club was founded in the 1760s and, for the next twenty years until the formation of Marylebone Cricket Club (MCC) and the opening of Lord's Old Ground in 1787, Hambledon was both the game's greatest club and its focal point.[citation needed] MCC quickly became the sport's premier club and the custodian of the Laws of Cricket. New Laws introduced in the latter part of the 18th century included the three stump wicket and leg before wicket (lbw).[37]
",Cricket
"The 19th century saw underarm bowling superseded by first roundarm and then overarm bowling. Both developments were controversial.[38] Organisation of the game at county level led to the creation of the county clubs, starting with Sussex in 1839.[39] In December 1889, the eight leading county clubs formed the official County Championship, which began in 1890.[40]
",Cricket
"The most famous player of the 19th century was W. G. Grace, who started his long and influential career in 1865. It was especially during the career of Grace that the distinction between amateurs and professionals became blurred by the existence of players like him who were nominally amateur but, in terms of their financial gain, de facto professional. Grace himself was said to have been paid more money for playing cricket than any professional.[citation needed]
",Cricket
"The last two decades before the First World War have been called the ""Golden Age of cricket"". It is a nostalgic name prompted by the collective sense of loss resulting from the war, but the period did produce some great players and memorable matches, especially as organised competition at county and Test level developed.[41]
",Cricket
"Meanwhile, the British Empire had been instrumental in spreading the game overseas and by the middle of the 19th century it had become well established in Australia, the Caribbean, India, New Zealand, North America and South Africa.[42] In 1844, the first-ever international match took place between the United States and Canada.[43] In 1859, a team of English players went to North America on the first overseas tour.[44]
",Cricket
"In 1862, an English team made the first tour of Australia.[45] The first Australian team to travel overseas consisted of Aboriginal stockmen who toured England in 1868.[46]
",Cricket
"In 1876–77, an England team took part in what was retrospectively recognised as the first-ever Test match at the Melbourne Cricket Ground against Australia.[47] The rivalry between England and Australia gave birth to The Ashes in 1882 and this has remained Test cricket's most famous contest.[48] Test cricket began to expand in 1888–89 when South Africa played England.[citation needed]
",Cricket
"The inter-war years were dominated by Australia's Don Bradman, statistically the greatest Test batsman of all time. Test cricket continued to expand during the 20th century with the addition of the West Indies (1928), New Zealand (1930) and India (1932) before the Second World War and then Pakistan (1952), Sri Lanka (1982), Zimbabwe (1992) and Bangladesh (2000) in the post-war period.[49][50] South Africa was banned from international cricket from 1970 to 1992 as part of the apartheid boycott.[51]
",Cricket
"Cricket entered a new era in 1963 when English counties introduced the limited overs variant.[52] As it was sure to produce a result, limited overs cricket was lucrative and the number of matches increased.[53] The first Limited Overs International was played in 1971 and the governing International Cricket Council (ICC), seeing its potential, staged the first limited overs Cricket World Cup in 1975.[54] In the 21st century, a new limited overs form, Twenty20, made an immediate impact.[citation needed] On 22 June 2017, Afghanistan and Ireland became the 11th and 12th ICC full members, enabling them to play Test cricket.[55][56]
",Cricket
"In cricket, the rules of the game are specified in a code called The Laws of Cricket (hereinafter called ""the Laws"") which has a global remit. There are 42 Laws (always written with a capital ""L""). The earliest known version of the code was drafted in 1744 and, since 1788, it has been owned and maintained by its custodian, the Marylebone Cricket Club (MCC) in London.[57]
",Cricket
"Cricket is a bat-and-ball game played on a cricket field (see image, right) between two teams of eleven players each.[58] The field is usually circular or oval in shape and the edge of the playing area is marked by a boundary, which may be a fence, part of the stands, a rope, a painted line or a combination of these; the boundary must if possible be marked along its entire length.[59]
",Cricket
"In the approximate centre of the field is a rectangular pitch (see image, below) on which a wooden target called a wicket is sited at each end; the wickets are placed 22 yards (20 m) apart.[60] The pitch is a flat surface 3 metres (9.8 ft) wide, with very short grass that tends to be worn away as the game progresses (cricket can also be played on artificial surfaces, notably matting). Each wicket is made of three wooden stumps topped by two bails.[61]
",Cricket
"As illustrated above, the pitch is marked at each end with four white painted lines: a bowling crease, a popping crease and two return creases. The three stumps are aligned centrally on the bowling crease, which is eight feet eight inches long. The popping crease is drawn four feet in front of the bowling crease and parallel to it; although it is drawn as a twelve-foot line (six feet either side of the wicket), it is in fact unlimited in length. The return creases are drawn at right angles to the popping crease so that they intersect the ends of the bowling crease; each return crease is drawn as an eight-foot line, so that it extends four feet behind the bowling crease, but is also in fact unlimited in length.[62]
",Cricket
"Before a match begins, the team captains (who are also players) toss a coin to decide which team will bat first and so take the first innings.[63] Innings is the term used for each phase of play in the match.[63] In each innings, one team bats, attempting to score runs, while the other team bowls and fields the ball, attempting to restrict the scoring and dismiss the batsmen.[64][65] When the first innings ends, the teams change roles; there can be two to four innings depending upon the type of match. A match with four scheduled innings is played over three to five days; a match with two scheduled innings is usually completed in a single day.[63] During an innings, all eleven members of the fielding team take the field, but only two members of the batting team are on the field at any given time.[63] The order of batsmen is usually announced just before the match, but it can be varied.[58]
",Cricket
"The main objective of each team is to score more runs than their opponents but, in some forms of cricket, it is also necessary to dismiss all of the opposition batsmen in their final innings in order to win the match, which would otherwise be drawn.[66] If the team batting last is all out having scored fewer runs than their opponents, they are said to have ""lost by n runs"" (where n is the difference between the aggregate number of runs scored by the teams). If the team that bats last scores enough runs to win, it is said to have ""won by n wickets"", where n is the number of wickets left to fall. For example, a team that passes its opponents' total having lost six wickets (i.e., six of their batsmen have been dismissed) have won the match ""by four wickets"".[66]
",Cricket
"In a two-innings-a-side match, one team's combined first and second innings total may be less than the other side's first innings total. The team with the greater score is then said to have ""won by an innings and n runs"", and does not need to bat again: n is the difference between the two teams' aggregate scores. If the team batting last is all out, and both sides have scored the same number of runs, then the match is a tie; this result is quite rare in matches of two innings a side with only 62 happening in first-class matches from the earliest known instance in 1741 until January 2017. In the traditional form of the game, if the time allotted for the match expires before either side can win, then the game is declared a draw.[66]
",Cricket
"If the match has only a single innings per side, then a maximum number of overs applies to each innings. Such a match is called a ""limited overs"" or ""one-day"" match, and the side scoring more runs wins regardless of the number of wickets lost, so that a draw cannot occur. If this kind of match is temporarily interrupted by bad weather, then a complex mathematical formula, known as the Duckworth-Lewis method after its developers, is often used to recalculate a new target score. A one-day match can also be declared a ""no-result"" if fewer than a previously agreed number of overs have been bowled by either team, in circumstances that make normal resumption of play impossible; for example, wet weather.[66]
",Cricket
"In all forms of cricket, the umpires can abandon the match if bad light or rain makes it impossible to continue.[67] There have been instances of entire matches, even Test matches scheduled to be played over five days, being lost to bad weather without a ball being bowled: for example, the third Test of the 1970/71 series in Australia.[68]
",Cricket
"i) A used white ball. White balls are mainly used in limited overs cricket, especially in matches played at night, under floodlights (left).
",Cricket
"The essence of the sport is that a bowler delivers (i.e., bowls) the ball from his end of the pitch towards the batsman who, armed with a bat is ""on strike"" at the other end (see next sub-section: Basic gameplay).
",Cricket
"The bat is made of wood, usually salix alba (white willow), and has the shape of a blade topped by a cylindrical handle. The blade must not be more than four and one quarter inches (108 mm) wide and the total length of the bat not more than 38 inches (965 mm). There is no standard for the weight which is usually between 2 lb 7 oz and 3 lb (1.1 and 1.4 kg).[69][70]
",Cricket
"The ball is a hard leather-seamed spheroid, with a circumference of 22.9 centimetres (9.0 in). The ball has a ""seam"": six rows of stitches attaching the leather shell of the ball to the string and cork interior. The seam on a new ball is prominent, and helps the bowler propel it in a less predictable manner. During matches, the quality of the ball deteriorates to a point where it is no longer usable, and during the course of this deterioration its behaviour in flight will change and can influence the outcome of the match. Players will therefore attempt to modify the ball's behaviour by modifying its physical properties. Polishing the ball and wetting it with sweat or saliva is legal, even when the polishing is deliberately done on one side only to increase the ball's swing through the air, but the acts of rubbing other substances into the ball, scratching the surface or picking at the seam is illegal ball tampering.[71]
",Cricket
"During normal play, thirteen players and two umpires are on the field. Two of the players are batsmen and the rest are all eleven members of the fielding team. The other nine players in the batting team are off the field in the pavilion. The image with overlay below shows what is happening when a ball is being bowled and which of the personnel are on or close to the pitch. The photo was taken during an international match between Australia and Sri Lanka; Muttiah Muralitharan of Sri Lanka is bowling to Australian batsman Adam Gilchrist.
",Cricket
"In the photo, the two batsmen (3 & 8; wearing yellow) have taken position at each end of the pitch (6). Three members of the fielding team (4, 10 & 11; wearing dark blue) are in shot. One of the two umpires (1; wearing white hat) is stationed behind the wicket (2) at the bowler's (4) end of the pitch. The bowler (4) is bowling the ball (5) from his end of the pitch to the batsman (8) at the other end who is called the ""striker"". The other batsman (3) at the bowling end is called the ""non-striker"". The wicket-keeper (10), who is a specialist, is positioned behind the striker's wicket (9) and behind him stands one of the fielders in a position called ""first slip"" (11). While the bowler and the first slip are wearing conventional kit only, the two batsmen and the wicket-keeper are wearing protective gear including safety helmets, padded gloves and leg guards (pads).
",Cricket
"While the umpire (1) in shot stands at the bowler's end of the pitch, his colleague stands in the outfield, usually in or near the fielding position called ""square leg"", so that he is in line with the popping crease (7) at the striker's end of the pitch. The bowling crease (not numbered) is the one on which the wicket is located between the return creases (12). The bowler (4) intends to hit the wicket (9) with the ball (5) or, at least, to prevent the striker (8) from scoring runs. The striker (8) intends, by using his bat, to defend his wicket and, if possible, to hit the ball away from the pitch in order to score runs.
",Cricket
"Some players are skilled in both batting and bowling so are termed all-rounders. Adam Gilchrist, pictured above, was a wicket-keeper/batsman, another type of all-rounder. Bowlers are also classified according to their style, generally as fast bowlers, medium pace seam bowlers or, like Muttiah Muralitharan pictured above, spinners. Batsmen are classified according to whether they are right-handed or left-handed.
",Cricket
"Of the eleven fielders, three are in shot in the image above. The other eight are elsewhere on the field, their positions determined on a tactical basis by the captain or the bowler. Fielders often change position between deliveries, again as directed by the captain or bowler.[72]
",Cricket
"If a fielder is injured or becomes ill during a match, a substitute is allowed to field instead of him, but the substitute cannot bowl or act as a captain. The substitute leaves the field when the injured player is fit to return.[73] The Laws of Cricket were updated in 2017 to allow substitutes to act as wicket-keepers,[74] a situation that first occurred when Mumbai Indians' wicket-keeper Ishan Kishan was injured in a match on 18 April 2018.[75]
",Cricket
"The captain is often the most experienced player in the team, certainly the most tactically astute, and can possess any of the main skillsets as a batsman, a bowler or a wicket-keeper. Within the Laws, the captain has certain responsibilities in terms of nominating his players to the umpires before the match and ensuring that his players conduct themselves ""within the spirit and traditions of the game as well as within the Laws"".[58]
",Cricket
"The wicket-keeper (sometimes called simply the ""keeper"") is a specialist fielder subject to various rules within the Laws about his equipment and demeanour. He is the only member of the fielding side who can effect a stumping and is the only one permitted to wear gloves and external leg guards.[76] Depending on their primary skills, the other ten players in the team tend to be classified as specialist batsmen or specialist bowlers. Generally, a team will include five or six specialist batsmen and four or five specialist bowlers, plus the wicket-keeper.[77][78]
",Cricket
"The wicket-keeper and the batsmen wear protective gear because of the hardness of the ball, which can be delivered at speeds of more than 145 kilometres per hour (90 mph) and presents a major health and safety concern. Protective clothing includes pads (designed to protect the knees and shins), batting gloves or wicket-keeper's gloves for the hands, a safety helmet for the head and a box inside the trousers (to protect the crotch area).[79] Some batsmen wear additional padding inside their shirts and trousers such as thigh pads, arm pads, rib protectors and shoulder pads. The only fielders allowed to wear protective gear are those in positions very close to the batsman (i.e., if they are alongside or in front of him), but they cannot wear gloves or external leg guards.[72]
",Cricket
"Subject to certain variations, on-field clothing generally includes a collared shirt with short or long sleeves; long trousers; woollen pullover (if needed); cricket cap (for fielding) or a safety helmet; and spiked shoes or boots to increase traction. The kit is traditionally all white and this remains the case in Test and first-class cricket but, in limited overs cricket, team colours are worn instead.[80]
",Cricket
"The innings (ending with 's' in both singular and plural form) is the term used for each phase of play during a match. Depending on the type of match being played, each team has either one or two innings. Sometimes all eleven members of the batting side take a turn to bat but, for various reasons, an innings can end before they have all done so. The innings terminates if the batting team is ""all out"", a term defined by the Laws: ""at the fall of a wicket or the retirement of a batsman, further balls remain to be bowled but no further batsman is available to come in"".[63] In this situation, one of the batsman has not been dismissed and is termed not out; this is because he has no partners left and there must always be two active batsmen while the innings is in progress.
",Cricket
"An innings may end early while there are still two not out batsmen:[63]
",Cricket
"The Laws state that, throughout an innings, ""the ball shall be bowled from each end alternately in overs of 6 balls"".[81] The name ""over"" came about because the umpire calls ""Over!"" when six balls have been bowled. At this point, another bowler is deployed at the other end, and the fielding side changes ends while the batsmen do not. A bowler cannot bowl two successive overs, although a bowler can (and usually does) bowl alternate overs, from the same end, for several overs which are termed a ""spell"". The batsmen do not change ends at the end of the over, and so the one who was non-striker is now the striker and vice-versa. The umpires also change positions so that the one who was at ""square leg"" now stands behind the wicket at the non-striker's end and vice-versa.[81]
",Cricket
"The game on the field is regulated by the two umpires, one of whom stands behind the wicket at the bowler's end, the other in a position called ""square leg"" which is about 15–20 metres away from the batsman on strike and in line with the popping crease on which he is taking guard. The umpires have several responsibilities including adjudication on whether a ball has been correctly bowled (i.e., not a no-ball or a wide); when a run is scored; whether a batsman is out (the fielding side must first appeal to the umpire, usually with the phrase ""How's that?"" or ""Owzat?""); when intervals start and end; and the suitability of the pitch, field and weather for playing the game. The umpires are authorised to interrupt or even abandon a match due to circumstances likely to endanger the players, such as a damp pitch or deterioration of the light.[67]
",Cricket
"Off the field in televised matches, there is usually a third umpire who can make decisions on certain incidents with the aid of video evidence. The third umpire is mandatory under the playing conditions for Test and Limited Overs International matches played between two ICC full member countries. These matches also have a match referee whose job is to ensure that play is within the Laws and the spirit of the game.[67]
",Cricket
"The match details, including runs and dismissals, are recorded by two official scorers, one representing each team. The scorers are directed by the hand signals of an umpire (see image, right). For example, the umpire raises a forefinger to signal that the batsman is out (has been dismissed); he raises both arms above his head if the batsman has hit the ball for six runs. The scorers are required by the Laws to record all runs scored, wickets taken and overs bowled; in practice, they also note significant amounts of additional data relating to the game.[82]
",Cricket
"A match's statistics are summarised on a scorecard. Prior to the popularisation of scorecards, most scoring was done by men sitting on vantage points cuttings notches on tally sticks and runs were originally called notches.[83] According to Rowland Bowen, the earliest known scorecard templates were introduced in 1776 by T. Pratt of Sevenoaks and soon came into general use.[84] It is believed that scorecards were printed and sold at Lord's for the first time in 1846.[85]
",Cricket
"Besides observing the Laws, cricketers must respect the ""Spirit of Cricket,"" which is the ""Preamble to the Laws,"" first published in the 2000 code, and updated in 2017, and now opens with this statement:[86]
",Cricket
"""Cricket owes much of its appeal and enjoyment to the fact that it should be played not only according to the Laws, but also within the Spirit of Cricket"".",Cricket
"The Preamble is a short statement that emphasises the ""Positive behaviours that make cricket an exciting game that encourages leadership,friendship and teamwork.""[87]
",Cricket
"The major responsibility for ensuring fair play is placed firmly on the captains, but extends to all players, umpires, teachers, coaches and parents involved.
",Cricket
"The umpires are the sole judges of fair and unfair play. They are required under the Laws to intervene in case of dangerous or unfair play or in cases of unacceptable conduct by a player.
",Cricket
"Previous versions of the Spirit identified actions that were deemed contrary (for example, appealing knowing that the batsman is not out) but all specifics are now covered in the Laws of Cricket, the relevant governing playing regulations and disciplinary codes, or left to the judgement of the umpires, captains, their clubs and governing bodies. The terse expression of the Spirit of Cricket now avoids the diversity of cultural conventions that exist on the detail of sportsmanship – or its absence.
",Cricket
"Most bowlers are considered specialists in that they are selected for the team because of their skill as a bowler, although some are all-rounders and even specialist batsmen bowl occasionally. The specialist bowlers are active multiple times during an innings, but may not bowl two overs consecutively. If the captain wants a bowler to ""change ends"", another bowler must temporarily fill in so that the change is not immediate.[81]
",Cricket
"A bowler reaches his delivery stride by means of a ""run-up"" and an over is deemed to have begun when the bowler starts his run-up for the first delivery of that over, the ball then being ""in play"".[81] Fast bowlers, needing momentum, take a lengthy run up while bowlers with a slow delivery take no more than a couple of steps before bowling. The fastest bowlers can deliver the ball at a speed of over 145 kilometres per hour (90 mph) and they sometimes rely on sheer speed to try and defeat the batsman, who is forced to react very quickly.[89] Other fast bowlers rely on a mixture of speed and guile by making the ball seam or swing (i.e. curve) in flight. This type of delivery can deceive a batsman into miscuing his shot, for example, so that the ball just touches the edge of the bat and can then be ""caught behind"" by the wicket-keeper or a slip fielder.[89] At the other end of the bowling scale is the spin bowler who bowls at a relatively slow pace and relies entirely on guile to deceive the batsman. A spinner will often ""buy his wicket"" by ""tossing one up"" (in a slower, steeper parabolic path) to lure the batsman into making a poor shot. The batsman has to be very wary of such deliveries as they are often ""flighted"" or spun so that the ball will not behave quite as he expects and he could be ""trapped"" into getting himself out.[90] In between the pacemen and the spinners are the medium paced seamers who rely on persistent accuracy to try and contain the rate of scoring and wear down the batsman's concentration.[89]
",Cricket
"There are ten ways in which a batsman can be dismissed: five relatively common and five extremely rare. The common forms of dismissal are bowled,[91] caught,[92] leg before wicket (lbw),[93] run out[94] and stumped.[95] Rare methods are hit wicket,[96] hit the ball twice,[97] obstructing the field,[98] handled the ball[99] and timed out.[100] The Laws state that the fielding team, usually the bowler in practice, must appeal for a dismissal before the umpire can give his decision. If the batsman is out, the umpire raises a forefinger and says ""Out!""; otherwise, he will shake his head and say ""Not out"".[101] There is, effectively, an eleventh method of dismissal, retired out, which is not an on-field dismissal as such but rather a retrospective one for which no fielder is credited.[102]
",Cricket
"Batsmen take turns to bat via a batting order which is decided beforehand by the team captain and presented to the umpires, though the order remains flexible when the captain officially nominates the team.[58] Substitute batsmen are not allowed.[73]
",Cricket
"A skilled batsman can use a wide array of ""shots"" or ""strokes"" in both defensive and attacking mode. The idea is to hit the ball to best effect with the flat surface of the bat's blade. If the ball touches the side of the bat it is called an ""edge"". The batsman does not have to play a shot and can allow the ball to go through to the wicketkeeper. Equally, he does not have to attempt a run when he hits the ball with his bat. Batsmen do not always seek to hit the ball as hard as possible, and a good player can score runs just by making a deft stroke with a turn of the wrists or by simply ""blocking"" the ball but directing it away from fielders so that he has time to take a run. A wide variety of shots are played, the batsman's repertoire including strokes named according to the style of swing and the direction aimed: e.g., ""cut"", ""drive"", ""hook"", ""pull"".[103]
",Cricket
"The batsman on strike (i.e. the ""striker"") must prevent the ball hitting the wicket, and try to score runs by hitting the ball with his bat so that he and his partner have time to run from one end of the pitch to the other before the fielding side can return the ball. To register a run, both runners must touch the ground behind the popping crease with either their bats or their bodies (the batsmen carry their bats as they run). Each completed run increments the score of both the team and the striker.[104]
",Cricket
"The decision to attempt a run is ideally made by the batsman who has the better view of the ball's progress, and this is communicated by calling: usually ""yes"", ""no"" or ""wait"". More than one run can be scored from a single hit: hits worth one to three runs are common, but the size of the field is such that it is usually difficult to run four or more.[104] To compensate for this, hits that reach the boundary of the field are automatically awarded four runs if the ball touches the ground en route to the boundary or six runs if the ball clears the boundary without touching the ground within the boundary. In these cases the batsmen do not need to run.[105] Hits for five are unusual and generally rely on the help of ""overthrows"" by a fielder returning the ball. If an odd number of runs is scored by the striker, the two batsmen have changed ends, and the one who was non-striker is now the striker. Only the striker can score individual runs, but all runs are added to the team's total.[104]
",Cricket
"Additional runs can be gained by the batting team as extras (called ""sundries"" in Australia) due to errors made by the fielding side. This is achieved in four ways: no-ball, a penalty of one extra conceded by the bowler if he breaks the rules;[106] wide, a penalty of one extra conceded by the bowler if he bowls so that the ball is out of the batsman's reach;[107] bye, an extra awarded if the batsman misses the ball and it goes past the wicket-keeper and gives the batsmen time to run in the conventional way;[108] leg bye, as for a bye except that the ball has hit the batsman's body, though not his bat.[108] If the bowler has conceded a no-ball or a wide, his team incurs an additional penalty because that ball (i.e., delivery) has to be bowled again and hence the batting side has the opportunity to score more runs from this extra ball.[106][107]
",Cricket
"Women's cricket was first recorded in Surrey in 1745.[109] International development began at the start of the 20th century and the first Test Match was played between Australia and England in December 1934.[110] The following year, New Zealand women joined them, and in 2007 Netherlands women became the tenth women's Test nation when they made their debut against South Africa women. In 1958, the International Women's Cricket Council was founded (it merged with the ICC in 2005).[110] In 1973, the first Cricket World Cup of any kind took place when a Women's World Cup was held in England.[110]  In 2005, the International Women's Cricket Council was merged with the International Cricket Council (ICC) to form one unified body to help manage and develop cricket. The ICC Women's Rankings were launched on 1 October 2015 covering all three formats of women's cricket. In October 2018 following the ICC's decision to award T20 International status to all members, the Women's rankings were split into separate ODI (for Full Members) and T20I lists.[111]
",Cricket
"The International Cricket Council (ICC), which has its headquarters in Dubai, is the global governing body of cricket. It was founded as the Imperial Cricket Conference in 1909 by representatives from England, Australia and South Africa, renamed the International Cricket Conference in 1965, and took up its current name in 1989.[110] The ICC in 2017 has 105 member nations, twelve of which hold full membership and can play Test cricket.[112] The ICC is responsible for the organisation and governance of cricket's major international tournaments, notably the men's and women's versions of the Cricket World Cup. It also appoints the umpires and referees that officiate at all sanctioned Test matches, Limited Overs Internationals and Twenty20 Internationals.
",Cricket
"Each member nation has a national cricket board which regulates cricket matches played in its country, selects the national squad, and organises home and away tours for the national team.[113] In the West Indies, which for cricket purposes is a federation of nations, these matters are addressed by Cricket West Indies.[114]
",Cricket
"The table below lists the ICC full members and their national cricket boards:[115]
",Cricket
"Cricket is a multi-faceted sport with multiple formats that can effectively be divided into first-class cricket, limited overs cricket and, historically, single wicket cricket. The highest standard is Test cricket (always written with a capital ""T"") which is in effect the international version of first-class cricket and is restricted to teams representing the twelve countries that are full members of the ICC (see above). Although the term ""Test match"" was not coined until much later, Test cricket is deemed to have begun with two matches between Australia and England in the 1876–77 Australian season; since 1882, most Test series between England and Australia have been played for a trophy known as The Ashes. The term ""first-class"", in general usage, is applied to top-level domestic cricket. Test matches are played over five days and first-class over three to four days; in all of these matches, the teams are allotted two innings each and the draw is a valid result.[117]
",Cricket
"Limited overs cricket is always scheduled for completion in a single day. There are two types: List A which normally allows fifty overs per team; and Twenty20 in which the teams have twenty overs each. Both of the limited overs forms are played internationally as Limited Overs Internationals (LOI) and Twenty20 Internationals (T20I). List A was introduced in England in the 1963 season as a knockout cup contested by the first-class county clubs. In 1969, a national league competition was established. The concept was gradually introduced to the other leading cricket countries and the first limited overs international was played in 1971. In 1975, the first Cricket World Cup took place in England. Twenty20 is a new variant of limited overs itself with the purpose being to complete the match within about three hours, usually in an evening session. The first Twenty20 World Championship was held in 2007. Limited overs matches cannot be drawn, although a tie is possible and an unfinished match is a ""no result"".[118][119]
",Cricket
"Single wicket was popular in the 18th and 19th centuries and its matches were generally considered top-class. In this form, although each team may have from one to six players, there is only one batsman in at a time and he must face every delivery bowled while his innings lasts. Single wicket has rarely been played since limited overs cricket began. Matches tended to have two innings per team like a full first-class one and they could end in a draw.[120]
",Cricket
"Most international matches are played as parts of 'tours', when one nation travels to another for a number of weeks or months, and plays a number of matches of various sorts against the host nation. Sometimes a perpetual trophy is awarded to the winner of the Test series, the most famous of which is The Ashes.
",Cricket
"The ICC also organises competitions that are for several countries at once, including the Cricket World Cup, ICC T20 World Cup and ICC Champions Trophy. A league competition for Test matches played as part of normal tours, the ICC World Test Championship, has been proposed several times, and is currently planned to begin in 2019. The ICC maintains Test rankings, ODI rankings and T20 rankings systems for the countries which play these forms of cricket.
",Cricket
"Competitions for member nations of the ICC with Associate status include the ICC Intercontinental Cup, for first-class cricket matches, and the World Cricket League for one-day matches, the final matches of which now also serve as the ICC World Cup Qualifier.
",Cricket
"First-class cricket in England is played for the most part by the 18 county clubs which contest the County Championship. The concept of a champion county has existed since the 18th century but the official competition was not established until 1890.[40] The most successful club has been Yorkshire, who had won 32 official titles (plus one shared) to 2017.[121]
",Cricket
"Australia established its national first-class championship in 1892–93 when the Sheffield Shield was introduced. In Australia, the first-class teams represent the various states.[122] New South Wales has the highest number of titles.
",Cricket
"The other ICC full members have national championship trophies called the Ahmad Shah Abdali 4-day Tournament (Afghanistan); the National Cricket League (Bangladesh); the Ranji Trophy (India); the Inter-Provincial Championship (Ireland); the Plunket Shield (New Zealand); the Quaid-e-Azam Trophy (Pakistan); the Currie Cup (South Africa); the Premier Trophy (Sri Lanka); the Shell Shield (West Indies); and the Logan Cup (Zimbabwe).
",Cricket
"The world's earliest known cricket match was a village cricket meeting in Kent which has been deduced from a 1640 court case recording a ""cricketing"" of ""the Weald and the Upland"" versus ""the Chalk Hill"" at Chevening ""about thirty years since"" (i.e., c. 1611). Inter-parish contests became popular in the first half of the 17th century and continued to develop through the 18th with the first local leagues being founded in the second half of the 19th.[17]
",Cricket
"At the grassroots level, local club cricket is essentially an amateur pastime for those involved but still usually involves teams playing in competitions at weekends or in the evening. Schools cricket, first known in southern England in the 17th century, has a similar scenario and both are widely played in the countries where cricket is popular.[123] Although there can be variations in game format, compared with professional cricket, the Laws are always observed and club/school matches are therefore formal and competitive events.[124] The sport has numerous informal variants such as French cricket.[125]
",Cricket
"Cricket has had a broad impact on popular culture, both in the Commonwealth of Nations and elsewhere. It has, for example, influenced the lexicon of these nations, especially the English language, with various phrases such as ""that's not cricket"" (that's unfair), ""had a good innings"" (lived a long life) and ""sticky wicket"". ""On a sticky wicket"" (aka ""sticky dog"" or ""glue pot"")[126] is a metaphor[127] used to describe a difficult circumstance. It originated as a term for difficult batting conditions in cricket, caused by a damp and soft pitch.[128]
",Cricket
"Cricket is the subject of works by noted English poets, including William Blake and Lord Byron.[129] Beyond a Boundary (1963), written by Trinidadian C. L. R. James, is often named the best book on any sport ever written.[130]
",Cricket
"In the visual arts, notable cricket paintings include Albert Chevallier Tayler's Kent vs Lancashire at Canterbury (1907) and Russell Drysdale's The Cricketers (1948), which has been called ""possibly the most famous Australian painting of the 20th century.""[131] French impressionist Camille Pissarro painted cricket on a visit to England in the 1890s.[129] Francis Bacon, an avid cricket fan, captured a batsman in motion.[129] Caribbean artist Wendy Nanan's cricket images[132] are featured in a limited edition first day cover for Royal Mail's ""World of Invention"" stamp issue, which celebrated the London Cricket Conference 1–3 March 2007, first international workshop of its kind and part of the celebrations leading up to the 2007 Cricket World Cup.[133]
",Cricket
"Cricket has close historical ties with Australian rules football and many players have competed at top levels in both sports.[134] In 1858, prominent Australian cricketer Tom Wills called for the formation of a ""foot-ball club"" with ""a code of laws"" to keep cricketers fit during the off-season. The Melbourne Football Club was founded the following year, and Wills and three other members codified the first laws of the game.[135] It is typically played on modified cricket fields.[136]
",Cricket
"In England, a number of association football clubs owe their origins to cricketers who sought to play football as a means of keeping fit during the winter months. Derby County was founded as a branch of the Derbyshire County Cricket Club in 1884;[137] Aston Villa (1874) and Everton (1876) were both founded by members of church cricket teams.[138] Sheffield United's Bramall Lane ground was, from 1854, the home of the Sheffield Cricket Club, and then of Yorkshire; it was not used for football until 1862 and was shared by Yorkshire and Sheffield United from 1889 to 1973.[139]
",Cricket
"In the late 19th century, a former cricketer, English-born Henry Chadwick of Brooklyn, New York, was credited with devising the baseball box score[140] (which he adapted from the cricket scorecard) for reporting game events. The first box score appeared in an 1859 issue of the Clipper.[141] The statistical record is so central to the game's ""historical essence"" that Chadwick is sometimes referred to as ""the Father of Baseball"" because he facilitated the popularity of the sport in its early days.[142]
",Cricket
"Related sports
",Cricket
"Organisations and competitions
",Cricket
"Statistics and records
",Cricket
"News and other resources
",Cricket
"
",Cricket
"Cricket is a multi-faceted sport with multiple formats, depending on the standard of play, the desired level of formality, and the time available. One of the main differences is between matches limited by time in which the teams have two innings apiece, and those limited by number of overs in which they have a single innings each. The former, known as first-class cricket if played at the senior level, has a scheduled duration of three to five days (there have been examples of ""timeless"" matches too); the latter, known as limited overs cricket because each team bowls a limit of typically 50 overs, has a planned duration of one day only. A separate form of limited overs is Twenty20, originally designed so that the whole game could be played in a single evening, in which each team has an innings limited to twenty overs.
",Cricket
"Double innings matches usually have at least six hours of playing time each day. Limited overs matches often last at least six hours; and Twenty20 matches are generally completed in under four hours. In a full day's play scheduled for at least six hours, there are formal intervals on each day for lunch and tea with brief informal breaks for drinks. There is also a short interval between innings.
",Cricket
"Local club cricket teams, which consist of amateur players, rarely play matches that last longer than a single day; these may loosely be divided into declaration matches, in which a specified maximum time or number of overs is assigned to the game in total and the teams swap roles only when the batting team is either completely dismissed or declares; and limited overs matches, in which a specified maximum number of overs is assigned for each team's innings individually. These will vary in length between 30 and 60 overs per side at the weekend and the 20-over format in the evenings. Indoor cricket is a variant of the sport played in sports halls during the winter months.
",Cricket
"At still lower levels, the rules are often changed simply to make the game playable with limited resources, or to render it more convenient and enjoyable for the participants. Informal variants of the sport are played in areas as diverse as sandy beaches and ice floes.
",Cricket
"Four forms of cricket have been played at what may be termed the highest international or domestic level of the game. Three are contested currently and one is historic. There is no official term for this level of cricket collectively, although the individual forms do have official designations and are defined by the International Cricket Council (ICC). In the past, before any official definition was agreed upon, highest standard matches were routinely described as ""great"" or ""important"" or ""top-class""; or even ""first-class"" before this became the official term for one type of cricket (see below).[1] Note that ""minor cricket"" is a term used officially in England and Wales at least.
",Cricket
"Matches played at the highest international and domestic levels are those in which players and/or teams of a recognised high standard are taking part. In modern domestic cricket, it includes first-class cricket, List A cricket and top-class Twenty20 competitions for both men and women. Test cricket, One Day Internationals (ODIs) and Twenty20 Internationals (T20Is) are variations of those forms within the international sphere. Historically (see History of cricket), top-class matches were those held by substantial sources to have historical significance including single wicket and those double innings matches without statistical significance: i.e., lacking scorecards and other statistical data.
",Cricket
"The oldest known English county teams are Kent, Surrey and Sussex, all of which have histories commencing in the early 18th century. These counties had achieved a high standard long before their modern county clubs were founded (from 1839 to 1845), and so they have always had first-class status.[2] Following a meeting in May 1894 of Marylebone Cricket Club (MCC) and the County Championship clubs, the concept of ""first-class cricket"" was officially defined.[3] By 1895, several other counties had also been recognised as having first-class status, as had MCC itself from its foundation in 1787.[2] Top-class limited overs cricket began in 1963 when the County Championship clubs took part in the first seasonal knockout tournament, which was won by Sussex. Hence, like all the other first-class counties, Sussex for example is classified as a List A team from 1963;[4] and as a top-class Twenty20 team since 2003.[5]
",Cricket
"First-class cricket is a form of the game in which teams of a recognised high standard compete. Test cricket is first-class at international level; the term ""first-class"" is habitually applied to domestic matches only, although a player's Test statistics are included in his overall first-class statistics. A first-class match must have eleven players per side, two innings apiece and a scheduled duration of at least three days. Historically, however, there have been instances of first-class matches being arranged for less than three days, and there have been others with twelve or thirteen players per side; these are exceptional cases and form a tiny percentage of the whole. If the game is not completed within the allotted time then it is drawn, regardless of who has scored the most runs when time expires. Limited overs matches in which the teams have only one innings each are not first-class (see List A and Twenty20 sections below) and these cannot result in a draw (they can, however, result in a tie or be declared a ""no result"").
",Cricket
"Test matches, other games between two Test nations, games between two domestic teams deemed first-class in countries holding full membership of the ICC, and games between a Test nation's national side (or a team drawn from a national touring squad) and a first-class domestic team from a Test nation, are deemed to be first-class. A match between a leading ICC associate member and another team adjudged first-class would be granted first-class status, but domestic matches in the associate member country are minor.
",Cricket
"The origin of the term ""first-class cricket"" is unknown but, along with other terms, it was used loosely for top-class eleven-a-side matches before it acquired its official status in 1894 (see above). Subsequently, at a meeting of the Imperial Cricket Conference (ICC) in May 1947, it was formally defined on a global basis. A key omission of both the MCC and ICC rulings was any attempt to define first-class cricket retrospectively and it was stipulated in the ICC ruling that the definition ""will not have retrospective effect"".[6] Many historians and statisticians have subjectively classified chosen pre-1895 matches as first-class but these are unofficial ratings and differences of opinion among the experts has led to variations in published cricket statistics. The main problem with ""first-class cricket"" is that it can be a misleading concept as it is essentially statistical and may typically ignore the historical aspect of a match if statistical information is missing, as is invariably the case with matches played up to 1825. Nevertheless, the recognition of any match as first-class by a substantial source qualifies it as such and it follows that the teams, venues and players involved in such matches before 1895 are the equivalent of first-class teams, venues and players since 1895. Substantial sources interested in 18th and 19th century cricket include Arthur Haygarth, F. S. Ashley-Cooper, H. T. Waghorn, G. B. Buckley, H. S. Altham, Roy Webber, John Arlott, Bill Frindall, the ACS and various internet sites (see Historical sources). Writing in 1951, Roy Webber drew a line between what is important historically and what should form part of the statistical record when he argued that the majority of matches prior to 1864 (i.e., the year in which overarm bowling was legalised) ""cannot be regarded as (statistically) first-class"" and their records are used ""for their historical associations"".[7]
",Cricket
"Limited overs cricket played with 40 to 60 overs per team, known statistically as List A cricket, is a second form of cricket which differs from first-class as the teams play one innings each and are allowed a maximum number of overs per innings. Matches are scheduled for completion in a single day's play, though they can in theory continue into a second day if impacted by bad weather. Most cricketing nations have some form of domestic List A competition. The over limits range from forty to sixty. The categorisation of ""List A"" was only endorsed by the ICC in 2006; the Association of Cricket Statisticians and Historians created it for the purpose of providing a parallel to first-class cricket in their record books.
",Cricket
"Twenty20 is a separate form of limited overs cricket and is not part of List A. It is a third form of cricket originally devised in England in 2003. The teams have one innings each in which the maximum number of overs is twenty. Twenty20 competitions are held internationally and there are domestic championships in all the main cricketing nations.
",Cricket
"The Indian Premier League is a professional Twenty20 cricket league in India contested during April and May of every year by franchise teams representing Indian cities. The league was founded by the Board of Control for Cricket in India (BCCI) in 2007. The IPL is the most-attended cricket league in the world and ranks sixth among all sports leagues.
",Cricket
"T10 format is an evolution of cricket, from 50-overs per side games, to 20-overs, and now to 10. It was first played in 2017 from 14-17 December at the Sharjah Cricket Stadium, approved by the Emirates Cricket Board[citation needed] in a professional cricket league owned and launched by T10 Sports Management[citation needed]. Each team has one innings of 10 overs per game, limited to 90 minutes. The league is played in a round robin format that is followed by the semifinals and the final. If there is a tie, the result is decided based on Super Over. In August 2018, International Cricket Council (ICC) officially sanctioned the second season of T10 to be held in Sharjah starting November 23, 2018 [8]. 6 teams will compete.
",Cricket
"There are numerous forms of cricket which, although they are not played professionally or at a recognised high standard, are still popular as common formats of amateur cricket. The double innings, limited overs, Twenty20 and single wicket forms are played by amateur teams: for example, Grade cricket in Australia and the Minor Counties Cricket Championship in England and Wales play the double innings form.
",Cricket
"Club cricket, by far and away the widest form of cricket played worldwide, is largely amateur, but still formal, cricket, with the teams organised into leagues. The games are sometimes limited-overs, with each innings usually lasting between twenty and fifty overs. Other matches are played to time restrictions. Restrictions in overs or time may be placed on each side individually, or they may stipulate the total length of the match. The latter more traditional case is often known as declaration cricket.
",Cricket
"Club cricket is played extensively in cricketing nations, and also by immigrants from cricketing nations. Club cricket most often takes place on a natural grass wicket, often maintained by the players themselves, although at a lower level it may take place on an artificial turf pitch, though the rest of actual field will be natural grass.
",Cricket
"This is the most traditional version of cricket, with rules most closely replicating the original rules of cricket from the 16th and 17th century. It is a single innings game with a set time limit for the entire game to be completed in. To win the game, a side must both score the highest aggregate number of runs and take all ten of the opposition wickets. It is up to the side batting first to declare when they feel they have enough runs to be able to win the match. In this format of cricket, if the side batting second do not lose all ten of their wickets, the match is said to have ended in a draw.
",Cricket
"Declaration cricket is generally played over a single day, although two day games lasting an entire weekend are also common. This format is often seen as ""old-fashioned"" and is typically used for friendly matches rather than in organised league play.
",Cricket
"Cricket is also played in several different shortened forms, designed to pack as much action as possible into an hour or two, enabling them to be played as a single contest in an evening, or as a series of multiple contests between different teams that cover the entire day. Such forms have evolved since the 1980s, and take cricket an additional step beyond one-day cricket. Most forms will resemble twenty-twenty cricket in nature, although shorter formats with reduced numbers of players, typically 6-aside or 8-aside, are also common for tournament play.
",Cricket
"Indoor cricket is a format of the game designed to be played in an indoor sports hall at times of the year when outdoor play is not possible. There are two recognised forms of indoor cricket. The traditional version played with a hard ball is popular in the UK. This format is played with six players per side and features modified rules designed specifically for indoor play. A soft ball version is played by junior cricketers in the UK and is also popular amongst adults in the Southern Hemisphere.
",Cricket
"A match in which, as the name implies, there is a single batsman at any time. It is probably the oldest form of cricket as, at its most basic level, it involves one player against another. Historically, its matches were top-class and it has known periods of huge popularity, especially in the mid-18th century when it was the most popular form of cricket thanks to its gambling associations, and in the first half of the 19th century. Matches can involve teams with a single player only but the lucrative 18th century games were mostly between teams of three to five players known as ""threes"", ""fours"" or ""fives"". Only those players designated as team members can bat or bowl but it is normal to have the full quota of fielders including a wicket-keeper.
",Cricket
"Double-wicket or ""pairs"" cricket is a form of cricket with two teams of two players each which are pitched against each other for a limited number of overs. A player getting out in this form of cricket does not retire but continues to bat but is penalised a stipulated number of runs for each time he gets out.
",Cricket
"It is a simplified, high-speed version of the game played on a small pitch with plastic equipment, aimed mainly at encouraging youngsters to take part.[9]
",Cricket
"Backyard cricket, Beach cricket, Street cricket are all different names used to describe a wide range of related informal games. The rules are often ad hoc, and the laws of cricket, such as those involving leg before wicket, penalty runs, and others, are ignored or modified to suit both the setting and participants' preferences. In India and Pakistan, there is Gali cricket ('gali' in Hindi means 'street'. It is pronounced as 'gully' but should not be confused with the fielding position). Often, there are no teams, and each player plays for himself, and fields when he is not batting. Often, there is one wicket, and one bowling position, and no overs. If the batsman runs an odd number of runs, he is allowed to walk back to the wicket before the next ball is bowled.[citation needed]
",Cricket
"Informal cricket in the UK is often known as garden cricket and is played in gardens and recreation grounds around the country. Because of limited space in gardens and the potential damage to property, one particular version of garden cricket is unique in that there are no concept of runs as attacking shots are expressly forbidden, and instead the winning batsman is the one who can survive the longest number of deliveries. Typically this will be played with a tennis ball or other soft bouncy ball, and modified rules, such as one hand one bounce are often employed. The length of the wicket will typically be roughly 15 metres, and the non-bowling fielders will be encircled close round the bat looking for a catching chance.
",Cricket
"It is a game in which the ball is bowled at the legs of the batsman, with the batsman's legs forming the wicket. It is often played by children. A tennis ball is often used rather than the harder cricket ball. Much like beach cricket, the rules may vary wildly.[citation needed]
",Cricket
"This type of cricket is popular in the South Asian sub-continent, USA and Canada. In this game a harder version of tennis ball is used. The number of overs in the game varies from 6 to 25 overs. Considering that the ball is not as hard as the professional cricket ball, the use of protective gear like gloves, pads and helmets is optional. As tennis ball cricket games are shorter when compared to the conventional version, it suits the US and Canadian lifestyle where one would see a large number of people participating. Where cricket pitches are not available, part of a baseball diamond is used as a pitch in most parts of USA and Canada.[citation needed]
",Cricket
"This type of cricket is popular in Pakistan, Bangladesh and somewhat gaining popularity in other South Asian countries and Europe due to the export of the idea from Pakistan. Tape ball cricket has been a cricket culture in Pakistan since the 1980s. Pakistanis who have settled in the west have introduced this theme and have tape ball leagues throughout UK, USA, and Canada. In this game a tennis ball is covered with insulating tape. This results in a heavier ball. Fast bowlers can generate extra swing in both directions while finger spinners can produce turn. The game is usually a limited over match with 4–12 overs. In Karachi and Lahore regular tournaments are held. Night matches are common, especially during the month of Ramadan.[10]
",Cricket
"Also known as Kirikiti, or Samoan Cricket, it is the national game of Samoa and is especially popular in New Zealand. The game is descended from the cricket brought to Samoa by British missionaries; teams of unlimited size follow rules opaque to outside observers in a game/dance/feast event that can last several days.[citation needed]
",Cricket
"Continuous cricket is a game involving one batsman, who upon hitting the ball, must run to a marker, which is square of the wicket. The bowler may bowl as soon as the ball is returned, regardless of whether or not the batsman is still running. The game can be played in teams, or as a group, where players rotate between fielding positions, batting and bowling.[11]
",Cricket
"Founded in Australia, it is for those over 60 years of age, slightly modified from the standard.[citation needed]
",Cricket
"It is a form of cricket that also resembles baseball, mainly played by women.[citation needed]
",Cricket
"""La plaquita"" ('The little plate') or ""la placa"" ('The plate') is an obscure variation, played in the streets of Caribbean countries such as the Dominican Republic between two couples, usually making use of broomsticks as bats, rubber or tennis balls, and old licence plates as wickets (with their ends twisted to make them stand up). The game is divided in alternate 3-out innings as in baseball. The first team to reach 100 or 200 runs wins. A similar version is played on the streets of Brazil and is known as bats or taco ('taco' being Portuguese for 'bat').[12]
",Cricket
"It is a peculiar form of cricket played in the Trobriand Islands, in Papua New Guinea. Although cricket was introduced by the British as part of colonial agenda, it was adopted into local Trobriand culture and many modifications and cultural adaptations were made over the years. Some of these include: under-arm bowling; outs are celebrated with dances; the ""home"" team (the tribal community which organised a match) always wins; any number of players can take part in a match; players dress in traditional war costumes.[citation needed]
",Cricket
"Table Cricket is an indoor version of the game designed primarily for physically challenged cricketers.
",Cricket
"A card game based on cricket. See main article.
",Cricket
"Popular with families, especially those with younger children. This type of cricket is everyone against the batsman, with the person who gets the batsman out becoming the new batsman.
",Cricket
"It is popular with school children in India, Pakistan and Sri Lanka, specially the Backbenchers occupying the last few rows, who most often play it sneakily during the ongoing class, a favorite childhood nostalgia of adults to reminisce about. It has several variants and is mostly played by two players. The runs are scored by flipping the book open at random and the last digit of the right-side (even-numbered) page is counted as the number of runs scored. 0 (and sometimes 8) are assigned to special rules, typically a wicket is lost when a person scores 0 and scoring 8 would be substituted for a No ball run and an additional chance. To give an example, if the batting side opened the book at page 26, then 6 runs would be scored. For the toss, what is generally done is that both the players open a page and the one whose last digit is greater wins.
",Cricket
"Yet another version of cricket appeared during the 1950s in the UK in the Eagle comic. A page was chosen and each letter or symbol was counted according to a formula. This produced a remarkably realistic scorecard with the majority of innings around 150 to 300 scored at about 4 runs per over.
",Cricket
"This form is popular with school children (usually older ones who require scientific calculators for maths and science).
",Cricket
"A player starts by clearing the memory on their calculator. The player will then use the random number generator on their calculator to bring up a number between 0 and 1. The number of runs scored is the first digit after the decimal point (for example, if the random number generator provides 0.521, 5 runs are scored). Scoring is kept by using the memory addition function on the calculator, or by pen and paper. Scoring a 0 is considered out. The player who has the highest score wins.
",Cricket
"Galli cricket, which literally means lane cricket, is a type of the backyard cricket popular in India, Bangladesh, Pakistan and Sri Lanka, except that it is often played on the busy narrow inner lanes, and sometimes even on the main roads, of the suburbs and cities, shared with pedestrians, rickshaws, vehicular traffic, cows, pets and stray animals with cacophony of sights, sounds and colors. It is often played with the soft tennis ball, improvised wickets and bat, where rules are flexible, and hitting in the air or the to the wall, specially on someone's window pane glass is considered out. Kids would often get a scolding from the neighbors whose window pane glasses are broken, thus scoring rules are designed to keep the ball on the ground.[13]
",Cricket
"
",Cricket
"Hand cricket is widely popular among the school children in India, Sri Lanka and Pakistan. It is played through gestures (called 'throws'), each of them signifying a number, that are made simultaneously by two players, each using one hand, by locking the wrist, they simultaneously try to immediately release the certain number of fingers (as determined by the each player) before the other player does. Rather than calling it Hand Cricket, children prefer to call it 'Odd-Eve'. There are many variants in the game it self. T20s, tournaments, single wicket, and many more. For Tournaments, there needs to be at least three teams playing. 
There are different limits for hand throws. The most common is up to six. But there are children who play up to 10, which is the maximum you can have. 
Rules are simple; 
1. Both the batsman and the bowler have to throw at the same time, or else it wouldn't be counted as a run. 
2. There have to be overs, or else there should only be the captain bowling. This should be decided between the team players.
3. There should be a team up to 4-5 players. As in my school, we have a way selecting players by the face of hand, it is quite difficult to choose when there are a lot of hands. 
4. The players shouldn't be demotivated. the game is of luck, not skill. We don't have the skill of telepathy to know what is the other player thinking. Hence, the players shouldn't be at fault. The player might be playing good at one instance, and bad at the other.
",Cricket
"Hand cricket is a very interesting game, as it also helps children to add numbers quickly and without taking time to calculate.
",Cricket
"A one-person game played with pencils marked by hand to function as 'long dice'. A Japanese variant of these for use in other games are called 'battle pencils'.[14] It may also simply be played with conventional dice. The aim is to generate scores and attribute them to imaginary players and teams by compiling a scorecard. The game has been marketed commercially featuring plastic or metal long dice (rollers) and playing rules.[15]
",Cricket
"Also called car cricket. A travel game based on the names of public houses passed on the route. Runs are scored according to the number of legs, arms or other items featured in the pub name. The exact rules vary according to the participants. See main article.'
",Cricket
"Test cricket is the longest form of the sport of cricket and is considered its highest standard.[1][2] Test matches are played between national representative teams with ""Test status"", as determined and conferred by the International Cricket Council (ICC). The two teams of 11 players each play a four-innings match, which may last up to five days (or longer in some historical cases). It is generally considered the most complete examination of teams' playing ability and endurance.[3][4][5] The name Test stems from the long, gruelling match being both mentally and physically testing.[6]
",Cricket
"The first officially recognised Test match took place on 15–19 March 1877 and was played between England and Australia at the Melbourne Cricket Ground (MCG), where Australia won by 45 runs.[7] A Test match to celebrate 100 years of Test cricket was held in Melbourne on 12–17 March 1977, in which Australia beat England by 45 runs—the same margin as that first Test.[8]
",Cricket
"In October 2012, the ICC recast the playing conditions for Test matches, permitting day/night Test matches.[9] The first day/night game took place between Australia and New Zealand at the Adelaide Oval, Adelaide, on 27 November–1 December 2015.[10]
",Cricket
"The equivalent for women is Women's Test cricket, which is played over four days with slight differences in format from men's Tests.
",Cricket
"
",Cricket
"Test matches are the highest level of cricket, although, statistically, their data form part of first-class cricket. Matches are played between national representative teams with ""Test status"", as determined by the International Cricket Council. As of  June 2017[update], twelve national teams have Test status, the most recently promoted being Afghanistan and Ireland on 22 June 2017.[11] Zimbabwe's Test status was voluntarily suspended, because of poor performances between 2006 and 2011; it returned to competition in August 2011.[12]
",Cricket
"In January 2014, during an ICC meeting in Dubai, the pathway for new potential Test nations was laid out with the winners of the next round of the ICC Intercontinental Cup playing a 5-day match against the bottom ranked Test nation. If the Associate team defeats the Test nation, then they could be added as the new Test country and granted full membership.[13]
",Cricket
"A list of matches, defined as ""Tests"", was first drawn up by Australian Clarence Moody in the mid-1890s. Representative matches played by simultaneous England touring sides of 1891–92 (in Australia and South Africa) and 1929–30 (in the West Indies and New Zealand) are deemed to have ""Test status"".
",Cricket
"In 1970, a series of five ""Test matches"" was played in England between England and a Rest of the World XI. These matches, originally scheduled between England and South Africa, were amended after South Africa was suspended from international cricket because of their government's policy of apartheid. Although initially given Test status (and included as Test matches in some record books, including Wisden Cricketers' Almanack), this was later withdrawn and a principle was established that official Test matches can only be between nations (although the geographically and demographically small countries of the West Indies have since 1928 been permitted to field a coalition side). Despite this, in 2005, the ICC ruled that the six-day Super Series match that took place in October 2005, between Australia and a World XI, was an official Test match. Some cricket writers and statisticians, including Bill Frindall, ignored the ICC's ruling and excluded the 2005 match from their records. The series of ""Test matches"" played in Australia between Australia and a World XI in 1971/72 do not have Test status. The commercial ""Supertests"" organised by Kerry Packer as part of his World Series Cricket enterprise and played between ""WSC Australia"", ""WSC World XI"" and ""WSC West Indies"" from 1977 to 1979 have never been regarded as official Test matches.
",Cricket
"There are currently twelve Test-playing men's teams. The teams all represent individual, independent nations, except for England, the West Indies, and Ireland. Test status is conferred upon a country or group of countries by the International Cricket Council. Teams that do not have Test status can play in the ICC Intercontinental Cup, specifically designed to allow non-Test teams to play under conditions similar to Tests. The teams are listed below with the date of each team's Test debut:
",Cricket
"In the mid 2010s, the ICC evaluated proposals for dividing Test cricket into two tiers, with promotion and relegation between Tier-1 and Tier-2. These proposals were supported by some national cricket govening bodies,[14][15] but opposed by others.[16][17] These proposals were ultimately not implemented.[18][19][20]
",Cricket
"
",Cricket
"A standard day of Test cricket consists of three sessions of two hours each, the breaks between sessions being 40 minutes for lunch and 20 minutes for tea. However the times of sessions and intervals may be altered in certain circumstances: if bad weather or a change of innings occurs close to a scheduled break, the break may be taken immediately; if there has been a loss of playing time, for example because of bad weather, the session times may be adjusted to make up the lost time; if the batting side is nine wickets down at the scheduled tea break, then the interval may be delayed until either 30 minutes has elapsed or the team is all out;[21] the final session may be extended by up to 30 minutes if 90 or more overs have not been bowled in that day's play (subject to any reduction for adverse weather);[22] the final session may be extended by 30 minutes (except on the 5th day) if the umpires believe the result can be decided within that time.[23]
",Cricket
"Today, Test matches are scheduled to be played across five consecutive days. However, in the early days of Test cricket, matches were played for three or four days. Four-day Test matches were last played in 1973, between New Zealand and Pakistan.[24] Until the 1980s, it was usual to include a 'rest day,' often a Sunday. There have also been 'Timeless Tests', which did not end after a predetermined maximum time. In 2005, Australia played a match scheduled for six days against a World XI, which the ICC sanctioned as an official Test match, though the match reached a conclusion on the fourth day. In October 2017, the ICC approved a request for a four-day Test match, between South Africa and Zimbabwe, which started on 26 December 2017 and ended on the second day, 27 December.[25] The ICC will trial the four-day Test format until the 2019 Cricket World Cup.[26]
",Cricket
"There have been attempts by the ICC, the sport's governing body, to introduce day-night Test matches.[27] In 2012, The International Cricket Council passed playing conditions that allowed for the staging of day-night Test matches.[9] The first day-night Test took place during New Zealand's tour to Australia in November 2015.[10]
",Cricket
"Test cricket is played in innings (the word denotes both the singular and the plural). In each innings, one team bats and the other bowls (or fields). Ordinarily four innings are played in a Test match, and each team bats twice and bowls twice. Before the start of play on the first day, the two team captains and the match referee toss a coin; the captain who wins the toss decides whether his team will bat or bowl first.
",Cricket
"In the following scenarios, the team that bats first is referred to as Team A and their opponents as Team B.
",Cricket
"Usually the teams will alternate at the completion of each innings. Thus, Team A will bat (and Team B will bowl) until its innings ends, and then Team B will bat and Team A will bowl. When Team B's innings ends, Team A begin their second innings, and this is followed by Team B's second innings. The winning team is the one that scores more runs in their two innings.
",Cricket
"A team's innings ends in one of the following ways:[28]
",Cricket
"If, at the completion of its first innings, Team B's first innings total is 200 or more fewer than Team A's, the captain of Team A may (but is not required to) order Team B to have their second innings next. This is called enforcing the follow on.[29] In this case, the usual order of the third and fourth innings is reversed: Team A will bat in the fourth innings. It is rare for a team forced to follow on to win the match. In Test cricket it has only happened three times, although over 285 follow-ons have been enforced: Australia was the losing team on each occasion, twice to England, in 1894 and in 1981, and once to India in 2001.[30]
",Cricket
"If the whole of the first day's play of a Test match has been lost because of bad weather or other reasons like bad light, then Team A may enforce the follow on if Team B's first innings total is 150 or more fewer than Team A's. During the 2nd Test between England and New Zealand at Headingley in 2013, England batted first after the first day was lost because of rain.[31] New Zealand, batting second, scored 180 runs fewer than England, meaning England could have enforced the follow on, though chose not to. This is similar to four-day first-class cricket, where the follow on can be enforced if the difference is 150 runs or fewer. If the Test is 2 days or fewer then the ""follow-on"" value is 100 runs.
",Cricket
"After 80 overs, the captain of the bowling side may take a new ball, although this is not required.[32] The captain will usually take the new ball: being harder and smoother than an old ball, a new ball generally favours faster bowlers who can make it bounce more variably. The roughened, softer surface of an old ball can be more conducive to spin bowlers, or those using reverse swing. The captain may delay the decision to take the new ball if he wishes to continue with his spinners (because the pitch favours spin). After a new ball has been taken, should an innings last a further 80 overs, then the captain will have the option to take another new ball.
",Cricket
"A Test match may end in one of six results:
",Cricket
"Test cricket is almost always played as a series of matches between two countries, with all matches in the series taking place in the same country (the host). Often there is a perpetual trophy that is awarded to the winner, the most famous of which is the Ashes contested between England and Australia. There have been two exceptions to the bilateral nature of Test cricket: the 1912 Triangular Tournament, a three-way competition between England, Australia and South Africa (hosted by England), and the Asian Test Championship, an event held in 1998–99 and 2001–02.
",Cricket
"The number of matches in Test series has varied from one to seven.[39] Up until the early 1990s,[40] Test series between international teams were organised between the two national cricket organisations with umpires provided by the home team. With the entry of more countries into Test cricket, and a wish by the ICC to maintain public interest in Tests in the face of the popularity of one-day cricket, a rotation system was introduced that sees all ten Test teams playing each other over a six-year cycle, and an official ranking system (with a trophy held by the highest-ranked team). In this system, umpires are provided by the ICC. An elite panel of eleven umpires has been established, and the panel is supplemented by an additional International Panel that includes three umpires named by each Test-playing country. The elite umpires officiate almost all Test matches, usually not Tests involving their home country.
",Cricket
"
",Cricket
"There has been no World Cup for Test Match conducted thus far by the ICC, since it is the longest format  of the game, due to which the Championship might go on for years together. However, of late, the ICC is holding discussions to conduct a World Test Championship over a span of 2 years (starting its first Championship from 2019-2021 ), after failing to execute their plan of conducting this tournament twice, once during 2013 and another time during 2017. The schedule for 2019–21 Test Championship is a set of typical bilateral series in various countries put together, where one team is the host and other team is the visitor. The length of the series would vary between 2 matches to 5 matches. The result of each match carries a weighting of the points depending upon the length of the series: a single match in a 5-match series will account for 20% of the series points, while a match in a 2-match series will account for 50% of the series points.[original research?]
",Cricket
"Sides designated as ""England"" began to play in the late 18th century, but these teams were not truly representative. Early international cricket was disrupted by the French Revolution and the American Civil War. The earliest international cricket match was between USA and Canada, on 24 and 25 September 1844.[41] This has never been officially considered a ""Test match"". Tours of national English sides abroad took place, particularly to the US, Australia and New Zealand. The Australian Aborigines team became the first organised overseas cricketers to tour England in 1868.
",Cricket
"Two rival English tours of Australia were proposed in the early months of 1877, with James Lillywhite campaigning for a professional tour and Fred Grace for an amateur one. Grace's tour fell through and it was Lillywhite's team that toured New Zealand and Australia in 1876–77. Two matches against a combined Australian XI were later classified as the first official Test matches. The first match was won by Australia, by 45 runs, and the second by England. After reciprocal tours established a pattern of international cricket, The Ashes was established as an ongoing competition during the Australian tour of England in 1882. Surprisingly beaten, a mock obituary of English cricket was published in the Sporting Times the following day: the phrase ""The body shall be cremated and the ashes taken to Australia"" prompted the subsequent creation of the Ashes urn. The series of 1884–85 was the first to be held over five matches: Shaw, writing in 1901, considered the side to be ""the best ever to have left England"".
",Cricket
"South Africa became the third team to play Test cricket in 1888–89, when they hosted a tour by an under-strength England side.
",Cricket
"The following are the perpetual trophies in Test cricket.
",Cricket
"
",Cricket
"
",Cricket
"Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 20-metre (22-yard) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The batting side scores runs by striking the ball bowled at the wicket with the bat, while the bowling and fielding side tries to prevent this and dismiss each player (so they are ""out""). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side catching the ball after it is hit by the bat, but before it hits the ground. When ten players have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches. They communicate with two off-field scorers who record the match's statistical information.
",Cricket
"There are various formats ranging from Twenty20, played over a few hours with each team batting for a single innings of 20 overs, to Test matches, played over five days with unlimited overs and the teams each batting for two innings of unlimited length. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid spheroid made of compressed leather with a slightly raised sewn seam enclosing a cork core which is layered with tightly wound string.
",Cricket
"Historically, cricket's origins are uncertain and the earliest definite reference is in south-east England in the middle of the 16th century. It spread globally with the expansion of the British Empire, leading to the first international matches in the second half of the 19th century. The game's governing body is the International Cricket Council (ICC), which has over 100 members, twelve of which are full members who play Test matches. The game's rules are held in a code called the Laws of Cricket which is owned and maintained by Marylebone Cricket Club (MCC) in London. The sport is followed primarily in the Indian subcontinent, Australasia, the United Kingdom, Ireland, southern Africa and the West Indies, its globalisation occurring during the expansion of the British Empire and remaining popular into the 21st century.[1] Women's cricket, which is organised and played separately, has also achieved international standard. The most successful side playing international cricket is Australia, having won seven One Day International trophies, including five World Cups, more than any other country, and having been the top-rated Test side more than any other country.
",Cricket
"
",Cricket
"Cricket is one of many games in the ""club ball"" sphere that basically involve hitting a ball with a hand-held implement; others are baseball, golf, hockey, tennis, squash, and table tennis.[2] In cricket's case, a key difference is the existence of a solid target structure, the wicket (originally, it is thought, a ""wicket gate"" through which sheep were herded), that the batsman must defend.[3] The cricket historian Harry Altham identified three ""groups"" of ""club ball"" games: the ""hockey group"", in which the ball is driven to and fro between two targets (the goals); the ""golf group"", in which the ball is driven towards an undefended target (the hole); and the ""cricket group"", in which ""the ball is aimed at a mark (the wicket) and driven away from it"".[4]
",Cricket
"It is generally believed that cricket originated as a children's game in the south-eastern counties of England, sometime during the medieval period.[3] Although there are claims for prior dates, the earliest definite reference to cricket being played comes from evidence given at a court case in Guildford on Monday, 17 January 1597 (Julian calendar; equating to 30 January 1598 in the Gregorian calendar). The case concerned ownership of a certain plot of land and the court heard the testimony of a 59-year-old coroner, John Derrick, who gave witness that:[5][6][7]
",Cricket
"""Being a scholler in the ffree schoole of Guldeford hee and diverse of his fellows did runne and play there at creckett and other plaies"".",Cricket
"Given Derrick's age, it was about half a century earlier when he was at school and so it is certain that cricket was being played c. 1550 by boys in Surrey.[7] The view that it was originally a children's game is reinforced by Randle Cotgrave's 1611 English-French dictionary in which he defined the noun ""crosse"" as ""the crooked staff wherewith boys play at cricket"" and the verb form ""crosser"" as ""to play at cricket"".[8][9]
",Cricket
"One possible source for the sport's name is the Old English word ""cryce"" (or ""cricc"") meaning a crutch or staff. In Samuel Johnson's Dictionary, he derived cricket from ""cryce, Saxon, a stick"".[5] In Old French, the word ""criquet"" seems to have meant a kind of club or stick.[10] Given the strong medieval trade connections between south-east England and the County of Flanders when the latter belonged to the Duchy of Burgundy, the name may have been derived from the Middle Dutch (in use in Flanders at the time) ""krick""(-e), meaning a stick (crook).[10] Another possible source is the Middle Dutch word ""krickstoel"", meaning a long low stool used for kneeling in church and which resembled the long low wicket with two stumps used in early cricket.[11] According to Heiner Gillmeister, a European language expert of Bonn University, ""cricket"" derives from the Middle Dutch phrase for hockey, met de (krik ket)sen (i.e., ""with the stick chase"").[12] Gillmeister has suggested that not only the name but also the sport itself may be of Flemish origin.[12]
",Cricket
"Although the main object of the game has always been to score the most runs, the early form of cricket differed from the modern game in certain key technical aspects. The ball was bowled underarm by the bowler and all along the ground towards a batsman armed with a bat that, in shape, resembled a hockey stick; the batsman defended a low, two-stump wicket; and runs were called ""notches"" because the scorers recorded them by notching tally sticks.[13][14][15]
",Cricket
"In 1611, the year Cotgrave's dictionary was published, ecclesiastical court records at Sidlesham in Sussex state that two parishioners, Bartholomew Wyatt and Richard Latter, failed to attend church on Easter Sunday because they were playing cricket. They were fined 12d each and ordered to do penance.[16] This is the earliest mention of adult participation in cricket and it was around the same time that the earliest known organised inter-parish or village match was played – at Chevening, Kent.[5][17] In 1624, a player called Jasper Vinall died after he was accidentally struck on the head during a match between two parish teams in Sussex.[18]
",Cricket
"Cricket remained a low-key local pursuit for much of the century.[9] It is known, through numerous references found in the records of ecclesiastical court cases, to have been proscribed at times by the Puritans before and during the Commonwealth.[19][20] The problem was nearly always the issue of Sunday play as the Puritans considered cricket to be ""profane"" if played on the Sabbath, especially if large crowds and/or gambling were involved.[21][22]
",Cricket
"According to the social historian Derek Birley, there was a ""great upsurge of sport after the Restoration"" in 1660.[23] Gambling on sport became a problem significant enough for Parliament to pass the 1664 Gambling Act, limiting stakes to £100 which was in any case a colossal sum exceeding the annual income of 99% of the population.[23] Along with prizefighting, horse racing and blood sports, cricket was perceived to be a gambling sport.[24] Rich patrons made matches for high stakes, forming teams in which they engaged the first professional players.[25] By the end of the century, cricket had developed into a major sport which was spreading throughout England and was already being taken abroad by English mariners and colonisers – the earliest reference to cricket overseas is dated 1676.[26] A 1697 newspaper report survives of ""a great cricket match"" played in Sussex ""for fifty guineas apiece"" – this is the earliest known match that is generally considered top-class.[27][28]
",Cricket
"The patrons, and other players from the social class known as the ""gentry"", began to classify themselves as ""amateurs""[fn 1] to establish a clear distinction vis-à-vis the professionals, who were invariably members of the working class, even to the point of having separate changing and dining facilities.[29] The gentry, including such high-ranking nobles as the Dukes of Richmond, exerted their honour code of noblesse oblige to claim rights of leadership in any sporting contests they took part in, especially as it was necessary for them to play alongside their ""social inferiors"" if they were to win their bets.[30] In time, a perception took hold that the typical amateur who played in first-class cricket, until 1962 when amateurism was abolished, was someone with a public school education who had then gone to one of Cambridge or Oxford University – society insisted that such people were ""officers and gentlemen"" whose destiny was to provide leadership.[31] In a purely financial sense, the cricketing amateur would theoretically claim expenses for playing while his professional counterpart played under contract and was paid a wage or match fee; in practice, many amateurs claimed somewhat more than actual expenditure and the derisive term ""shamateur"" was coined to describe the syndrome.[32][33]
",Cricket
"The game underwent major development in the 18th century to become England's national sport.[citation needed] Its success was underwritten by the twin necessities of patronage and betting.[34] Cricket was prominent in London as early as 1707 and, in the middle years of the century, large crowds flocked to matches on the Artillery Ground in Finsbury.[citation needed] The single wicket form of the sport attracted huge crowds and wagers to match, its popularity peaking in the 1748 season.[35] Bowling underwent an evolution around 1760 when bowlers began to pitch the ball instead of rolling or skimming it towards the batsman. This caused a revolution in bat design because, to deal with the bouncing ball, it was necessary to introduce the modern straight bat in place of the old ""hockey stick"" shape.[36][citation needed]
",Cricket
"The Hambledon Club was founded in the 1760s and, for the next twenty years until the formation of Marylebone Cricket Club (MCC) and the opening of Lord's Old Ground in 1787, Hambledon was both the game's greatest club and its focal point.[citation needed] MCC quickly became the sport's premier club and the custodian of the Laws of Cricket. New Laws introduced in the latter part of the 18th century included the three stump wicket and leg before wicket (lbw).[37]
",Cricket
"The 19th century saw underarm bowling superseded by first roundarm and then overarm bowling. Both developments were controversial.[38] Organisation of the game at county level led to the creation of the county clubs, starting with Sussex in 1839.[39] In December 1889, the eight leading county clubs formed the official County Championship, which began in 1890.[40]
",Cricket
"The most famous player of the 19th century was W. G. Grace, who started his long and influential career in 1865. It was especially during the career of Grace that the distinction between amateurs and professionals became blurred by the existence of players like him who were nominally amateur but, in terms of their financial gain, de facto professional. Grace himself was said to have been paid more money for playing cricket than any professional.[citation needed]
",Cricket
"The last two decades before the First World War have been called the ""Golden Age of cricket"". It is a nostalgic name prompted by the collective sense of loss resulting from the war, but the period did produce some great players and memorable matches, especially as organised competition at county and Test level developed.[41]
",Cricket
"Meanwhile, the British Empire had been instrumental in spreading the game overseas and by the middle of the 19th century it had become well established in Australia, the Caribbean, India, New Zealand, North America and South Africa.[42] In 1844, the first-ever international match took place between the United States and Canada.[43] In 1859, a team of English players went to North America on the first overseas tour.[44]
",Cricket
"In 1862, an English team made the first tour of Australia.[45] The first Australian team to travel overseas consisted of Aboriginal stockmen who toured England in 1868.[46]
",Cricket
"In 1876–77, an England team took part in what was retrospectively recognised as the first-ever Test match at the Melbourne Cricket Ground against Australia.[47] The rivalry between England and Australia gave birth to The Ashes in 1882 and this has remained Test cricket's most famous contest.[48] Test cricket began to expand in 1888–89 when South Africa played England.[citation needed]
",Cricket
"The inter-war years were dominated by Australia's Don Bradman, statistically the greatest Test batsman of all time. Test cricket continued to expand during the 20th century with the addition of the West Indies (1928), New Zealand (1930) and India (1932) before the Second World War and then Pakistan (1952), Sri Lanka (1982), Zimbabwe (1992) and Bangladesh (2000) in the post-war period.[49][50] South Africa was banned from international cricket from 1970 to 1992 as part of the apartheid boycott.[51]
",Cricket
"Cricket entered a new era in 1963 when English counties introduced the limited overs variant.[52] As it was sure to produce a result, limited overs cricket was lucrative and the number of matches increased.[53] The first Limited Overs International was played in 1971 and the governing International Cricket Council (ICC), seeing its potential, staged the first limited overs Cricket World Cup in 1975.[54] In the 21st century, a new limited overs form, Twenty20, made an immediate impact.[citation needed] On 22 June 2017, Afghanistan and Ireland became the 11th and 12th ICC full members, enabling them to play Test cricket.[55][56]
",Cricket
"In cricket, the rules of the game are specified in a code called The Laws of Cricket (hereinafter called ""the Laws"") which has a global remit. There are 42 Laws (always written with a capital ""L""). The earliest known version of the code was drafted in 1744 and, since 1788, it has been owned and maintained by its custodian, the Marylebone Cricket Club (MCC) in London.[57]
",Cricket
"Cricket is a bat-and-ball game played on a cricket field (see image, right) between two teams of eleven players each.[58] The field is usually circular or oval in shape and the edge of the playing area is marked by a boundary, which may be a fence, part of the stands, a rope, a painted line or a combination of these; the boundary must if possible be marked along its entire length.[59]
",Cricket
"In the approximate centre of the field is a rectangular pitch (see image, below) on which a wooden target called a wicket is sited at each end; the wickets are placed 22 yards (20 m) apart.[60] The pitch is a flat surface 3 metres (9.8 ft) wide, with very short grass that tends to be worn away as the game progresses (cricket can also be played on artificial surfaces, notably matting). Each wicket is made of three wooden stumps topped by two bails.[61]
",Cricket
"As illustrated above, the pitch is marked at each end with four white painted lines: a bowling crease, a popping crease and two return creases. The three stumps are aligned centrally on the bowling crease, which is eight feet eight inches long. The popping crease is drawn four feet in front of the bowling crease and parallel to it; although it is drawn as a twelve-foot line (six feet either side of the wicket), it is in fact unlimited in length. The return creases are drawn at right angles to the popping crease so that they intersect the ends of the bowling crease; each return crease is drawn as an eight-foot line, so that it extends four feet behind the bowling crease, but is also in fact unlimited in length.[62]
",Cricket
"Before a match begins, the team captains (who are also players) toss a coin to decide which team will bat first and so take the first innings.[63] Innings is the term used for each phase of play in the match.[63] In each innings, one team bats, attempting to score runs, while the other team bowls and fields the ball, attempting to restrict the scoring and dismiss the batsmen.[64][65] When the first innings ends, the teams change roles; there can be two to four innings depending upon the type of match. A match with four scheduled innings is played over three to five days; a match with two scheduled innings is usually completed in a single day.[63] During an innings, all eleven members of the fielding team take the field, but only two members of the batting team are on the field at any given time.[63] The order of batsmen is usually announced just before the match, but it can be varied.[58]
",Cricket
"The main objective of each team is to score more runs than their opponents but, in some forms of cricket, it is also necessary to dismiss all of the opposition batsmen in their final innings in order to win the match, which would otherwise be drawn.[66] If the team batting last is all out having scored fewer runs than their opponents, they are said to have ""lost by n runs"" (where n is the difference between the aggregate number of runs scored by the teams). If the team that bats last scores enough runs to win, it is said to have ""won by n wickets"", where n is the number of wickets left to fall. For example, a team that passes its opponents' total having lost six wickets (i.e., six of their batsmen have been dismissed) have won the match ""by four wickets"".[66]
",Cricket
"In a two-innings-a-side match, one team's combined first and second innings total may be less than the other side's first innings total. The team with the greater score is then said to have ""won by an innings and n runs"", and does not need to bat again: n is the difference between the two teams' aggregate scores. If the team batting last is all out, and both sides have scored the same number of runs, then the match is a tie; this result is quite rare in matches of two innings a side with only 62 happening in first-class matches from the earliest known instance in 1741 until January 2017. In the traditional form of the game, if the time allotted for the match expires before either side can win, then the game is declared a draw.[66]
",Cricket
"If the match has only a single innings per side, then a maximum number of overs applies to each innings. Such a match is called a ""limited overs"" or ""one-day"" match, and the side scoring more runs wins regardless of the number of wickets lost, so that a draw cannot occur. If this kind of match is temporarily interrupted by bad weather, then a complex mathematical formula, known as the Duckworth-Lewis method after its developers, is often used to recalculate a new target score. A one-day match can also be declared a ""no-result"" if fewer than a previously agreed number of overs have been bowled by either team, in circumstances that make normal resumption of play impossible; for example, wet weather.[66]
",Cricket
"In all forms of cricket, the umpires can abandon the match if bad light or rain makes it impossible to continue.[67] There have been instances of entire matches, even Test matches scheduled to be played over five days, being lost to bad weather without a ball being bowled: for example, the third Test of the 1970/71 series in Australia.[68]
",Cricket
"i) A used white ball. White balls are mainly used in limited overs cricket, especially in matches played at night, under floodlights (left).
",Cricket
"The essence of the sport is that a bowler delivers (i.e., bowls) the ball from his end of the pitch towards the batsman who, armed with a bat is ""on strike"" at the other end (see next sub-section: Basic gameplay).
",Cricket
"The bat is made of wood, usually salix alba (white willow), and has the shape of a blade topped by a cylindrical handle. The blade must not be more than four and one quarter inches (108 mm) wide and the total length of the bat not more than 38 inches (965 mm). There is no standard for the weight which is usually between 2 lb 7 oz and 3 lb (1.1 and 1.4 kg).[69][70]
",Cricket
"The ball is a hard leather-seamed spheroid, with a circumference of 22.9 centimetres (9.0 in). The ball has a ""seam"": six rows of stitches attaching the leather shell of the ball to the string and cork interior. The seam on a new ball is prominent, and helps the bowler propel it in a less predictable manner. During matches, the quality of the ball deteriorates to a point where it is no longer usable, and during the course of this deterioration its behaviour in flight will change and can influence the outcome of the match. Players will therefore attempt to modify the ball's behaviour by modifying its physical properties. Polishing the ball and wetting it with sweat or saliva is legal, even when the polishing is deliberately done on one side only to increase the ball's swing through the air, but the acts of rubbing other substances into the ball, scratching the surface or picking at the seam is illegal ball tampering.[71]
",Cricket
"During normal play, thirteen players and two umpires are on the field. Two of the players are batsmen and the rest are all eleven members of the fielding team. The other nine players in the batting team are off the field in the pavilion. The image with overlay below shows what is happening when a ball is being bowled and which of the personnel are on or close to the pitch. The photo was taken during an international match between Australia and Sri Lanka; Muttiah Muralitharan of Sri Lanka is bowling to Australian batsman Adam Gilchrist.
",Cricket
"In the photo, the two batsmen (3 & 8; wearing yellow) have taken position at each end of the pitch (6). Three members of the fielding team (4, 10 & 11; wearing dark blue) are in shot. One of the two umpires (1; wearing white hat) is stationed behind the wicket (2) at the bowler's (4) end of the pitch. The bowler (4) is bowling the ball (5) from his end of the pitch to the batsman (8) at the other end who is called the ""striker"". The other batsman (3) at the bowling end is called the ""non-striker"". The wicket-keeper (10), who is a specialist, is positioned behind the striker's wicket (9) and behind him stands one of the fielders in a position called ""first slip"" (11). While the bowler and the first slip are wearing conventional kit only, the two batsmen and the wicket-keeper are wearing protective gear including safety helmets, padded gloves and leg guards (pads).
",Cricket
"While the umpire (1) in shot stands at the bowler's end of the pitch, his colleague stands in the outfield, usually in or near the fielding position called ""square leg"", so that he is in line with the popping crease (7) at the striker's end of the pitch. The bowling crease (not numbered) is the one on which the wicket is located between the return creases (12). The bowler (4) intends to hit the wicket (9) with the ball (5) or, at least, to prevent the striker (8) from scoring runs. The striker (8) intends, by using his bat, to defend his wicket and, if possible, to hit the ball away from the pitch in order to score runs.
",Cricket
"Some players are skilled in both batting and bowling so are termed all-rounders. Adam Gilchrist, pictured above, was a wicket-keeper/batsman, another type of all-rounder. Bowlers are also classified according to their style, generally as fast bowlers, medium pace seam bowlers or, like Muttiah Muralitharan pictured above, spinners. Batsmen are classified according to whether they are right-handed or left-handed.
",Cricket
"Of the eleven fielders, three are in shot in the image above. The other eight are elsewhere on the field, their positions determined on a tactical basis by the captain or the bowler. Fielders often change position between deliveries, again as directed by the captain or bowler.[72]
",Cricket
"If a fielder is injured or becomes ill during a match, a substitute is allowed to field instead of him, but the substitute cannot bowl or act as a captain. The substitute leaves the field when the injured player is fit to return.[73] The Laws of Cricket were updated in 2017 to allow substitutes to act as wicket-keepers,[74] a situation that first occurred when Mumbai Indians' wicket-keeper Ishan Kishan was injured in a match on 18 April 2018.[75]
",Cricket
"The captain is often the most experienced player in the team, certainly the most tactically astute, and can possess any of the main skillsets as a batsman, a bowler or a wicket-keeper. Within the Laws, the captain has certain responsibilities in terms of nominating his players to the umpires before the match and ensuring that his players conduct themselves ""within the spirit and traditions of the game as well as within the Laws"".[58]
",Cricket
"The wicket-keeper (sometimes called simply the ""keeper"") is a specialist fielder subject to various rules within the Laws about his equipment and demeanour. He is the only member of the fielding side who can effect a stumping and is the only one permitted to wear gloves and external leg guards.[76] Depending on their primary skills, the other ten players in the team tend to be classified as specialist batsmen or specialist bowlers. Generally, a team will include five or six specialist batsmen and four or five specialist bowlers, plus the wicket-keeper.[77][78]
",Cricket
"The wicket-keeper and the batsmen wear protective gear because of the hardness of the ball, which can be delivered at speeds of more than 145 kilometres per hour (90 mph) and presents a major health and safety concern. Protective clothing includes pads (designed to protect the knees and shins), batting gloves or wicket-keeper's gloves for the hands, a safety helmet for the head and a box inside the trousers (to protect the crotch area).[79] Some batsmen wear additional padding inside their shirts and trousers such as thigh pads, arm pads, rib protectors and shoulder pads. The only fielders allowed to wear protective gear are those in positions very close to the batsman (i.e., if they are alongside or in front of him), but they cannot wear gloves or external leg guards.[72]
",Cricket
"Subject to certain variations, on-field clothing generally includes a collared shirt with short or long sleeves; long trousers; woollen pullover (if needed); cricket cap (for fielding) or a safety helmet; and spiked shoes or boots to increase traction. The kit is traditionally all white and this remains the case in Test and first-class cricket but, in limited overs cricket, team colours are worn instead.[80]
",Cricket
"The innings (ending with 's' in both singular and plural form) is the term used for each phase of play during a match. Depending on the type of match being played, each team has either one or two innings. Sometimes all eleven members of the batting side take a turn to bat but, for various reasons, an innings can end before they have all done so. The innings terminates if the batting team is ""all out"", a term defined by the Laws: ""at the fall of a wicket or the retirement of a batsman, further balls remain to be bowled but no further batsman is available to come in"".[63] In this situation, one of the batsman has not been dismissed and is termed not out; this is because he has no partners left and there must always be two active batsmen while the innings is in progress.
",Cricket
"An innings may end early while there are still two not out batsmen:[63]
",Cricket
"The Laws state that, throughout an innings, ""the ball shall be bowled from each end alternately in overs of 6 balls"".[81] The name ""over"" came about because the umpire calls ""Over!"" when six balls have been bowled. At this point, another bowler is deployed at the other end, and the fielding side changes ends while the batsmen do not. A bowler cannot bowl two successive overs, although a bowler can (and usually does) bowl alternate overs, from the same end, for several overs which are termed a ""spell"". The batsmen do not change ends at the end of the over, and so the one who was non-striker is now the striker and vice-versa. The umpires also change positions so that the one who was at ""square leg"" now stands behind the wicket at the non-striker's end and vice-versa.[81]
",Cricket
"The game on the field is regulated by the two umpires, one of whom stands behind the wicket at the bowler's end, the other in a position called ""square leg"" which is about 15–20 metres away from the batsman on strike and in line with the popping crease on which he is taking guard. The umpires have several responsibilities including adjudication on whether a ball has been correctly bowled (i.e., not a no-ball or a wide); when a run is scored; whether a batsman is out (the fielding side must first appeal to the umpire, usually with the phrase ""How's that?"" or ""Owzat?""); when intervals start and end; and the suitability of the pitch, field and weather for playing the game. The umpires are authorised to interrupt or even abandon a match due to circumstances likely to endanger the players, such as a damp pitch or deterioration of the light.[67]
",Cricket
"Off the field in televised matches, there is usually a third umpire who can make decisions on certain incidents with the aid of video evidence. The third umpire is mandatory under the playing conditions for Test and Limited Overs International matches played between two ICC full member countries. These matches also have a match referee whose job is to ensure that play is within the Laws and the spirit of the game.[67]
",Cricket
"The match details, including runs and dismissals, are recorded by two official scorers, one representing each team. The scorers are directed by the hand signals of an umpire (see image, right). For example, the umpire raises a forefinger to signal that the batsman is out (has been dismissed); he raises both arms above his head if the batsman has hit the ball for six runs. The scorers are required by the Laws to record all runs scored, wickets taken and overs bowled; in practice, they also note significant amounts of additional data relating to the game.[82]
",Cricket
"A match's statistics are summarised on a scorecard. Prior to the popularisation of scorecards, most scoring was done by men sitting on vantage points cuttings notches on tally sticks and runs were originally called notches.[83] According to Rowland Bowen, the earliest known scorecard templates were introduced in 1776 by T. Pratt of Sevenoaks and soon came into general use.[84] It is believed that scorecards were printed and sold at Lord's for the first time in 1846.[85]
",Cricket
"Besides observing the Laws, cricketers must respect the ""Spirit of Cricket,"" which is the ""Preamble to the Laws,"" first published in the 2000 code, and updated in 2017, and now opens with this statement:[86]
",Cricket
"""Cricket owes much of its appeal and enjoyment to the fact that it should be played not only according to the Laws, but also within the Spirit of Cricket"".",Cricket
"The Preamble is a short statement that emphasises the ""Positive behaviours that make cricket an exciting game that encourages leadership,friendship and teamwork.""[87]
",Cricket
"The major responsibility for ensuring fair play is placed firmly on the captains, but extends to all players, umpires, teachers, coaches and parents involved.
",Cricket
"The umpires are the sole judges of fair and unfair play. They are required under the Laws to intervene in case of dangerous or unfair play or in cases of unacceptable conduct by a player.
",Cricket
"Previous versions of the Spirit identified actions that were deemed contrary (for example, appealing knowing that the batsman is not out) but all specifics are now covered in the Laws of Cricket, the relevant governing playing regulations and disciplinary codes, or left to the judgement of the umpires, captains, their clubs and governing bodies. The terse expression of the Spirit of Cricket now avoids the diversity of cultural conventions that exist on the detail of sportsmanship – or its absence.
",Cricket
"Most bowlers are considered specialists in that they are selected for the team because of their skill as a bowler, although some are all-rounders and even specialist batsmen bowl occasionally. The specialist bowlers are active multiple times during an innings, but may not bowl two overs consecutively. If the captain wants a bowler to ""change ends"", another bowler must temporarily fill in so that the change is not immediate.[81]
",Cricket
"A bowler reaches his delivery stride by means of a ""run-up"" and an over is deemed to have begun when the bowler starts his run-up for the first delivery of that over, the ball then being ""in play"".[81] Fast bowlers, needing momentum, take a lengthy run up while bowlers with a slow delivery take no more than a couple of steps before bowling. The fastest bowlers can deliver the ball at a speed of over 145 kilometres per hour (90 mph) and they sometimes rely on sheer speed to try and defeat the batsman, who is forced to react very quickly.[89] Other fast bowlers rely on a mixture of speed and guile by making the ball seam or swing (i.e. curve) in flight. This type of delivery can deceive a batsman into miscuing his shot, for example, so that the ball just touches the edge of the bat and can then be ""caught behind"" by the wicket-keeper or a slip fielder.[89] At the other end of the bowling scale is the spin bowler who bowls at a relatively slow pace and relies entirely on guile to deceive the batsman. A spinner will often ""buy his wicket"" by ""tossing one up"" (in a slower, steeper parabolic path) to lure the batsman into making a poor shot. The batsman has to be very wary of such deliveries as they are often ""flighted"" or spun so that the ball will not behave quite as he expects and he could be ""trapped"" into getting himself out.[90] In between the pacemen and the spinners are the medium paced seamers who rely on persistent accuracy to try and contain the rate of scoring and wear down the batsman's concentration.[89]
",Cricket
"There are ten ways in which a batsman can be dismissed: five relatively common and five extremely rare. The common forms of dismissal are bowled,[91] caught,[92] leg before wicket (lbw),[93] run out[94] and stumped.[95] Rare methods are hit wicket,[96] hit the ball twice,[97] obstructing the field,[98] handled the ball[99] and timed out.[100] The Laws state that the fielding team, usually the bowler in practice, must appeal for a dismissal before the umpire can give his decision. If the batsman is out, the umpire raises a forefinger and says ""Out!""; otherwise, he will shake his head and say ""Not out"".[101] There is, effectively, an eleventh method of dismissal, retired out, which is not an on-field dismissal as such but rather a retrospective one for which no fielder is credited.[102]
",Cricket
"Batsmen take turns to bat via a batting order which is decided beforehand by the team captain and presented to the umpires, though the order remains flexible when the captain officially nominates the team.[58] Substitute batsmen are not allowed.[73]
",Cricket
"A skilled batsman can use a wide array of ""shots"" or ""strokes"" in both defensive and attacking mode. The idea is to hit the ball to best effect with the flat surface of the bat's blade. If the ball touches the side of the bat it is called an ""edge"". The batsman does not have to play a shot and can allow the ball to go through to the wicketkeeper. Equally, he does not have to attempt a run when he hits the ball with his bat. Batsmen do not always seek to hit the ball as hard as possible, and a good player can score runs just by making a deft stroke with a turn of the wrists or by simply ""blocking"" the ball but directing it away from fielders so that he has time to take a run. A wide variety of shots are played, the batsman's repertoire including strokes named according to the style of swing and the direction aimed: e.g., ""cut"", ""drive"", ""hook"", ""pull"".[103]
",Cricket
"The batsman on strike (i.e. the ""striker"") must prevent the ball hitting the wicket, and try to score runs by hitting the ball with his bat so that he and his partner have time to run from one end of the pitch to the other before the fielding side can return the ball. To register a run, both runners must touch the ground behind the popping crease with either their bats or their bodies (the batsmen carry their bats as they run). Each completed run increments the score of both the team and the striker.[104]
",Cricket
"The decision to attempt a run is ideally made by the batsman who has the better view of the ball's progress, and this is communicated by calling: usually ""yes"", ""no"" or ""wait"". More than one run can be scored from a single hit: hits worth one to three runs are common, but the size of the field is such that it is usually difficult to run four or more.[104] To compensate for this, hits that reach the boundary of the field are automatically awarded four runs if the ball touches the ground en route to the boundary or six runs if the ball clears the boundary without touching the ground within the boundary. In these cases the batsmen do not need to run.[105] Hits for five are unusual and generally rely on the help of ""overthrows"" by a fielder returning the ball. If an odd number of runs is scored by the striker, the two batsmen have changed ends, and the one who was non-striker is now the striker. Only the striker can score individual runs, but all runs are added to the team's total.[104]
",Cricket
"Additional runs can be gained by the batting team as extras (called ""sundries"" in Australia) due to errors made by the fielding side. This is achieved in four ways: no-ball, a penalty of one extra conceded by the bowler if he breaks the rules;[106] wide, a penalty of one extra conceded by the bowler if he bowls so that the ball is out of the batsman's reach;[107] bye, an extra awarded if the batsman misses the ball and it goes past the wicket-keeper and gives the batsmen time to run in the conventional way;[108] leg bye, as for a bye except that the ball has hit the batsman's body, though not his bat.[108] If the bowler has conceded a no-ball or a wide, his team incurs an additional penalty because that ball (i.e., delivery) has to be bowled again and hence the batting side has the opportunity to score more runs from this extra ball.[106][107]
",Cricket
"Women's cricket was first recorded in Surrey in 1745.[109] International development began at the start of the 20th century and the first Test Match was played between Australia and England in December 1934.[110] The following year, New Zealand women joined them, and in 2007 Netherlands women became the tenth women's Test nation when they made their debut against South Africa women. In 1958, the International Women's Cricket Council was founded (it merged with the ICC in 2005).[110] In 1973, the first Cricket World Cup of any kind took place when a Women's World Cup was held in England.[110]  In 2005, the International Women's Cricket Council was merged with the International Cricket Council (ICC) to form one unified body to help manage and develop cricket. The ICC Women's Rankings were launched on 1 October 2015 covering all three formats of women's cricket. In October 2018 following the ICC's decision to award T20 International status to all members, the Women's rankings were split into separate ODI (for Full Members) and T20I lists.[111]
",Cricket
"The International Cricket Council (ICC), which has its headquarters in Dubai, is the global governing body of cricket. It was founded as the Imperial Cricket Conference in 1909 by representatives from England, Australia and South Africa, renamed the International Cricket Conference in 1965, and took up its current name in 1989.[110] The ICC in 2017 has 105 member nations, twelve of which hold full membership and can play Test cricket.[112] The ICC is responsible for the organisation and governance of cricket's major international tournaments, notably the men's and women's versions of the Cricket World Cup. It also appoints the umpires and referees that officiate at all sanctioned Test matches, Limited Overs Internationals and Twenty20 Internationals.
",Cricket
"Each member nation has a national cricket board which regulates cricket matches played in its country, selects the national squad, and organises home and away tours for the national team.[113] In the West Indies, which for cricket purposes is a federation of nations, these matters are addressed by Cricket West Indies.[114]
",Cricket
"The table below lists the ICC full members and their national cricket boards:[115]
",Cricket
"Cricket is a multi-faceted sport with multiple formats that can effectively be divided into first-class cricket, limited overs cricket and, historically, single wicket cricket. The highest standard is Test cricket (always written with a capital ""T"") which is in effect the international version of first-class cricket and is restricted to teams representing the twelve countries that are full members of the ICC (see above). Although the term ""Test match"" was not coined until much later, Test cricket is deemed to have begun with two matches between Australia and England in the 1876–77 Australian season; since 1882, most Test series between England and Australia have been played for a trophy known as The Ashes. The term ""first-class"", in general usage, is applied to top-level domestic cricket. Test matches are played over five days and first-class over three to four days; in all of these matches, the teams are allotted two innings each and the draw is a valid result.[117]
",Cricket
"Limited overs cricket is always scheduled for completion in a single day. There are two types: List A which normally allows fifty overs per team; and Twenty20 in which the teams have twenty overs each. Both of the limited overs forms are played internationally as Limited Overs Internationals (LOI) and Twenty20 Internationals (T20I). List A was introduced in England in the 1963 season as a knockout cup contested by the first-class county clubs. In 1969, a national league competition was established. The concept was gradually introduced to the other leading cricket countries and the first limited overs international was played in 1971. In 1975, the first Cricket World Cup took place in England. Twenty20 is a new variant of limited overs itself with the purpose being to complete the match within about three hours, usually in an evening session. The first Twenty20 World Championship was held in 2007. Limited overs matches cannot be drawn, although a tie is possible and an unfinished match is a ""no result"".[118][119]
",Cricket
"Single wicket was popular in the 18th and 19th centuries and its matches were generally considered top-class. In this form, although each team may have from one to six players, there is only one batsman in at a time and he must face every delivery bowled while his innings lasts. Single wicket has rarely been played since limited overs cricket began. Matches tended to have two innings per team like a full first-class one and they could end in a draw.[120]
",Cricket
"Most international matches are played as parts of 'tours', when one nation travels to another for a number of weeks or months, and plays a number of matches of various sorts against the host nation. Sometimes a perpetual trophy is awarded to the winner of the Test series, the most famous of which is The Ashes.
",Cricket
"The ICC also organises competitions that are for several countries at once, including the Cricket World Cup, ICC T20 World Cup and ICC Champions Trophy. A league competition for Test matches played as part of normal tours, the ICC World Test Championship, has been proposed several times, and is currently planned to begin in 2019. The ICC maintains Test rankings, ODI rankings and T20 rankings systems for the countries which play these forms of cricket.
",Cricket
"Competitions for member nations of the ICC with Associate status include the ICC Intercontinental Cup, for first-class cricket matches, and the World Cricket League for one-day matches, the final matches of which now also serve as the ICC World Cup Qualifier.
",Cricket
"First-class cricket in England is played for the most part by the 18 county clubs which contest the County Championship. The concept of a champion county has existed since the 18th century but the official competition was not established until 1890.[40] The most successful club has been Yorkshire, who had won 32 official titles (plus one shared) to 2017.[121]
",Cricket
"Australia established its national first-class championship in 1892–93 when the Sheffield Shield was introduced. In Australia, the first-class teams represent the various states.[122] New South Wales has the highest number of titles.
",Cricket
"The other ICC full members have national championship trophies called the Ahmad Shah Abdali 4-day Tournament (Afghanistan); the National Cricket League (Bangladesh); the Ranji Trophy (India); the Inter-Provincial Championship (Ireland); the Plunket Shield (New Zealand); the Quaid-e-Azam Trophy (Pakistan); the Currie Cup (South Africa); the Premier Trophy (Sri Lanka); the Shell Shield (West Indies); and the Logan Cup (Zimbabwe).
",Cricket
"The world's earliest known cricket match was a village cricket meeting in Kent which has been deduced from a 1640 court case recording a ""cricketing"" of ""the Weald and the Upland"" versus ""the Chalk Hill"" at Chevening ""about thirty years since"" (i.e., c. 1611). Inter-parish contests became popular in the first half of the 17th century and continued to develop through the 18th with the first local leagues being founded in the second half of the 19th.[17]
",Cricket
"At the grassroots level, local club cricket is essentially an amateur pastime for those involved but still usually involves teams playing in competitions at weekends or in the evening. Schools cricket, first known in southern England in the 17th century, has a similar scenario and both are widely played in the countries where cricket is popular.[123] Although there can be variations in game format, compared with professional cricket, the Laws are always observed and club/school matches are therefore formal and competitive events.[124] The sport has numerous informal variants such as French cricket.[125]
",Cricket
"Cricket has had a broad impact on popular culture, both in the Commonwealth of Nations and elsewhere. It has, for example, influenced the lexicon of these nations, especially the English language, with various phrases such as ""that's not cricket"" (that's unfair), ""had a good innings"" (lived a long life) and ""sticky wicket"". ""On a sticky wicket"" (aka ""sticky dog"" or ""glue pot"")[126] is a metaphor[127] used to describe a difficult circumstance. It originated as a term for difficult batting conditions in cricket, caused by a damp and soft pitch.[128]
",Cricket
"Cricket is the subject of works by noted English poets, including William Blake and Lord Byron.[129] Beyond a Boundary (1963), written by Trinidadian C. L. R. James, is often named the best book on any sport ever written.[130]
",Cricket
"In the visual arts, notable cricket paintings include Albert Chevallier Tayler's Kent vs Lancashire at Canterbury (1907) and Russell Drysdale's The Cricketers (1948), which has been called ""possibly the most famous Australian painting of the 20th century.""[131] French impressionist Camille Pissarro painted cricket on a visit to England in the 1890s.[129] Francis Bacon, an avid cricket fan, captured a batsman in motion.[129] Caribbean artist Wendy Nanan's cricket images[132] are featured in a limited edition first day cover for Royal Mail's ""World of Invention"" stamp issue, which celebrated the London Cricket Conference 1–3 March 2007, first international workshop of its kind and part of the celebrations leading up to the 2007 Cricket World Cup.[133]
",Cricket
"Cricket has close historical ties with Australian rules football and many players have competed at top levels in both sports.[134] In 1858, prominent Australian cricketer Tom Wills called for the formation of a ""foot-ball club"" with ""a code of laws"" to keep cricketers fit during the off-season. The Melbourne Football Club was founded the following year, and Wills and three other members codified the first laws of the game.[135] It is typically played on modified cricket fields.[136]
",Cricket
"In England, a number of association football clubs owe their origins to cricketers who sought to play football as a means of keeping fit during the winter months. Derby County was founded as a branch of the Derbyshire County Cricket Club in 1884;[137] Aston Villa (1874) and Everton (1876) were both founded by members of church cricket teams.[138] Sheffield United's Bramall Lane ground was, from 1854, the home of the Sheffield Cricket Club, and then of Yorkshire; it was not used for football until 1862 and was shared by Yorkshire and Sheffield United from 1889 to 1973.[139]
",Cricket
"In the late 19th century, a former cricketer, English-born Henry Chadwick of Brooklyn, New York, was credited with devising the baseball box score[140] (which he adapted from the cricket scorecard) for reporting game events. The first box score appeared in an 1859 issue of the Clipper.[141] The statistical record is so central to the game's ""historical essence"" that Chadwick is sometimes referred to as ""the Father of Baseball"" because he facilitated the popularity of the sport in its early days.[142]
",Cricket
"Related sports
",Cricket
"Organisations and competitions
",Cricket
"Statistics and records
",Cricket
"News and other resources
",Cricket
"
",Cricket
"Limited overs cricket, also known as one-day cricket, which includes List A cricket and Twenty20 cricket, is a version of the sport of cricket in which a match is generally completed in one day, whereas Test and first-class matches can take up to five days to complete. The name reflects the rule that in the match each team bowls a set maximum number of overs, usually between 20 and 50, although shorter and longer forms of limited overs cricket have been played.
",Cricket
"One-day cricket is popular with spectators as it can encourage aggressive, risky, entertaining batting, often results in cliffhanger endings, and ensures that a spectator can watch an entire match without committing to five days of continuous attendance.
",Cricket
"
",Cricket
"Each team bats only once, and each innings is limited to a set number of overs, usually fifty in a One Day International and between forty and sixty in a List A. List A is a classification of the limited-overs (one-day) form of cricket, technically as the domestic level.
",Cricket
"Despite its name, important one-day matches, international and domestic, often have two days set aside, the second day being a ""reserve"" day to allow more chance of the game being completed if a result is not possible on the first day (for instance if play is prevented or interrupted by rain).
",Cricket
"As mentioned above, in almost all competitive one-day games, a restriction is placed on the number of overs that may be bowled by any one bowler. This is to prevent a side playing two top-class bowlers with extremely good stamina who can bowl throughout their opponents' innings. The usual limitation is set so that a side must include at least five players who bowl. For example, the usual limit for twenty-over cricket is four overs per bowler, for forty-over cricket eight per bowler and for fifty-over cricket ten per bowler. There are exceptions: Pro Cricket in the United States restricted bowlers to five overs each, thus leaving a side requiring only four bowlers.
",Cricket
"The idea for a one-day, limited 50-over cricket tournament, was first played in the inaugural match of the All India Pooja Cricket Tournament in 1951 in the small town of Thrippunithura in Kerala. It is thought to be the brain child of KV Kelappan Thampuran, a former cricketer and the first Secretary of the Kerala Cricket Association.[1] The one day limited over cricket game was later adapted and played between English county teams for the first instance on 2 May 1962. Leicestershire beat Derbyshire and Northamptonshire beat Nottinghamshire over 65 overs in the ""Midlands Knock-Out Cup"", which Northamptonshire went on to win a week later. The following year, the first full-scale one-day competition between first-class teams was played, the knock-out Gillette Cup, won by Sussex. The number of overs was reduced to 60 for the 1964 season. League one-day cricket also began in England, when the John Player Sunday League was started in 1969 with forty over matches. Both these competitions have continued every season since inauguration, though the sponsorship has changed. There is now one 50 over competition, which is called the Royal London One-Day Cup. 
",Cricket
"The first Limited Overs International (LOI) or One-Day International (ODI) match was played in Melbourne in 1971, and the quadrennial cricket World Cup began in 1975. Many of the ""packaging"" innovations, such as coloured clothing, were as a result of World Series Cricket, a ""rebel"" series set up outside the cricketing establishment by Australian entrepreneur Kerry Packer. For more details, see History of cricket.
",Cricket
"Twenty20, a curtailed form of one-day cricket with 20 overs per side, was first played in England in 2003. It has proven very popular, and several Twenty20 matches have been played between national teams. It makes several changes to the usual laws of cricket, including the addition of a ""bowl-out"" (similar to a penalty shoot-out in football) to decide the result of tied matches, which was subsequently dispensed in favour of a Super Over.
",Cricket
"100-ball cricket, another form of one-day cricket with 100 deliveries  per side, will launch in England in 2020. It is designed to further shorten game time and hopes to attract a new audience. It makes further changes to the usual laws of cricket, including the addition of one 10-ball over which is bowled by each side in addition to 15 traditional 6-ball overs.
",Cricket
"One Day International matches are usually played in brightly coloured clothing often in a ""day-night"" format where the first innings of the day occurs in the afternoon and the second occurs under stadium lights.
",Cricket
"Every four years, the Cricket World Cup involves all the Test-playing nations and other national sides who qualify through the ICC World Cup Qualifier. It usually consists of round-robin stages, followed by semi-finals and a final.  The International Cricket Council (ICC) determines the venue far in advance.
",Cricket
"The ICC Champions Trophy also involves all the Test-playing nations, and is held between World Cups. It usually consists of a round-robin group stage, semifinals, and a final.
",Cricket
"Each Test-playing country often hosts triangular tournaments, between the host nation and two touring sides. There is usually a round-robin group stage, and then the leading two teams play each other in a final, or sometimes a best-of-three final. When there is only one touring side, there is still often a best-of-five or best-of-seven series of limited overs matches.
",Cricket
"The ICC World Cricket League is an ODI competition for national teams with Associate or Affiliate status.
",Cricket
"Domestic one-day competitions exist in almost every country where cricket is played.
",Cricket
"List A cricket is a classification of the limited-overs (one-day) form of the sport of cricket. Much as domestic first-class cricket is the level below international Test match cricket, so List A cricket is the domestic level of one-day cricket below One Day Internationals. Twenty20 matches do not qualify for the present.
",Cricket
"Most cricketing nations have some form of domestic List A competition. The number of overs in List A cricket ranges from forty to sixty overs per side.
",Cricket
"The Association of Cricket Statisticians and Historians created this category for the purpose of providing an equivalent to first-class cricket, to allow the generation of career records and statistics for comparable one-day matches. Only the more important one-day competitions in each country, plus matches against a touring Test team, are included. The categorisation of cricket matches as ""List A"" was not officially endorsed by the International Cricket Council until 2006, when the ICC announced that it and its member associations would be determining this classification in a manner similar to that done for first class matches.[2]
",Cricket
"The Matador BBQ One Day Cup is a 50 overs tournament held since 1969. The sides that compete are the following: 
",Cricket
"In 2006 Cricket Australia introduced the KFC Twenty20 Big Bash which was amongst the state teams (as above). In 2011 this was expanded to the KFC Twenty20 Big Bash League, consisting of teams based in the capital cities of Australia. The teams are as follows:
",Cricket
"The National One Day Cricket League is sponsored by Mirzapore Tea. It currently runs from November to March, with each team playing the other home and away once in a round-robin format. These six teams compete for the League title:
",Cricket
"Each county has a team representing them in each league and are as followed with their home ground:
",Cricket
"North Group:
",Cricket
"Birmingham Bears (Edgbaston)
Derbyshire Falcons (Derby County Ground)
Durham Jets (Riverside Ground)
Lancashire Lightning (Old Trafford)
Leicestershire Foxes (Grace Road)
Northamptonshire Steelbacks (Northampton County Ground)
Nottinghamshire Outlaws (Trent Bridge)
Worcestershire Rapids (New Road)
Yorkshire Vikings (Headingley)
",Cricket
"South Group:
",Cricket
"Essex Eagles (Chelmsford County Ground)
Glamorgan Dragons (Sophia Gardens)
Gloucestershire (Bristol County Ground)
Hampshire Royals (Rose Bowl)
Kent Spitfires (St Lawrence Ground)
Middlesex (Lord's)
Somerset (Taunton County Ground)
Surrey (The Oval)
Sussex Sharks (Hove County Ground)
",Cricket
"The Pakistani domestic competition changes regularly, but for 2005–06 there are plans for three one-day tournaments for men:
",Cricket
"— a professional franchise Twenty20 men's cricket league. The league is headquartered in Lahore, consists of five franchises nominally representing cities in
Pakistan . It is operated by the Pakistan Cricket Board (PCB) and was established in 2016.Following are the teams:
",Cricket
"The local competition in South Africa is the Standard Bank Cup (formerly Benson & Hedges Series) played between 6 teams:
",Cricket
"The games are 45-overs, and based on a home-and-away round-robin match system (each team plays ten matches) with semi-finals and a final. The Eagles were the winners of the 2004/2005 and 2005/2006 competitions.
",Cricket
"20 teams compete in the Premier Limited-Overs Tournament, which is an expansion from 16 in the last season. Games are played over 50 overs per side, and the teams are divided into two groups, where each team meets the other once over a period of a month. The four top teams from each group qualify for the quarter-finals, and there is then a direct knock-out system until a winner is found after three knock-out stages. The competing teams are:
",Cricket
"The KFC Cup is the main regional one-day competition in the West Indies, named after its chief sponsor, the fast food chain KFC. In recent years, it has been run over a week's time as a group stage followed by knock-out stages. Guyana are the current holders, after they beat Barbados in the final, and they are also the team to have won it most, with nine titles, although two of them have been shared. Trinidad and Tobago are second in that history, having won seven titles.
",Cricket
"In the 2005–06 edition of the KFC Cup, the six permanent first class regions of the West Indies contested the tournament:
",Cricket
"The world record for the highest innings total in any List A limited overs match is 496 for 4 by Surrey against Gloucestershire in their Friends Provident Trophy 50-overs match at the Oval, London on 29 April 2007. That surpassed the 443 for nine by Sri Lanka against the Netherlands in their One Day International 50-overs match at Amstelveen on 4 July 2006, which was the record ODI score at the time. On 19th June 2018, England set a new international record, totalling 481 for 6 against Australia at Trent Bridge. The lowest ever total is 23 by Yorkshire against Middlesex at Headingley in 1974 in a 40-overs match. The record low score in ODIs was set by Zimbabwe, who managed just 35 against Sri Lanka in Harare on 25 April 2004.
",Cricket
"The most runs scored by both sides in any List A limited overs match is 872: Australia, batting first, scored 434 for four in 50 overs, and yet were beaten by South Africa who scored 438 for nine with a ball to spare during their One Day International at Johannesburg in 2006.
",Cricket
"The highest individual innings is 268 by Ali Brown for Surrey against Glamorgan in a 50-overs match at The Oval in 2002. The best bowling figures are eight for 15 by Rahul Sanghvi for Delhi against Himachal Pradesh in a 50-overs match at Una in 1997.
The highest international individual innings is by Rohit Sharma who scored 264.
The highest score in any formal limited overs match is believed to be United's 630 for five against Bay Area in a 45 overs match at Richmond, California in August 2006.[3]
",Cricket
"The most runs in an over was scored by Herschelle Gibbs of the South African cricket team when, in the 2007 Cricket World Cup in the West Indies, he hit 6 sixes in one over bowled by Daan van Bunge of the Netherlands.[4]
",Cricket
"This record is shared by Yuvraj Singh of India who achieved this feat in the 2007 ICC World Twenty20 in South Africa, he hit 6 sixes in an over bowled by Stuart Broad of England.
",Cricket
"Sachin Tendulkar holds the record of being the first male cricketer to score a double century in ODIs (200 not out). He achieved this feat against South Africa on 24 February 2010, at Gwalior, India. Virender Sehwag is the second male cricketer to score a double century, when he scored 219 before being caught out against West Indies on 8 December 2011, at Indore, India. Rohit Sharma became the third male cricketer to score a double century, when he scored 209 against Australia on 2 November 2013.
",Cricket
"
",Cricket
"
",Cricket
"Twenty20 cricket, sometimes written Twenty-20, and often abbreviated to T20, is a short form of cricket. At the professional level, it was originally introduced by the England and Wales Cricket Board (ECB) in 2003 for the inter-county competition in England and Wales.[1] In a Twenty20 game the two teams have a single innings each, which is restricted to a maximum of 20 overs. Together with first-class and List A cricket, Twenty20 is one of the three current forms of cricket recognised by the International Cricket Council (ICC) as being at the highest international or domestic level. 
A typical Twenty20 game is completed in about three hours, with each innings lasting around 90 minutes and an official 10 minute break between the innings. This is much shorter than previously-existing forms of the game, and is closer to the timespan of other popular team sports. It was introduced to create a fast-paced form of the game which would be attractive to spectators at the ground and viewers on television.
",Cricket
"The game has succeeded in spreading around the cricket world. On most international tours there is at least one Twenty20 match and all Test-playing nations have a domestic cup competition. The inaugural ICC World Twenty20 was played in South Africa in 2007 with India winning by five runs against Pakistan in the final.[2] Pakistan won the second tournament in 2009,[3] and England won the title in the West Indies 2010. West Indies won in 2012, with Sri Lanka winning the 2014 tournament. West Indies are the reigning champions, winning the 2016 competition, and in doing so, became the first nation to win the tournament twice.
",Cricket
"When the Benson & Hedges Cup ended in 2002, the ECB needed another one day competition to fill its place. Cricketing authorities were looking to boost the game's popularity with the younger generation in response to dwindling crowds and reduced sponsorship. It was intended to deliver fast-paced, exciting cricket accessible to thousands of fans who were put off by the longer versions of the game. Stuart Robertson, the marketing manager of the ECB, proposed a 20 over per innings game to county chairmen in 2001 and they voted 11–7 in favour of adopting the new format.[4]
",Cricket
"The first official Twenty20 matches were played on 13 June 2003 between the English counties in the Twenty20 Cup.[5] The first season of Twenty20 in England was a relative success, with the Surrey Lions defeating the Warwickshire Bears by 9 wickets in the final to claim the title.[6] The first Twenty20 match held at Lord's, on 15 July 2004 between Middlesex and Surrey, attracted a crowd of 27,509, the highest attendance for any county cricket game at the ground – other than a one-day final – since 1953.[7]
",Cricket
"Thirteen teams from different parts of the country participated in Pakistan's inaugural competition in 2004, with Faisalabad Wolves the first winners. On 12 January 2005 Australia's first Twenty20 game was played at the WACA Ground between the Western Warriors and the Victorian Bushrangers. It drew a sell-out crowd of 20,000, which was the first time in nearly 25 years the ground had been completely sold out.[8]
",Cricket
"Starting 11 July 2006 19 West Indies regional teams competed in what was named the Stanford 20/20 tournament. The event was financially backed by billionaire Allen Stanford, who gave at least US$28,000,000 funding money. It was intended that the tournament would be an annual event. Guyana won the inaugural event, defeating Trinidad and Tobago by 5 wickets, securing US$1,000,000 in prize money.[9][10]
",Cricket
"On 5 January 2007 Queensland Bulls played the New South Wales Blues at The Gabba, Brisbane. A crowd of 11,000 was expected based on pre-match ticket sales. However, an unexpected 16,000 turned up on the day to buy tickets, causing disruption and confusion for surprised Gabba staff as they were forced to throw open gates and grant many fans free entry. Attendance reached 27,653.[11]
",Cricket
"For 1 February 2008 Twenty20 match between Australia and India, 85,824[12] people attended the match at the Melbourne Cricket Ground involving the Twenty20 World Champions against the ODI World Champions.
",Cricket
"The Stanford Super Series was held in October 2008 between Middlesex and Trinidad and Tobago, the respective winners of the English and Caribbean Twenty20 competitions, and a Stanford Superstars team formed from West Indies domestic players; Trinidad and Tobago won the competition, securing US$280,000 prize money.[13][14] On 1 November, the Stanford Superstars played England in what was expected to be the first of five fixtures in as many years with the winner claiming a US$20,000,000 in each match. The Stanford Superstars won the first match,[15] however no further fixtures were held as Allen Stanford was charged with fraud in 2009.[16]
",Cricket
"Several T20 leagues started after the popularity of 2007 ICC World Twenty20.[17] BCCI started the Indian Premier League in 2008, which utilizes the North American sports franchise system with eight teams in major Indian markets, and is currently in its eleventh season of competition. In September 2017, the broadcasting and digital rights for the next five years (2018-2022) of the IPL were sold to Star India for US$2.55 billion,[18] making it one of the world's most lucrative sports league per match. The IPL has seen a spike in its brand valuation to US$5.3 billion after the 10th edition, according to global valuation and corporate finance advisor Duff & Phelps.[19] Big Bash League, Bangladesh Premier League, Pakistan Super League, Caribbean Premier League started thereafter and remained popular with the fans.[20][21] Women's Big Bash League was started in 2015 by Cricket Australia, while Kia Super League was started in England and Wales in 2016. Canada also started their own league, known as 2018 Global T20 Canada, which had an inaugural edition of the tournament in 2018.
",Cricket
"The first Twenty20 International match was held on 5 August 2004 between the England and New Zealand women's teams with New Zealand winning by nine runs[22]
",Cricket
"On 17 February 2005 Australia defeated New Zealand in the first men's full international Twenty20 match, played at Eden Park in Auckland. The game was played in a light-hearted manner – both sides turned out in kit similar to that worn in the 1980s, the New Zealand team's a direct copy of that worn by the Beige Brigade. Some of the players also sported moustaches/beards and hairstyles popular in the 1980s taking part in a competition amongst themselves for best retro look, at the request of the Beige Brigade. Australia won the game comprehensively, and as the result became obvious towards the end of the NZ innings, the players and umpires took things less seriously – Glenn McGrath jokingly replayed the Trevor Chappell underarm incident  from a 1981 ODI between the two sides, and Billy Bowden showed him a mock red card (red cards are not normally used in cricket) in response.
",Cricket
"The first Twenty20 international in England was played between England and Australia at the Rose Bowl in Hampshire on 13 June 2005, which England won by a margin of 100 runs, a record victory which lasted until 2007.[23]
",Cricket
"On 9 January 2006 Australia and South Africa met in the first international Twenty20 game in Australia. In a first, each player's nickname appeared on the back of his uniform, rather than his surname. The international match drew a crowd of 38,894 people at The Gabba. Australia convincingly won the match with man of the match Damien Martyn scoring 96 runs.
",Cricket
"On 16 February 2006 New Zealand defeated West Indies in a tie-breaking bowl-out 3–0; 126 runs were scored apiece in the game proper. The game was the last international match played by Chris Cairns – NZC handed out life-size cardboard masks of his face to patrons as they entered the ground.
",Cricket
"Every two years an ICC World Twenty20 tournament is to take place, except in the event of an ICC Cricket World Cup being scheduled in the same year, in which case it will be held the year before. The first tournament was in 2007 in South Africa where India defeated Pakistan in the final. Two Associate teams had played in the first tournament, selected through the 2007 ICC World Cricket League Division One, a 50-over competition. In December 2007 it was decided to hold a qualifying tournament with a 20-over format to better prepare the teams. With six participants, two would qualify for the 2009 World Twenty20 and would each receive $250,000 in prize money.[24] The second tournament was won by Pakistan who beat Sri Lanka by 8 wickets in England on 21 June 2009. The 2010 ICC World Twenty20 tournament was held in West Indies in May 2010, where England defeated Australia by 7 wickets. The 2012 ICC World Twenty20 was won by the West-Indies, by defeating Sri Lanka at the finals. It was the first time in Cricket history when a T20 World Cup tournament took place in an Asian country. The 2014 ICC World Twenty20 was won by Sri Lanka, by defeating India at the finals, where the tournament was held in Bangladesh. The 2016 ICC World Twenty20 was won by West-Indies, by defeating England at the finals, where the tournament was held in India.
",Cricket
"Twenty20 cricket is claimed to have resulted in a more athletic and explosive form of cricket. Indian fitness coach Ramji Srinivasan declared in an interview with the Indian fitness website Takath.com, that Twenty20 had ""raised the bar"" in terms of fitness levels for all players, demanding higher levels of strength, speed, agility and reaction time from all players regardless of role in the team.[25] Matthew Hayden credited retirement from international cricket with aiding his performance in general and fitness in particular in the Indian Premier League.[26][27]
",Cricket
"In June 2009, speaking at the annual Cowdrey Lecture at Lord's, former Australian wicketkeeper Adam Gilchrist pushed for Twenty20 to be made an Olympic sport. ""It would,"" he said, ""be difficult to see a better, quicker or cheaper way of spreading the game throughout the world.""[28]
",Cricket
"Former Australian captain Ricky Ponting, on the other hand, has criticized Twenty20 as being detrimental to Test cricket and for hampering batsmen's scoring skills and concentration.[29] Former Australian captain Greg Chappell made similar complaints, fearing that young players would play too much T20 and not develop their batting skills fully, while former England player Alex Tudor feared the same for bowling skills.[30][31]
",Cricket
"Former West Indies captains Clive Lloyd, Michael Holding and Garfield Sobers criticised Twenty20 for its role in discouraging players from representing their test cricket national side, with many West Indies players like Chris Gayle, Sunil Narine and Dwayne Bravo preferring instead to play in a Twenty20 franchise elsewhere in the world and make far more money.[32][33][34][35][36]
",Cricket
"Ricky Ponting, http://phone.espncricinfo.com/ci/content/story/529427.html.
",Cricket
"Twenty20 match format is a form of limited overs cricket in that it involves two teams, each with a single innings, the key feature being that each team bats for a maximum of 20 overs. In terms of visual format, the batting team members do not arrive from and depart to traditional dressing rooms, but come and go from a bench (typically a row of chairs) visible in the playing arena, analogous to association football's technical area or a baseball dugout.[37]
",Cricket
"The Laws of cricket apply to Twenty20, with some exceptions:[38]
",Cricket
"Currently, if the match ends with the scores tied and there must be a winner, the tie is broken with a one over per side Eliminator[39] or Super Over:[40][41]
Each team nominates three batsmen and one bowler to play a one-over per side ""mini-match"". The team which bats second in the match bats first in the Super Over.[42][43] In turn, each side bats one over bowled by the one nominated opposition bowler, with their innings over if they lose two wickets before the over is completed. The side with the higher score from their Super Over wins.
If the super over also ends up in a tie, the team that has scored the most boundaries (4s+6s) in the 20 overs wins.
",Cricket
"In the Australian domestic competition the Big Bash League the Super Over is played slightly differently, with no 2-wicket limit, and if the super over is also tied then a ""countback"" is used, with scores after the fifth ball for each team being used to determine the result. If it is still tied, then the countback goes to 4 balls and so on.[44] The latest Super Over to decide a match was between the Sydney Sixers winning against the Brisbane Heat on the 25th January 2017, in the Big Bash League at the Brisbane Cricket Ground, with the Sixers winning 0/22 to 0/15 in the Super Over after tying on 164.[45]
",Cricket
"Tied Twenty20 matches were previously decided by a bowl-out.[46]
",Cricket
"Women's and men's Twenty20 Internationals have been played since 2004 and 2005 respectively. To date, 25 nations have played the format, including all test playing nations. This considers only matches between teams with Twenty20 International status, which is limited by the International Cricket Council to a small number of top teams.
",Cricket
"In November 2011, the ICC released the first Twenty20 International rankings for the men's game, based on the same system as the Test and ODI rankings. The rankings cover a 2 to 3-year period, with matches since the most recent 1 August weighted fully, matches in the preceding 12 months weighted two-thirds, and matches in the 12 months preceding that weighted one-third. To qualify for the rankings, teams must have played at least eight Twenty20 Internationals in the ranking period.[47][48]
",Cricket
"The ICC do not maintain a separate Twenty20 ranking for the women's game, instead aggregating performance over all three forms of the game into one overall women's teams ranking.[49]
",Cricket
"This is a list of the current Twenty20 domestic competitions in several of the leading cricket countries.
",Cricket
"See also: List of Twenty20 International records
",Cricket
"
",Movie
"A film, also called a movie, motion picture, moving picture, or photoplay, is a series of still images that, when shown on a screen, create the illusion of moving images. (See the glossary of motion picture terms.)
",Movie
"This optical illusion causes the audience to perceive continuous motion between separate objects viewed in rapid succession. The process of filmmaking is both an art and an industry. A film is created by photographing actual scenes with a motion-picture camera, by photographing drawings or miniature models using traditional animation techniques, by means of CGI and computer animation, or by a combination of some or all of these techniques, and other visual effects.
",Movie
"The word ""cinema"", short for cinematography, is often used to refer to filmmaking and the film industry, and to the art of filmmaking itself. The contemporary definition of cinema is the art of simulating experiences to communicate ideas, stories, perceptions, feelings, beauty or atmosphere by the means of recorded or programmed moving images along with other sensory stimulations.[1]
",Movie
"Films were originally recorded onto plastic film through a photochemical process and then shown through a movie projector onto a large screen. Contemporary films are now often fully digital through the entire process of production, distribution, and exhibition, while films recorded in a photochemical form traditionally included an analogous optical soundtrack (a graphic recording of the spoken words, music and other sounds that accompany the images which runs along a portion of the film exclusively reserved for it, and is not projected).
",Movie
"Films are cultural artifacts created by specific cultures. They reflect those cultures, and, in turn, affect them. Film is considered to be an important art form, a source of popular entertainment, and a powerful medium for educating—or indoctrinating—citizens. The visual basis of film gives it a universal power of communication. Some films have become popular worldwide attractions through the use of dubbing or subtitles to translate the dialog into other languages.
",Movie
"The individual images that make up a film are called frames. In the projection of traditional celluloid films, a rotating shutter causes intervals of darkness as each frame, in turn, is moved into position to be projected, but the viewer does not notice the interruptions because of an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after its source disappears. The perception of motion is due to a psychological effect called the phi phenomenon.
",Movie
"The name ""film"" originates from the fact that photographic film (also called film stock) has historically been the medium for recording and displaying motion pictures. Many other terms exist for an individual motion-picture, including picture, picture show, moving picture, photoplay, and flick. The most common term in the United States is movie, while in Europe film is preferred. Common terms for the field in general include the big screen, the silver screen, the movies, and cinema; the last of these is commonly used, as an overarching term, in scholarly texts and critical essays. In early years, the word sheet was sometimes used instead of screen.
",Movie
"Preceding film in origin by thousands of years, early plays and dances had elements common to film: scripts, sets, costumes, production, direction, actors, audiences, storyboards and scores. Much terminology later used in film theory and criticism apply, such as mise en scène (roughly, the entire visual picture at any one time). Owing to the lack of any technology for doing so, the moving images and sounds could not be recorded for replaying as with film.
",Movie
"The magic lantern, probably created by Christiaan Huygens in the 1650s, could be used to project animation, which was achieved by various types of mechanical slides. Typically, two glass slides, one with the stationary part of the picture and the other with the part that was to move, would be placed one on top of the other and projected together, then the moving slide would be hand-operated, either directly or by means of a lever or other mechanism. Chromotrope slides, which produced eye-dazzling displays of continuously cycling abstract geometrical patterns and colors, were operated by means of a small crank and pulley wheel that rotated a glass disc.[2]
",Movie
"In the mid-19th century, inventions such as Joseph Plateau's phenakistoscope and the later zoetrope demonstrated that a carefully designed sequence of drawings, showing phases of the changing appearance of objects in motion, would appear to show the objects actually moving if they were displayed one after the other at a sufficiently rapid rate. These devices relied on the phenomenon of persistence of vision to make the display appear continuous even though the observer's view was actually blocked as each drawing rotated into the location where its predecessor had just been glimpsed. Each sequence was limited to a small number of drawings, usually twelve, so it could only show endlessly repeating cyclical motions. By the late 1880s, the last major device of this type, the praxinoscope, had been elaborated into a form that employed a long coiled band containing hundreds of images painted on glass and used the elements of a magic lantern to project them onto a screen.
",Movie
"The use of sequences of photographs in such devices was initially limited to a few experiments with subjects photographed in a series of poses because the available emulsions were not sensitive enough to allow the short exposures needed to photograph subjects that were actually moving. The sensitivity was gradually improved and in the late 1870s, Eadweard Muybridge created the first animated image sequences photographed in real-time. A row of cameras was used, each, in turn, capturing one image on a photographic glass plate, so the total number of images in each sequence was limited by the number of cameras, about two dozen at most. Muybridge used his system to analyze the movements of a wide variety of animal and human subjects. Hand-painted images based on the photographs were projected as moving images by means of his zoopraxiscope.[3]
",Movie
"By the end of the 1880s, the introduction of lengths of celluloid photographic film and the invention of motion picture cameras, which could photograph an indefinitely long rapid sequence of images using only one lens, allowed several minutes of action to be captured and stored on a single compact reel of film. Some early films were made to be viewed by one person at a time through a ""peep show"" device such as the Kinetoscope and the mutoscope. Others were intended for a projector, mechanically similar to the camera and sometimes actually the same machine, which was used to shine an intense light through the processed and printed film and into a projection lens so that these ""moving pictures"" could be shown tremendously enlarged on a screen for viewing by an entire audience. The first kinetoscope film shown in public exhibition was Blacksmith Scene, produced by Edison Manufacturing Company in 1893. The following year the company would begin Edison Studios, which became an early leader in the film industry with notable early shorts including The Kiss, and would go on to produce close to 1,200 films.
",Movie
"The first public screenings of films at which admission was charged were made in 1895 by the American Woodville Latham and his sons, using films produced by their company, and by the – arguably better known – French brothers Auguste and Louis Lumière with ten of their own productions.[citation needed] Private screenings had preceded these by several months, with Latham's slightly predating the Lumière brothers'.[citation needed] Another opinion is that the first public exhibition of projected motion pictures in America was at Brooklyn Institute in New York City 9 May 1893.
",Movie
"The earliest films were simply one static shot that showed an event or action with no editing or other cinematic techniques. Around the turn of the 20th century, films started stringing several scenes together to tell a story. The scenes were later broken up into multiple shots photographed from different distances and angles. Other techniques such as camera movement were developed as effective ways to tell a story with film. Until sound film became commercially practical in the late 1920s, motion pictures were a purely visual art, but these innovative silent films had gained a hold on the public imagination. Rather than leave audiences with only the noise of the projector as an accompaniment, theater owners hired a pianist or organist or, in large urban theaters, a full orchestra to play music that fit the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music to be used for this purpose, and complete film scores were composed for major productions.
",Movie
"The rise of European cinema was interrupted by the outbreak of World War I, while the film industry in the United States flourished with the rise of Hollywood, typified most prominently by the innovative work of D. W. Griffith in The Birth of a Nation (1915) and Intolerance (1916). However, in the 1920s, European filmmakers such as Sergei Eisenstein, F. W. Murnau and Fritz Lang, in many ways inspired by the meteoric wartime progress of film through Griffith, along with the contributions of Charles Chaplin, Buster Keaton and others, quickly caught up with American film-making and continued to further advance the medium.
",Movie
"In the 1920s, the development of electronic sound recording technologies made it practical to incorporate a soundtrack of speech, music and sound effects synchronized with the action on the screen.[citation needed] The resulting sound films were initially distinguished from the usual silent ""moving pictures"" or ""movies"" by calling them ""talking pictures"" or ""talkies.""[citation needed] The revolution they wrought was swift. By 1930, silent film was practically extinct in the US and already being referred to as ""the old medium.""[citation needed]
",Movie
"Another major technological development was the introduction of ""natural color,"" which meant color that was photographically recorded from nature rather than added to black-and-white prints by hand-coloring, stencil-coloring or other arbitrary procedures, although the earliest processes typically yielded colors which were far from ""natural"" in appearance.[citation needed] While the advent of sound films quickly made silent films and theater musicians obsolete, color replaced black-and-white much more gradually.[citation needed] The pivotal innovation was the introduction of the three-strip version of the Technicolor process, first used for animated cartoons in 1932, then also for live-action short films and isolated sequences in a few feature films, then for an entire feature film, Becky Sharp, in 1935. The expense of the process was daunting, but favorable public response in the form of increased box office receipts usually justified the added cost. The number of films made in color slowly increased year after year.
",Movie
"In the early 1950s, the proliferation of black-and-white television started seriously depressing North American theater attendance.[citation needed] In an attempt to lure audiences back into theaters, bigger screens were installed, widescreen processes, polarized 3D projection, and stereophonic sound were introduced, and more films were made in color, which soon became the rule rather than the exception. Some important mainstream Hollywood films were still being made in black-and-white as late as the mid-1960s, but they marked the end of an era. Color television receivers had been available in the US since the mid-1950s, but at first, they were very expensive and few broadcasts were in color. During the 1960s, prices gradually came down, color broadcasts became common, and sales boomed. The overwhelming public verdict in favor of color was clear. After the final flurry of black-and-white films had been released in mid-decade, all Hollywood studio productions were filmed in color, with the usual exceptions made only at the insistence of ""star"" filmmakers such as Peter Bogdanovich and Martin Scorsese.[citation needed]
",Movie
"The decades following the decline of the studio system in the 1960s saw changes in the production and style of film. Various New Wave movements (including the French New Wave, Indian New Wave, Japanese New Wave, and New Hollywood) and the rise of film-school-educated independent filmmakers contributed to the changes the medium experienced in the latter half of the 20th century.[citation needed] Digital technology has been the driving force for change throughout the 1990s and into the 2000s. Digital 3D projection largely replaced earlier problem-prone 3D film systems and has become popular in the early 2010s.[citation needed]
",Movie
"""Film theory"" seeks to develop concise and systematic concepts that apply to the study of film as art. The concept of film as an art-form began in 1911 with Ricciotto Canudo's The Birth of the Sixth Art. Formalist film theory, led by Rudolf Arnheim, Béla Balázs, and Siegfried Kracauer, emphasized how film differed from reality and thus could be considered a valid fine art. André Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality, not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Jacques Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytic film theory, structuralist film theory, feminist film theory, and others. On the other hand, critics from the analytical philosophy tradition, influenced by Wittgenstein, try to clarify misconceptions used in theoretical studies and produce analysis of a film's vocabulary and its link to a form of life.
",Movie
"Film is considered to have its own language. James Monaco wrote a classic text on film theory, titled ""How to Read a Film,"" that addresses this. Director Ingmar Bergman famously said, ""Andrei Tarkovsky for me is the greatest director, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream."" An example of the language is a sequence of back and forth images of one speaking actor's left profile, followed by another speaking actor's right profile, then a repetition of this, which is a language understood by the audience to indicate a conversation. This describes another theory of film, the 180-degree rule, as a visual story-telling device with an ability to place a viewer in a context of being psychologically present through the use of visual composition and editing. The ""Hollywood style"" includes this narrative theory, due to the overwhelming practice of the rule by movie studios based in Hollywood, California, during film's classical era. Another example of cinematic language is having a shot that zooms in on the forehead of an actor with an expression of silent reflection that cuts to a shot of a younger actor who vaguely resembles the first actor, indicating that the first person is remembering a past self, an edit of compositions that causes a time transition.
",Movie
"Montage is the technique by which separate pieces of film are selected, edited, and then pieced together to make a new section of film. A scene could show a man going into battle, with flashbacks to his youth and to his home-life and with added special effects, placed into the film after filming is complete. As these were all filmed separately, and perhaps with different actors, the final version is called a montage. Directors developed a theory of montage, beginning with Eisenstein and the complex juxtaposition of images in his film Battleship Potemkin.[4] Incorporation of musical and visual counterpoint, and scene development through mise en scene, editing, and effects has led to more complex techniques comparable to those used in opera and ballet.
",Movie
"Film criticism is the analysis and evaluation of films. In general, these works can be divided into two categories: academic criticism by film scholars and journalistic film criticism that appears regularly in newspapers and other media. Film critics working for newspapers, magazines, and broadcast media mainly review new releases. Normally they only see any given film once and have only a day or two to formulate their opinions. Despite this, critics have an important impact on the audience response and attendance at films, especially those of certain genres. Mass marketed action, horror, and comedy films tend not to be greatly affected by a critic's overall judgment of a film. The plot summary and description of a film and the assessment of the director's and screenwriters' work that makes up the majority of most film reviews can still have an important impact on whether people decide to see a film. For prestige films such as most dramas and art films, the influence of reviews is important. Poor reviews from leading critics at major papers and magazines will often reduce audience interest and attendance.
",Movie
"The impact of a reviewer on a given film's box office performance is a matter of debate. Some observers claim that movie marketing in the 2000s is so intense, well-coordinated and well financed that reviewers cannot prevent a poorly written or filmed blockbuster from attaining market success. However, the cataclysmic failure of some heavily promoted films which were harshly reviewed, as well as the unexpected success of critically praised independent films indicates that extreme critical reactions can have considerable influence. Other observers note that positive film reviews have been shown to spark interest in little-known films. Conversely, there have been several films in which film companies have so little confidence that they refuse to give reviewers an advanced viewing to avoid widespread panning of the film. However, this usually backfires, as reviewers are wise to the tactic and warn the public that the film may not be worth seeing and the films often do poorly as a result. Journalist film critics are sometimes called film reviewers. Critics who take a more academic approach to films, through publishing in film journals and writing books about films using film theory or film studies approaches, study how film and filming techniques work, and what effect they have on people. Rather than having their reviews published in newspapers or appearing on television, their articles are published in scholarly journals or up-market magazines. They also tend to be affiliated with colleges or universities as professors or instructors.
",Movie
"The making and showing of motion pictures became a source of profit almost as soon as the process was invented. Upon seeing how successful their new invention, and its product, was in their native France, the Lumières quickly set about touring the Continent to exhibit the first films privately to royalty and publicly to the masses. In each country, they would normally add new, local scenes to their catalogue and, quickly enough, found local entrepreneurs in the various countries of Europe to buy their equipment and photograph, export, import, and screen additional product commercially. The Oberammergau Passion Play of 1898[citation needed] was the first commercial motion picture ever produced. Other pictures soon followed, and motion pictures became a separate industry that overshadowed the vaudeville world. Dedicated theaters and companies formed specifically to produce and distribute films, while motion picture actors became major celebrities and commanded huge fees for their performances. By 1917 Charlie Chaplin had a contract that called for an annual salary of one million dollars. From 1931 to 1956, film was also the only image storage and playback system for television programming until the introduction of videotape recorders.
",Movie
"In the United States, much of the film industry is centered around Hollywood, California. Other regional centers exist in many parts of the world, such as Mumbai-centered Bollywood, the Indian film industry's Hindi cinema which produces the largest number of films in the world.[5] Though the expense involved in making films has led cinema production to concentrate under the auspices of movie studios, recent advances in affordable film making equipment have allowed independent film productions to flourish.
",Movie
"Profit is a key force in the industry, due to the costly and risky nature of filmmaking; many films have large cost overruns, an example being Kevin Costner's Waterworld. Yet many filmmakers strive to create works of lasting social significance. The Academy Awards (also known as ""the Oscars"") are the most prominent film awards in the United States, providing recognition each year to films, based on their artistic merits. There is also a large industry for educational and instructional films made in lieu of or in addition to lectures and texts. Revenue in the industry is sometimes volatile due to the reliance on blockbuster films released in movie theaters. The rise of alternative home entertainment has raised questions about the future of the cinema industry, and Hollywood employment has become less reliable, particularly for medium and low-budget films.[6]
",Movie
"Derivative academic fields of study may both interact with and develop independently of filmmaking, as in film theory and analysis. Fields of academic study have been created that are derivative or dependent on the existence of film, such as film criticism, film history, divisions of film propaganda in authoritarian governments, or psychological on subliminal effects (e.g., of a flashing soda can during a screening). These fields may further create derivative fields, such as a movie review section in a newspaper or a television guide. Sub-industries can spin off from film, such as popcorn makers, and film-related toys (e.g., Star Wars figures). Sub-industries of pre-existing industries may deal specifically with film, such as product placement and other advertising within films.
",Movie
"The terminology used for describing motion pictures varies considerably between British and American English. In British usage, the name of the medium is ""film"". The word ""movie"" is understood but seldom used.[7][8] Additionally, ""the pictures"" (plural) is used semi-frequently to refer to the place where movies are exhibited, while in American English this may be called ""the movies"", but it is becoming outdated. In other countries, the place where movies are exhibited may be called a cinema or movie theatre. By contrast, in the United States, ""movie"" is the predominant form. Although the words ""film"" and ""movie"" are sometimes used interchangeably, ""film"" is more often used when considering artistic, theoretical, or technical aspects. The term ""movies"" more often refers to entertainment or commercial aspects, as where to go for fun evening on a date. For example, a book titled ""How to Understand a Film"" would probably be about the aesthetics or theory of film, while a book entitled ""Let's Go to the Movies"" would probably be about the history of entertaining movies and blockbusters.
",Movie
"Further terminology is used to distinguish various forms and media used in the film industry. ""Motion pictures"" and ""moving pictures"" are frequently used terms for film and movie productions specifically intended for theatrical exhibition, such as, for instance, Batman. ""DVD"" and ""videotape"" are video formats that can reproduce a photochemical film. A reproduction based on such is called a ""transfer."" After the advent of theatrical film as an industry, the television industry began using videotape as a recording medium. For many decades, tape was solely an analog medium onto which moving images could be either recorded or transferred. ""Film"" and ""filming"" refer to the photochemical medium that chemically records a visual image and the act of recording respectively. However, the act of shooting images with other visual media, such as with a digital camera, is still called ""filming"" and the resulting works often called ""films"" as interchangeable to ""movies,"" despite not being shot on film. ""Silent films"" need not be utterly silent, but are films and movies without an audible dialogue, including those that have a musical accompaniment. The word, ""Talkies,"" refers to the earliest sound films created to have audible dialogue recorded for playback along with the film, regardless of a musical accompaniment. ""Cinema"" either broadly encompasses both films and movies, or it is roughly synonymous with film and theatrical exhibition, and both are capitalized when referring to a category of art. The ""silver screen"" refers to the projection screen used to exhibit films and, by extension, is also used as a metonym for the entire film industry.
",Movie
"""Widescreen"" refers to a larger width to height in the frame, compared to earlier historic aspect ratios.[9] A ""feature-length film"", or ""feature film"", is of a conventional full length, usually 60 minutes or more, and can commercially stand by itself without other films in a ticketed screening.[10] A ""short"" is a film that is not as long as a feature-length film, often screened with other shorts, or preceding a feature-length film. An ""independent"" is a film made outside the conventional film industry.
",Movie
"In US usage, one talks of a ""screening"" or ""projection"" of a movie or video on a screen at a public or private ""theater."" In British English, a ""film showing"" happens at a cinema (never a ""theatre"", which is a different medium and place altogether).[8] A cinema usually refers to an arena designed specifically to exhibit films, where the screen is affixed to a wall, while a theater usually refers to a place where live, non-recorded action or combination thereof occurs from a podium or other type of stage, including the amphitheater. Theaters can still screen movies in them, though the theater would be retrofitted to do so. One might propose ""going to the cinema"" when referring to the activity, or sometimes ""to the pictures"" in British English, whereas the US expression is usually ""going to the movies."" A cinema usually shows a mass-marketed movie using a front-projection screen process with either a film projector or, more recently, with a digital projector. But, cinemas may also show theatrical movies from their home video transfers that include Blu-ray Disc, DVD, and videocassette when they possess sufficient projection quality or based upon need, such as movies that exist only in their transferred state, which may be due to the loss or deterioration of the film master and prints from which the movie originally existed. Due to the advent of digital film production and distribution, physical film might be absent entirely. A ""double feature"" is a screening of two independently marketed, stand-alone feature films. A ""viewing"" is a watching of a film. ""Sales"" and ""at the box office"" refer to tickets sold at a theater, or more currently, rights sold for individual showings. A ""release"" is the distribution and often simultaneous screening of a film. A ""preview"" is a screening in advance of the main release.
",Movie
"Any film may also have a ""sequel"", which portrays events following those in the film. Bride of Frankenstein is an early example. When there are more films than one with the same characters, story arcs, or subject themes, these movies become a ""series,"" such as the James Bond series. And, existing outside a specific story timeline usually, does not exclude a film from being part of a series. A film that portrays events occurring earlier in a timeline with those in another film, but is released after that film, is sometimes called a ""prequel,"" an example being Butch and Sundance: The Early Days.
",Movie
"The ""credits,"" or ""end credits,"" is a list that gives credit to the people involved in the production of a film. Films from before the 1970s usually start a film with credits, often ending with only a title card, saying ""The End"" or some equivalent, often an equivalent that depends on the language of the production[citation needed]. From then onward, a film's credits usually appear at the end of most films. However, films with credits that end a film often repeat some credits at or near the start of a film and therefore appear twice, such as that film's acting leads, while less frequently some appearing near or at the beginning only appear there, not at the end, which often happens to the director's credit. The credits appearing at or near the beginning of a film are usually called ""titles"" or ""beginning titles."" A post-credits scene is a scene shown after the end of the credits. Ferris Bueller's Day Off has a post-credit scene in which Ferris tells the audience that the film is over and they should go home.
",Movie
"A film's ""cast"" refers to a collection of the actors and actresses who appear, or ""star,"" in a film. A star is an actor or actress, often a popular one, and in many cases, a celebrity who plays a central character in a film. Occasionally the word can also be used to refer to the fame of other members of the crew, such as a director or other personality, such as Martin Scorsese. A ""crew"" is usually interpreted as the people involved in a film's physical construction outside cast participation, and it could include directors, film editors, photographers, grips, gaffers, set decorators, prop masters, and costume designers. A person can both be part of a film's cast and crew, such as Woody Allen, who directed and starred in Take the Money and Run.
",Movie
"A ""film goer,"" ""movie goer,"" or ""film buff"" is a person who likes or often attends films and movies, and any of these, though more often the latter, could also see oneself as a student to films and movies or the filmic process. Intense interest in films, film theory, and film criticism, is known as cinephilia. A film enthusiast is known as a cinephile or cineaste.
",Movie
"A preview performance refers to a showing of a film to a select audience, usually for the purposes of corporate promotions, before the public film premiere itself. Previews are sometimes used to judge audience reaction, which if unexpectedly negative, may result in recutting or even refilming certain sections based on the audience response. One example of a film that was changed after a negative response from the test screening is 1982's First Blood. After the test audience responded very negatively to the death of protagonist John Rambo, a Vietnam veteran, at the end of the film, the company wrote and re-shot a new ending in which the character survives.[11]
",Movie
"Trailers or previews are advertisements for films that will be shown in 1 to 3 months at a cinema. Back in the early days of cinema, with theaters that had only one or two screens, only certain trailers were shown for the films that were going to be shown there. Later, when theaters added more screens or new theaters were built with a lot of screens, all different trailers were shown even if they weren't going to play that film in that theater. Film studios realized that the more trailers that were shown (even if it wasn't going to be shown in that particular theater) the more patrons would go to a different theater to see the film when it came out. The term ""trailer"" comes from their having originally been shown at the end of a film program. That practice did not last long because patrons tended to leave the theater after the films ended, but the name has stuck. Trailers are now shown before the film (or the ""A film"" in a double feature program) begins. Film trailers are also common on DVDs and Blu-ray Discs, as well as on the Internet and mobile devices. Trailers are created to be engaging and interesting for viewers. As a result, in the Internet era, viewers often seek out trailers to watch them. Of the ten billion videos watched online annually in 2008, film trailers ranked third, after news and user-created videos.[12] Teasers are a much shorter preview or advertisement that lasts only 10 to 30 seconds. Teasers are used to get patrons excited about a film coming out in the next six to twelve months. Teasers may be produced even before the film production is completed.
",Movie
"Film is used for a range of goals, including education and propaganda. When the purpose is primarily educational, a film is called an ""educational film"". Examples are recordings of academic lectures and experiments, or a film based on a classic novel. Film may be propaganda, in whole or in part, such as the films made by Leni Riefenstahl in Nazi Germany, US war film trailers during World War II, or artistic films made under Stalin by Eisenstein. They may also be works of political protest, as in the films of Andrzej Wajda, or more subtly, the films of Andrei Tarkovsky. The same film may be considered educational by some, and propaganda by others as the categorization of a film can be subjective.
",Movie
"At its core, the means to produce a film depend on the content the filmmaker wishes to show, and the apparatus for displaying it: the zoetrope merely requires a series of images on a strip of paper. Film production can, therefore, take as little as one person with a camera (or even without a camera, as in Stan Brakhage's 1963 film Mothlight), or thousands of actors, extras, and crew members for a live-action, feature-length epic.
",Movie
"The necessary steps for almost any film can be boiled down to conception, planning, execution, revision, and distribution. The more involved the production, the more significant each of the steps becomes. In a typical production cycle of a Hollywood-style film, these main stages are defined as development, pre-production, production, post-production and distribution.
",Movie
"This production cycle usually takes three years. The first year is taken up with development. The second year comprises preproduction and production. The third year, post-production and distribution. The bigger the production, the more resources it takes, and the more important financing becomes; most feature films are artistic works from the creators' perspective (e.g., film director, cinematographer, screenwriter) and for-profit business entities for the production companies.
",Movie
"A film crew is a group of people hired by a film company, employed during the ""production"" or ""photography"" phase, for the purpose of producing a film or motion picture. Crew is distinguished from cast, who are the actors who appear in front of the camera or provide voices for characters in the film. The crew interacts with but is also distinct from the production staff, consisting of producers, managers, company representatives, their assistants, and those whose primary responsibility falls in pre-production or post-production phases, such as screenwriters and film editors. Communication between production and crew generally passes through the director and his/her staff of assistants. Medium-to-large crews are generally divided into departments with well-defined hierarchies and standards for interaction and cooperation between the departments. Other than acting, the crew handles everything in the photography phase: props and costumes, shooting, sound, electrics (i.e., lights), sets, and production special effects. Caterers (known in the film industry as ""craft services"") are usually not considered part of the crew.
",Movie
"Film stock consists of transparent celluloid, acetate, or polyester base coated with an emulsion containing light-sensitive chemicals. Cellulose nitrate was the first type of film base used to record motion pictures, but due to its flammability was eventually replaced by safer materials. Stock widths and the film format for images on the reel have had a rich history, though most large commercial films are still shot on (and distributed to theaters) as 35 mm prints.
Originally moving picture film was shot and projected at various speeds using hand-cranked cameras and projectors; though 1000 frames per minute (162/3 frame/s) is generally cited as a standard silent speed, research indicates most films were shot between 16 frame/s and 23 frame/s and projected from 18 frame/s on up (often reels included instructions on how fast each scene should be shown).[13] When sound film was introduced in the late 1920s, a constant speed was required for the sound head. 24 frames per second were chosen because it was the slowest (and thus cheapest) speed which allowed for sufficient sound quality.[citation needed] Improvements since the late 19th century include the mechanization of cameras – allowing them to record at a consistent speed, quiet camera design – allowing sound recorded on-set to be usable without requiring large ""blimps"" to encase the camera, the invention of more sophisticated filmstocks and lenses, allowing directors to film in increasingly dim conditions, and the development of synchronized sound, allowing sound to be recorded at exactly the same speed as its corresponding action. The soundtrack can be recorded separately from shooting the film, but for live-action pictures, many parts of the soundtrack are usually recorded simultaneously.
",Movie
"As a medium, film is not limited to motion pictures, since the technology developed as the basis for photography. It can be used to present a progressive sequence of still images in the form of a slideshow. Film has also been incorporated into multimedia presentations and often has importance as primary historical documentation. However, historic films have problems in terms of preservation and storage, and the motion picture industry is exploring many alternatives. Most films on cellulose nitrate base have been copied onto modern safety films. Some studios save color films through the use of separation masters: three B&W negatives each exposed through red, green, or blue filters (essentially a reverse of the Technicolor process). Digital methods have also been used to restore films, although their continued obsolescence cycle makes them (as of 2006) a poor choice for long-term preservation. Film preservation of decaying film stock is a matter of concern to both film historians and archivists and to companies interested in preserving their existing products in order to make them available to future generations (and thereby increase revenue). Preservation is generally a higher concern for nitrate and single-strip color films, due to their high decay rates; black-and-white films on safety bases and color films preserved on Technicolor imbibition prints tend to keep up much better, assuming proper handling and storage.
",Movie
"Some films in recent decades have been recorded using analog video technology similar to that used in television production. Modern digital video cameras and digital projectors are gaining ground as well. These approaches are preferred by some film-makers, especially because footage shot with digital cinema can be evaluated and edited with non-linear editing systems (NLE) without waiting for the film stock to be processed. The migration was gradual, and as of 2005, most major motion pictures were still shot on film.[needs update]
",Movie
"Independent filmmaking often takes place outside Hollywood, or other major studio systems. An independent film (or indie film) is a film initially produced without financing or distribution from a major film studio. Creative, business and technological reasons have all contributed to the growth of the indie film scene in the late 20th and early 21st century. On the business side, the costs of big-budget studio films also lead to conservative choices in cast and crew. There is a trend in Hollywood towards co-financing (over two-thirds of the films put out by Warner Bros. in 2000 were joint ventures, up from 10% in 1987).[14] A hopeful director is almost never given the opportunity to get a job on a big-budget studio film unless he or she has significant industry experience in film or television. Also, the studios rarely produce films with unknown actors, particularly in lead roles.
",Movie
"Before the advent of digital alternatives, the cost of professional film equipment and stock was also a hurdle to being able to produce, direct, or star in a traditional studio film. But the advent of consumer camcorders in 1985, and more importantly, the arrival of high-resolution digital video in the early 1990s, have lowered the technology barrier to film production significantly. Both production and post-production costs have been significantly lowered; in the 2000s, the hardware and software for post-production can be installed in a commodity-based personal computer. Technologies such as DVDs, FireWire connections and a wide variety of professional and consumer-grade video editing software make film-making relatively affordable.
",Movie
"Since the introduction of digital video DV technology, the means of production have become more democratized. Filmmakers can conceivably shoot a film with a digital video camera and edit the film, create and edit the sound and music, and mix the final cut on a high-end home computer. However, while the means of production may be democratized, financing, distribution, and marketing remain difficult to accomplish outside the traditional system. Most independent filmmakers rely on film festivals to get their films noticed and sold for distribution. The arrival of internet-based video websites such as YouTube and Veoh has further changed the filmmaking landscape, enabling indie filmmakers to make their films available to the public.
",Movie
"An open content film is much like an independent film, but it is produced through open collaborations; its source material is available under a license which is permissive enough to allow other parties to create fan fiction or derivative works, than a traditional copyright. Like independent filmmaking, open source filmmaking takes place outside Hollywood, or other major studio systems.
",Movie
"A fan film is a film or video inspired by a film, television program, comic book or a similar source, created by fans rather than by the source's copyright holders or creators. Fan filmmakers have traditionally been amateurs, but some of the most notable films have actually been produced by professional filmmakers as film school class projects or as demonstration reels. Fan films vary tremendously in length, from short faux-teaser trailers for non-existent motion pictures to rarer full-length motion pictures.
",Movie
"Film distribution is the process through which a film is made available for viewing by an audience. This is normally the task of a professional film distributor, who would determine the marketing strategy of the film, the media by which a film is to be exhibited or made available for viewing, and may set the release date and other matters. The film may be exhibited directly to the public either through a movie theater (historically the main way films were distributed) or television for personal home viewing (including on DVD-Video or Blu-ray Disc, video-on-demand, online downloading, television programs through broadcast syndication etc.). Other ways of distributing a film include rental or personal purchase of the film in a variety of media and formats, such as VHS tape or DVD, or Internet downloading of streaming using a computer.
",Movie
"Animation is a technique in which each frame of a film is produced individually, whether generated as a computer graphic, or by photographing a drawn image, or by repeatedly making small changes to a model unit (see claymation and stop motion), and then photographing the result with a special animation camera. When the frames are strung together and the resulting film is viewed at a speed of 16 or more frames per second, there is an illusion of continuous movement (due to the phi phenomenon). Generating such a film is very labor-intensive and tedious, though the development of computer animation has greatly sped up the process. Because animation is very time-consuming and often very expensive to produce, the majority of animation for TV and films comes from professional animation studios. However, the field of independent animation has existed at least since the 1950s, with animation being produced by independent studios (and sometimes by a single person). Several independent animation producers have gone on to enter the professional animation industry.
",Movie
"Limited animation is a way of increasing production and decreasing costs of animation by using ""short cuts"" in the animation process. This method was pioneered by UPA and popularized by Hanna-Barbera in the United States, and by Osamu Tezuka in Japan, and adapted by other studios as cartoons moved from movie theaters to television.[15] Although most animation studios are now using digital technologies in their productions, there is a specific style of animation that depends on film. Camera-less animation, made famous by film-makers like Norman McLaren, Len Lye, and Stan Brakhage, is painted and drawn directly onto pieces of film, and then run through a projector.
",Movie
"
",Movie
"The film industry or motion picture industry, comprises the technological and commercial institutions of filmmaking, i.e., film production companies, film studios, cinematography, animation, film production, screenwriting, pre-production, post production, film festivals, distribution and actors, film directors and other film crew personnel.
",Movie
"Though the expense involved in making films almost immediately led film production to concentrate under the auspices of standing production companies, advances in affordable film making equipment, and expansion of opportunities to acquire investment capital from outside the film industry itself, have allowed independent film production to evolve. Hollywood is the oldest film industry of the world, and the largest in terms of box office gross revenue. Indian cinema (including Bollywood) is the largest film industry in terms of the number of films produced and the number of tickets sold, with 3.5 billion tickets sold worldwide annually (compared to Hollywood's 2.6 billion tickets sold annually)[1] and 1,986 feature films produced annually.[2]
",Movie
"The worldwide theatrical market had a box office of US$38.6 billion in 2016. The top three continents/regions by box office gross were: Asia-Pacific with US$14.9 billion, the U.S. and Canada with US$11.4 billion, and Europe, the Middle East and North Africa with US$9.5 billion.[3][4] As of  2016[update], the largest markets by box office were, in decreasing order, the United States, China, Japan, India, and the United Kingdom. As of  2011[update], the countries with the largest number of film productions were India, Nigeria, and the United States.[5] In Europe, significant centers of movie production are France, Germany, Italy, Spain, and the United Kingdom.[6]
",Movie
"Distinct from the centers are the locations where movies are filmed. Because of labor and infrastructure costs, many films are produced in countries other than the one in which the company which pays for the film is located. For example, many U.S. movies are filmed in Canada, many Nigerian movies are filmed in Ghana, while many Indian movies are filmed in the Americas, Europe, Singapore etc.
",Movie
"The cinema of the United States, often generally referred to as Hollywood, has had a profound effect on cinema across the world since the early 20th century. The United States cinema (Hollywood) is the oldest film industry in the world which originated more than 121 years ago and also the largest film industry in terms of revenue. Hollywood is the primary nexus of the U.S. film industry with established film study facilities such as the American Film Institute, LA Film School and NYFA being established in the area.[7] However, four of the six major film studios are owned by East Coast companies. The major film studios of Hollywood including Metro-Goldwyn-Mayer, 20th Century Fox, Paramount Pictures and Lightstorm Entertainment are the primary source of the most commercially successful movies in the world, such as  Star Wars (1977), and Titanic (1997).
",Movie
"American film studios today collectively generate several hundred movies every year, making the United States one of the most prolific producers of films in the world. Only The Walt Disney Company — which owns the Walt Disney Studios — is fully based in Southern California.[8] And while Sony Pictures Entertainment is headquartered in Culver City, California, its parent company, the Sony Corporation, is headquartered in Tokyo, Japan. Most shooting now takes place in California, New York, Louisiana, Georgia and North Carolina. Hollywood is the most popular film industry with the highest number of screens, and is the highest-grossing film industry in the world. Between 2009-2015, Hollywood consistently grossed $10 billion (or more) annually.[9] Hollywood's award ceremony, the Academy Awards, officially known as The Oscars, is held by the Academy of Motion Picture Arts and Sciences (AMPAS) every year and a total of 2,947 Oscars have been awarded since the inception of the award.[10]
",Movie
"The earliest documented account of an exhibition of projected motion pictures in the United States was in June 1894 in Richmond, Indiana by Charles Francis Jenkins which makes United States cinema the earliest cinema in the whole world. Jenkins used his Phantoscope to project his film before an audience of family, friends and reporters. The film featured a vaudeville dancer performing a Butterfly Dance. Jenkins and his new partner Thomas Armat modified the Phantoscope for exhibitions in temporary theaters at the Cotton States Exposition in the fall of 1895. The Phantoscope was later sold to Thomas Edison, who changed the name of the projector to Edison's Vitascope.
",Movie
"Nestor Studios was Hollywood's first movie studio, founded on 27 October 1911. It was built by David Horsley for Nestor Motion Picture Company. It was then owned and operated by David Horsley and his brother, William Horsley. The first motion picture stage in Hollywood was built behind the tavern. Other East Coast studios had moved production to Los Angeles, prior to Nestor's move
west. The California weather allowed for year-round filming and the ambitious studio operated three principal divisions under its Canadian-born general manager, Al Christie. Other filmmakers began opening studios in the Hollywood area. The Horsleys operated the Nestor Studios at the Sunset and Gower location until 20 May 1912, when the Universal Studios was formed, headed by Carl Laemmle. Nestor, along with several other motion picture companies, including Laemmle's Independent Moving Pictures (IMP), was merged with Universal.
",Movie
"The Cinema of China is one of three distinct historical threads of Chinese-language cinema together with the cinema of Hong Kong and the cinema of Taiwan. Cinema was introduced in China in 1896 and the first Chinese film, The Battle of Dingjunshan, was made in 1905, with the film industry being centered on Shanghai in the first decades. China is the home of the largest film studio in the world, the Hengdian World Studios, and in 2010 it had the third largest film industry by number of feature films produced annually. For the next decade the production companies were mainly foreign-owned, and the domestic film industry was centered on Shanghai, a thriving entrepot and the largest city in the Far East. In 1913, the first independent Chinese screenplay, The Difficult Couple, was filmed in Shanghai by Zheng Zhengqiu and Zhang Shichuan.[11]
",Movie
"As the Sixth Generation gained international exposure, many subsequent movies were joint ventures and projects with international backers, but remained quite resolutely low-key and low budget. Jia's Platform (2000) was funded in part by Takeshi Kitano's production house,[12] while his Still Life was shot on HD video. Still Life was a surprise addition and Golden Lion winner of the 2006 Venice International Film Festival. Still Life, which concerns provincial workers around the Three Gorges region, sharply contrasts with the works of Fifth Generation Chinese directors like Zhang Yimou and Chen Kaige who were at the time producing House of Flying Daggers (2004) and The Promise (2005). It featured no star of international renown and was acted mostly by non-professionals. In 2012 the country became the second-largest market in the world by box office receipts. In 2014, the gross box office in China was ¥29.6 billion (US$4.82 billion), with domestic films having a share of 55%. The country is predicted to have the largest market in the world in 2017 or 2018.[13][14] China has also become a major hub of business for Hollywood studios.[15][16] In 2013, China's gross box office was ¥21.8 billion (US$3.6 billion), the second-largest film market in the world by box office receipts[17] whereas in 2014, China's box office gross was $4.8 Billion, being the second largest box office grosser in film industry.[18]
",Movie
"India is the largest producer of films in the world and second oldest film industry in the world which originated around about 105 years ago[when?].[19] In 2009 India produced a total of 2,961 films on celluloid; this figure includes 1,288 feature films.[20] Besides being the largest producer of films in the world, India also has the largest number of admissions.[21] Indian film industry is multi-lingual and the largest in the world in terms of ticket sales but 3rd largest in terms of revenue mainly due to having amongst the lowest ticket prices in the world.[22] The industry is viewed mainly by a vast film-going Indian public, and Indian films have been gaining increasing popularity in the rest of the world—notably in countries with large numbers of expatriate Indians. Indian film industry is also the dominant source of movies and entertainment in its neighboring countries of South Asia. The largest film and most popular industry in India is the Hindi film industry mostly concentrated in Mumbai (Bombay),[23] and is commonly referred to as Bollywood, an amalgamation of Bombay and Hollywood.
",Movie
"The other largest film industries are Tamil cinema, Telugu cinema, Kannada cinema, Malayalam cinema, Bangla cinema (cinema of West Bengal) and Marathi cinema, which are located in Chennai, Hyderabad, Bengaluru, Kochi, Kolkata and Mumbai. The remaining majority portion is spread across northern, western, and southern India (with Gujarati, Punjabi, Oriya, Bhojpuri, Assamese Cinema). However, there are several smaller centers of Indian film industries in regional languages centered in the states where those languages are spoken. Indian films are made filled with musicals, action, romance, comedy, and an increasing number of special effects. It encloses a number of several artforms like Indian classical music, folk music of different regions throughout the country, Indian classical dance, folk dance and much more. It is even the place for number of artists from the Indian subcontinent to showcase their talent. The Indian film industry produces more than 1000 films a year. Bollywood is the largest portion of this and is viewed all over the Indian Subcontinent, and is increasingly popular in UK, United States, Australia, New Zealand, Southeast Asia, Africa, the Gulf countries, European countries and China. The largest film studio complex in the world is Ramoji Film City located at Hyderabad, India, which opened in 1996 and measures 674 ha (1,666 acres). Comprising 47 sound stages, it has permanent sets ranging from railway stations to temples.[24]
",Movie
"Bollywood represents 43% of Indian net box office revenue, while Telugu and Tamil cinema represent 36%, and the rest of the regional cinema constitute 21%, as of  2014[update].[25]
",Movie
"The highest-grossing movie of India is Dangal which is also the 5th highest grossing non-English movie in the world.[citation needed]
",Movie
"The cinema of Nigeria, often referred to informally as Nollywood, was the second largest film industry, in terms of output, in 2009 and the third largest, in terms of overall revenues generated, in 2013.[26][27] Its history dates back to as early as the late 19th century and into the colonial era in the early 20th century. The history and development of the Nigerian motion picture industry is sometimes generally classified in four main eras: the Colonial era, Golden Age, Video film era and the emerging New Nigerian cinema.[28]
",Movie
"Film as a medium first arrived Nigeria in the late 19th century, in the form of peephole viewing of motion picture devices.[29] These were soon replaced in the early 20th century with improved motion picture exhibition devices, with the first set of films screened at the Glover Memorial Hall in Lagos from 12 to 22 August 1903.[30] The earliest feature film made in Nigeria is the 1926 Palaver produced by Geoffrey Barkas; the film was also the first film ever to feature Nigerian actors in speaking roles.[31][32] The first film entirely copyrighted to the Nigerian Film unit is Fincho (1957) by Sam Zebba; which is also the first Nigerian film to be shot in colour.[33]
",Movie
"After Nigeria's independence in 1960, the cinema business rapidly expanded, with new cinema houses being established.[34] As a result, Nigerian content in theatres increased in the late 1960s into the 1970s, especially productions from Western Nigeria, owing to former theatre practitioners such as Hubert Ogunde and Moses Olaiya transitioning into the big screen.[35] In 1972, the Indigenization Decree was issued by Yakubu Gowon, which demands the transfer of ownership of about a total of 300 film theatres from their foreign owners to Nigerians, which resulted in more Nigerians playing active roles in the cinema and film.[36] The oil boom of 1973 through 1978 also contributed immensely to the spontaneous boost of the cinema culture in Nigeria, as the increased purchasing power in Nigeria made a wide range of citizens to have disposable income to spend on cinema going and on home television sets.[37]
",Movie
"After the decline of the Golden era, Nigerian film industry experienced a second major boom in the 1990s, supposedly marked by the release of the direct-to-video film Living in Bondage (1992); the industry peaked in the mid 2000s to become the second largest film industry in the world in terms of the number of annual film productions, placing it ahead of the United States and behind only India.[26] The films started dominating screens across the African continent and by extension, the Caribbeans and the diaspora,[38] with the movies significantly influencing cultures, and the film actors becoming household names across the continent. The boom also led to a backlash against Nigerian films in several countries, bordering on theories such as the ""Nigerialization of Africa"".[39][40] Since the mid-2000s, the Nigerian cinema have undergone some restructuring to promote quality and professionalism, with The Figurine (2009) widely regarded as marking the major turn around of contemporary Nigerian cinema. There has since been a resurgence in the proliferation of cinema establishments, and a steady return of the cinema culture in Nigeria.[41]
",Movie
"The 2016 film The Wedding Party, directed by Kemi Adetiba, is[when?] the highest grossing Nollywood film of all time.[citation needed]
",Movie
"Since 1976, Cairo has held the annual Cairo International Film Festival (CIFF), which is accredited by the International Federation of Film Producers Association. In 1996, the Egyptian Media Production City (EMPC) was inaugurated in 6th of October City south of Cairo, although by 2001, only one of 29 planned studios was operational.[42] Censorship, formerly an obstacle to freedom of expression, has decreased remarkably by 2012, when the Egyptian cinema had begun to tackle boldly issues ranging from sexual issues[43]  to heavy government criticism.[44]
",Movie
"The 1940s, 1950s and the 1960s are generally considered the golden age of Egyptian cinema. As in the West, films responded to the popular imagination, with most falling into predictable genres (happy endings being the norm), and many actors making careers out of playing strongly typed parts. In the words of one critic, ""If an Egyptian film intended for popular audiences lacked any of these prerequisites, it constituted a betrayal of the unwritten contract with the spectator, the results of which would manifest themselves in the box office.""[45] Since the 1990s, Egypt's cinema has gone in separate directions. Smaller art films attract some international attention but sparse attendance at home. Popular films, often broad comedies such as What A Lie!, and the extremely profitable works of comedian Mohamed Saad, battle to hold audiences either drawn to Western films or, increasingly, wary of the perceived immorality of film.[46]
",Movie
"The cinema of Iran (Persian: سینمای ایران) or cinema of Persia refers to the cinema and film industries in Iran which produce a variety of commercial films annually. Iranian art films have garnered international fame and now enjoy a global following.[47]
",Movie
"Along with China, Iran has been lauded as one of the best exporters of cinema in the 1990s.[48] Some critics now[when?] rank Iran as the world's most important national cinema, artistically, with a significance that invites comparison to Italian neorealism and similar movements in past decades.[47] A range of international film festivals have honored Iranian cinema in the last twenty years.[when?] Austrian filmmaker Michael Haneke and German filmmaker Werner Herzog have praised Iranian cinema as one of the world's most important artistic cinemas.[49]
",Movie
"See Cinema of Japan
",Movie
"The term ""cinema of Korea"" (or ""Korean cinema"") encompasses the motion picture industries of North and South Korea. As with all aspects of Korean life during the past century, the film industry has often been at the mercy of political events, from the late Joseon dynasty to the Korean War to domestic governmental interference. While both countries have relatively robust film industries today, only South Korean films have achieved wide international acclaim. North Korean films tend to portray their communist or revolutionary themes.
",Movie
"South Korean films enjoyed a ""Golden age"" during the late 1950s, and 1960s, but by the 1970s had become generally considered to be of low quality. Nonetheless, by 2005 South Korea had become one of few nations to watch more domestic than imported films in theatres[50] due largely to laws placing limits on the number of foreign films able to be shown per theatre per year.[51] In the theaters, Korean films must be played for 73 days per year since 2006. On cable TV 25% domestic film quota will be reduced to 20% after KOR-US FTA.[52]
The cinema of South Korea had a total box office gross in the country in 2015 of ₩884 billion and had 113,000,000 admissions, 52% of the total admissions.
",Movie
"Hong Kong is a filmmaking hub for the Chinese-speaking world (including the worldwide diaspora) and East Asia in general.  For decades it was the third largest motion picture industry in the world (after Bollywood and Hollywood) and the second largest exporter of films.[53] Despite an industry crisis starting in the mid-1990s and Hong Kong's return to Chinese sovereignty in July 1997 Hong Kong film has retained much of its distinctive identity and continues to play a prominent part on the world cinema stage.
Unlike many film industries, Hong Kong has enjoyed little to no direct government support, through either subsidies or import quotas.  It has always been a thoroughly commercial cinema, concentrating on crowd-pleasing genres, like comedy and action, and heavily reliant on formulas, sequels and remakes.  Typically of commercial cinemas, its heart is a highly developed star system, which in this case also features substantial overlap with the pop music industry.
",Movie
"The Yeşilçam film industry is the second largest European theatrical growth market and the 7th largest theatrical market in terms of admissions, only superseded by the ‘big 5’ EU markets and the Russian Federation.[citation needed] The Turkish film market also stands out in the pan-European landscape as the only market where national films regularly outperform US films.[54] It had 1.2 million number of admissions in film industry and 87 feature films were released in the year 2013.[55] Because of the exceptional box office success of Turkish films on the domestic market, the estimated 12.9 million admissions generated on non-national European markets only account for 7% of total admissions to Turkish films in Europe (including Turkey) between 2004 and 2013. This is the third lowest share among the 30 European markets for which such data are available and clearly illustrates the strong dependence of Turkish films on the domestic market, a feature which is shared by Polish and Russian films.[56]
",Movie
"Over the past ten years an increasing number of Turkish films and filmmakers have been selected for international film festivals and received a large number of awards, like Kış Uykusu (Winter's Sleep) won Cannes Film Festival Award for Best Film in 2014.[57] In terms of box office Turkey still ranks behind the Netherlands with just over EUR 200 million as Europe's eight largest box office market ahead of Sweden and Switzerland with a clear gap to the top 6 markets all of which registered GBO between EUR 504 million (Spain) up to over EUR 1 billion in France, the UK, Germany and the Russian Federation.[58] Cinema going is comparatively cheap in Turkey. In 2013 a cinema ticket cost on average EUR 4.0 in Turkey, and this is estimated to be the lowest average ticket price - measured in Euro - in Europe, marginally cheaper than in several Central and Eastern European markets like Croatia, Romania, Lithuania or Bulgaria.[59] When comparing ticket prices in Euro, one of course has to take into consideration that these comparisons are significantly affected by fluctuations in the exchange rates of the various currencies. Because of devaluation of the Turkish Lira against the Euro, average ticket prices measured in Euro remained fairly stable over the past 10 years.[59]
",Movie
"The cinema of Pakistan, or simply, Pakistani cinema (Urdu: پاکستانی سنیما‬‎) refers to Pakistan's film industry. Most of the feature films shot in Pakistan are in Urdu, the national language, but may also include films in English, the official language, and regional languages such as Punjabi, Pashto, Balochi, and Sindhi. Lahore has been described as the epicentre of Pakistani cinema, giving rise to the term ""Lollywood.""
",Movie
"Before the separation of Bangladesh, Pakistan had three main film production centres: Lahore, Karachi and Dhaka. The regime of Muhammad Zia-ul-Haq, VCRs, film piracy, the introduction of entertainment taxes, strict laws based upon ultra-conservative jurisprudence, was an obstacle to the industry's growth.[60] Once thriving, the cinema in Pakistan had a sudden collapse in the 1980s and by the 2000s ""an industry that once produced an average of 80 films annually was now struggling to even churn out more than two films a year."".[61][62] However, the industry has recently made a dramatic and remarkable comeback, evident from the fact that 18 of the 21 highest grossing Pakistani movies were released from 2013 through to the present, with Pakistani films frequently outcompeting Bollywood movies for the Pakistani audience, the industry is supported by Pakistani channels such as ARY and Geo whose entertainment divisions have invested significantly in Pakistani cinema when expanding from providing news and entertainment on TV channels, the lifting of strict regulations on production of films and reduction of taxes on cinemas helped to fuel an expansion across the industry from which the film industry has seen a revival.[citation needed]
",Movie
"The cinema of Bangladesh is the Bengali language film industry based in Dhaka, Bangladesh. The industry often has been a significant film industry since the early 1970s. The word ""Dhallywood"" is a portmanteau of the words Dhaka and Hollywood. The dominant style of Bangladeshi cinema is Melodramatic cinema, which developed from 1947 to 1990 and characterizes most films to this day. Cinema was introduced in Bangladesh in 1898 by Bradford Bioscope Company, credited to have arranged the first film release in Bangladesh. Between 1913 and 1914, the first production company named Picture House was opened. A short silent film titled Sukumari (The Good Girl) was the first produced film in the region during 1928. The first full-length film The Last Kiss, was released in 1931. From the separation of Bangladesh from Pakistan, Dhaka is the center of Bangladeshi film industry, and generated the majority share of revenue, production and audiences. The 1960s, 1970s, 1980s and the first half of the 1990s were the golden years for Bangladeshi films as the industry produced many successful films. The Face and the Mask, the first Bengali language Bangladeshi full-length feature film was produced in 1956.[63][64]
",Movie
"Directors such as Fateh Lohani, Zahir Raihan, Alamgir Kabir, Khan Ataur Rahman, Subhash Dutta, Ritwik Ghatak, Ehtesham, Chashi Nazrul Islam, Abdullah al Mamun, Sheikh Niamat Ali, Gazi Mazharul Anwar, Tanvir Mokammel, Tareque Masud, Morshedul Islam, Humayun Ahmed, Gias Uddin Selim, Amitabh Reza Chowdhury and others have made significant contributions to Bangladeshi mainstream cinema, parallel cinema, art films and won global acclaim. 
",Movie
"Matir Moina, a 2002 film by Tarek Masud, became the first Bangladeshi film to be honored at Cannes Film Festival. From the year 2014, a crossover between Bangladeshi actors and West Bengal's actors are heavily seen in the name of Bangladesh-India joint production. Among them is Agnee 2, Nawab, Boss 2, Black), Ekti Cinemar Golpo to name a few. Joya Ahsan earned huge reputation for 2015 film Rajkahini and Bisorjon (2017). The former was remade in Hindi as Begum Jaan and for the latter, Joya received coveted Filmfare East Award for Best Actress, the most prestigious award in Indian cinema. In course of time, several actors have worked in Bollywood Films as well. Which include Shabana Sadique in Shatru, Ferdous Ahmed in Mitti etc.[citation needed]
",Movie
"The biggest film studios in Southeast Asia has been soft opened on 5 November 2011 on 10 hectares of land in Nongsa, Batam Island, Indonesia. Infinite Frameworks (IFW) is a Singapore-based company (closed to Batam Island) which is owned by a consortium with 90 percent of it hold by Indonesian businessman and movie producer, Mike Wiluan.[65] In 2010-2011, due to the substantial increase in value added tax applied to foreign films, cinemas no longer had access to many foreign films, including Oscar-winning films. Foreign films include major box offices from the west, and other major film producers of the world. This has caused a massive ripple effect on the country's economy. It is assumed that this increases the purchase of unlicensed DVDs. However, even copyright violating DVDs took longer to obtain. The minimum cost to view a foreign film not screened locally, was 1 million Rupiah. This is equivalent to US$100, as it includes a plane ticket to Singapore.[66]
",Movie
"Trinidad and Tobago's film sector began emerging in the late fifties to early sixties and by the late seventies, there were a handful of local productions, both feature film and television.[67] The first full-length feature film to be produced in Trinidad and Tobago was “The Right and the Wrong” (1970) by Indian director/writer/producer, Harbance Kumar. The screenplay was written by the Trinidadian playwright, Freddie Kissoon.[68] The rest of the 20th century saw a couple more feature films being made in the country, with “Bim” (1974), being singled out by Bruce Paddington as ""one of the most important films to be produced in Trinidad and Tobago….and one of the classics of Caribbean cinema.”[69] It was one of the first films to feature an almost entirely Trinidadian cast and crew.[70] There was a rise in Trinidadian film production in the 2000s. Movies such as “Ivan the Terrible” (2004), “SistaGod” (2006), “I’m Santana: The Movie” (2012) and “God Loves the Fighter” (2013) were released both locally and internationally. “SistaGod” had its world premiere at the 2006 Toronto International Film Festival.[71]
",Movie
"The Trinidad and Tobago Film Company is the national agency that was established in 2006 to further development of the film industry. Trinidad and Tobago puts on a number of film festivals which are organized by different committees and organizations. These include the Secondary Schools Short Film Festival and Smartphone Film Festival organized by Trinidad and Tobago Film Company. There is also an annual Trinidad and Tobago Film Festival which runs for two weeks in the latter half of September.
",Movie
"Nepali film does not have a very long movie history, but the industry has its own place in the cultural heritage of the country. It is often referred to as 'Nepali Chalchitra' (which translates to ""Nepali movies"" in English). The terms Kollywood and Kallywood are also used, as a portmanteau of ""Kathmandu"" and ""Hollywood""; ""Kollywood"" however is more frequently used to refer to Tamil cinema.[1] Chhakka Panja has been considered the highest-grossing movie of all time in Nepali Movie Industry and Kohinoor the second highest. Nepali movies has recently begun receiving international acclaim with films such as The Black Hen (2015), Kagbeni (2006) and others. Nepali feature film White Sun (Seto Surya) has bagged the Best Film award at the 27th Singapore International Film Festival (SGIFF)(2016).
",Movie
"The Film Development Board (FDB) was established by the Government of Nepal for the development and promotion of the Nepali film industry. The Board is a liaison to facilitate the conceptualization, making, distribution and exhibition of Nepali films nationally. The Board attempts to bridge the gap between film entrepreneurship and government bureaucracy. The Board is a balance between the people at large, the government, and the process of film making. It is intended to act as the safeguard of the interests of the people, the watchdog of the government, and the advocate of filmmakers.
",Movie
"The first feature film to be made was the 1906 Australian silent The Story of the Kelly Gang, an account of the notorious gang led by Ned Kelly that was directed and produced by the Melburnians Dan Barry and Charles Tait. It ran, continuously, for eighty minutes.[72]
",Movie
"In the early 1910s, the film industry had fully emerged with D.W. Griffith's The Birth of a Nation. Also in the early 1900s motion picture production companies from New York and New Jersey started moving to California because of the good weather and longer days. Although electric lights existed at that time, none were powerful enough to adequately expose film; the best source of illumination for movie production was natural sunlight. Besides the moderate, dry climate, they were also drawn to the state because of its open spaces and wide variety of natural scenery.
",Movie
"The earliest documented account of an exhibition of projected motion pictures in the United States was in June 1894 in Richmond, Indiana by Charles Francis Jenkins.[citation needed] The first movie studio in the Hollywood area, Nestor Studios, was founded in 1911 by Al Christie for David Horsley in an old building on the northwest corner of Sunset Boulevard and Gower Street.  In the same year, another fifteen Independents settled in Hollywood.  Hollywood came to be so strongly associated with the film industry that the word ""Hollywood"" came to be used colloquially to refer to the entire industry.
",Movie
"In 1913 Cecil B. DeMille, in association with Jesse Lasky, leased a barn with studio facilities on the southeast corner of Selma and Vine Streets from the Burns and Revier Studio and Laboratory, which had been established there.  DeMille then began production of The Squaw Man (1914). It became known as the Lasky-DeMille Barn and is currently the location of the Hollywood Heritage Museum.
",Movie
"The Charlie Chaplin Studios, on the northeast corner of La Brea and De Longpre Avenues just south of Sunset Boulevard, was built in 1917.  It has had many owners after 1953, including Kling Studios, which housed production for the Superman TV series with George Reeves; Red Skelton, who used the sound stages for his CBS TV variety show; and CBS, who filmed the TV series Perry Mason with Raymond Burr there.  It has also been owned by Herb Alpert's A&M Records and Tijuana Brass Enterprises.  It is currently The Jim Henson Company, home of the Muppets. In 1969 The Los Angeles Cultural Heritage Board named the studio a historical cultural monument.
",Movie
"The famous Hollywood Sign originally read ""Hollywoodland."" It was erected in 1923 to advertise a new housing development in the hills above Hollywood. For several years the sign was left to deteriorate. In 1949 the Hollywood Chamber of Commerce stepped in and offered to remove the last four letters and repair the rest.
",Movie
"The sign, located at the top of Mount Lee, is now a registered trademark and cannot be used without the permission of the Hollywood Chamber of Commerce, which also manages the venerable Walk of Fame.
",Movie
"The first Academy Awards presentation ceremony took place on 16 May 1929, during a banquet held in the Blossom Room of the Hollywood Roosevelt Hotel on Hollywood Boulevard. Tickets were USD $10.00[citation needed] and there were 250 people in attendance.
",Movie
"From about 1930 five major Hollywood movie studios from all over the Los Angeles area, Paramount, RKO, 20th Century Fox, Metro-Goldwyn-Mayer and Warner Bros., owned large, grand theaters throughout the country for the exhibition of their movies.  The period between the years 1927 (the effective end of the silent era) to 1948 is considered the age of the ""Hollywood studio system"", or, in a more common term, the Golden Age of Hollywood.  In a landmark 1948 court decision, the Supreme Court ruled that movie studios could not own theaters and play only the movies of their studio and movie stars, thus an era of Hollywood history had unofficially ended.  By the mid-1950s, when television proved a profitable enterprise that was here to stay, movie studios started also being used for the production of programming in that medium, which is still the norm today.
",Movie
"Bollywood is the Hindi-language film industry based in Mumbai (formerly known as Bombay), Maharashtra, India. The term is often incorrectly used to refer to the whole of Indian cinema; however, it is only a part of the total Indian film industry, which includes other production centres producing films in multiple languages.[73] Bollywood is the largest film producer in India and one of the largest centres of film production in the world.[74][75][76]
",Movie
"Bollywood is formally referred to as Hindi cinema.[77] Linguistically, Bollywood films tend to use a colloquial dialect of Hindi-Urdu, or Hindustani, mutually intelligible to both Hindi and Urdu speakers,[78][79][80] while modern Bollywood films also increasingly incorporate elements of Hinglish.[78]
",Movie
"The Wrestlers (1899) and The Man and His Monkeys (1899), directed and produced by Harischandra Sakharam Bhatawdekar (H. S. Bhatavdekar), were the first two films made by Indian filmmakers, which were both short films. He was also the first Indian filmmaker to direct and produce the first documentary and news related film, titled The Landing of Sir M.M. Bhownuggree.
",Movie
"Pundalik (Shree Pundalik) (1912), by Dadasaheb Torne alias Rama Chandra Gopal, and Raja Harishchandra (1913), by Dadasaheb Phalke, were the first and second silent feature films respectively made in India.[81][82][83][84] By the 1930s the industry was producing more than 200 films per annum.[85] The first Indian sound film, Ardeshir Irani's Alam Ara (1931), was a major commercial success.[86] There was clearly a huge market for talkies and musicals; Bollywood and all the regional film industries quickly switched to sound filming. Joymoti (1935 film) by Jyoti Prasad Agarwalla was the first Indian dubbed film, released in Calcutta on 10 March 1935. Till then, all dialogues of all talkies were had to be recorded at locations during the shooting of the film. Through Joymoti (1935 film), dubbing technology was successfully introduced to Indian cinema by Assamese filmmaker Jyoti Prasad Agarwalla.[82]
",Movie
"The 1930s and 1940s were tumultuous times: India was buffeted by the Great Depression, World War II, the Indian independence movement, and the violence of the Partition. Most Bollywood films were unabashedly escapist, but there were also a number of filmmakers who tackled tough social issues, or used the struggle for Indian independence as a backdrop for their plots.[85]
",Movie
"In 1937 Ardeshir Irani, of Alam Ara fame, made the first colour film in Hindi, Kisan Kanya. The next year, he made another colour film, a version of Mother India. However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were the staple fare at the cinema.
",Movie
"Following India's independence, the period from the late 1940s to the early 1960s is regarded by film historians as the ""Golden Age"" of Hindi cinema.[87][88][89] Defining key figures during this time included Raj Kapoor, Guru Dutt,[90] Mehboob Khan,[91][92][93] and Dilip Kumar.[94][95]
",Movie
"The 1970s was when the name ""Bollywood"" was coined,[96][97] and when the quintessential conventions of commercial Bollywood films were established.[98] Key to this was the emergence of the masala film genre, which combines elements of multiple genres (action, comedy, romance, drama, melodrama, musical). The masala film was pioneered in the early 1970s by filmmaker Nasir Hussain,[99] along with screenwriter duo Salim-Javed, pioneering the Bollywood blockbuster format.[98]
",Movie
"Profitability of a film studio is crucially dependent on picking the right film projects and involving the right management and creative teams (cast, direction, visual design, score, photography, costume, set design, editing, and many additional specialties), but it also depends heavily on choosing the right scale and approach to film promotion, control over receipts through technologies such as digital rights management (DRM), sophisticated accounting practices, and management of ancillary revenue streams; in the extreme, for a major media franchise centered on film, the film might itself be only one large component of many large contributions to total franchise revenue.
",Movie
"Substantial revenues might also derive from licensing efforts and other activities centered around the management of intellectual property; for this reason, various national film industries are heavily invested in the politics and legal specifics of multilateral trade agreements. Hollywood is a large enough industry to impact America's net balance of trade, offsetting some of the decline of employment in the manufacturing sector of the American economy since the early 1980s; typical of recent economic trends in first world economies, a large number of manufacturing jobs paying moderate wages are offset by a smaller number of creating jobs paying high wages. As an intellectual property industry, the film industry (along with the software industry) are central to modern globalization.
",Movie
"The film industry differs from software, however, in functioning primarily as a cultural export. English-language culture is the most globalized of all cultures, despite China and India having internal populations to rival the global English footprint. For this reason, Hollywood is the dominant film industry in globalism and trade. India has both a large film industry and a large, internal English-language culture, though Bollywood has not yet become a center for English-language film exports on a major scale. Films designed for global export tend to be films centered on action rather than dialogue, especially nuanced dialogue which is not fully available to people speaking English as a second language (ESL). Unusually, Japanese Anime has a global audience, though these are sometimes dubbed into English by star-power Hollywood voice actors.
",Movie
"On the creative side, picking the right projects has traditionally been regarded as high risk. Increasingly the risk is being mitigated with approaches drawn from advanced data analytics, such as using models derived from machine learning (sometimes glorified as AI) to estimate the future revenue of a proposed project.[100]
",Movie
"The following is a list of the top 15 countries by the number of feature films (fiction, animation and documentary) produced, as determined by the UNESCO Institute for Statistics as of  2015[update],[101] unless otherwise noted.
",Movie
"
",Movie
"The Cinema of India consists of films produced in the nation of India.[8] Cinema is immensely popular in India, with as many as 1,600 films produced in various languages every year.[9][10] Indian cinema produces more films watched by more people than any other country; in 2011, over 3.5 billion tickets were sold across the globe, 900,000 more than Hollywood. Indian cinema is sometimes colloquially known as Indywood.[11]
",Movie
"As of 2013 India ranked first in terms of annual film output, followed by Nigeria,[9][12] Hollywood and China.[13] In 2012, India produced 1,602 feature films.[9] The Indian film industry reached overall revenues of $1.86 billion (₹93 billion) in 2011. In 2015, India had a total box office gross of US$2.1 billion,[7][14] third largest in the world.
",Movie
"Indian cinema is a global enterprise.[15] Its films have a following throughout Southern Asia, and across Asia, Europe, the Greater Middle East, North America, Eastern Africa, China and elsewhere, reaching in over 90 countries.[16] Biopics including Dangal became transnational blockbusters grossing over $300 million worldwide.[17]
",Movie
"Global enterprises such as 20th Century Fox, Sony Pictures, Walt Disney Pictures[18][19] and Warner Bros invested in the industry along with Indian enterprises such as AVM Productions, Prasad's Group, Sun Pictures, PVP Cinemas, Zee, UTV, Suresh Productions, Eros International, Ayngaran International, Pyramid Saimira, Aascar Films and Adlabs. By 2003 as many as 30 film production companies had been listed in the National Stock Exchange of India (NSE).[20]
",Movie
"The overall revenue of Indian cinema reached US$1.3 billion in 2000.[21] The industry is segmented by language. The Hindi language film industry is known as Bollywood, the largest sector, representing 43% of box office revenue. Combined Tamil (Kollywood) and Telugu (Tollywood) film industries revenues represent 36%.[22] The South Indian film industry encompasses five film cultures: Tamil, Telugu, Malayalam, Kannada and Tulu.
",Movie
"Millions of Indians overseas watch Indian films, accounting for some 12% of revenues.[23] Music rights alone account for 4–5% of net revenues.[21]
",Movie
"The history of cinema in India extends back to the beginning of the film era. Following the screening of the Lumière and Robert Paul moving pictures in London (1896), commercial cinematography became a worldwide sensation and by mid-1896 both Lumière and Robert Paul films had been shown in Bombay.[24]
",Movie
"In 1897, a film presentation by one Professor Stevenson featured a stage show at Calcutta's Star Theatre. With Stevenson's encouragement and camera Hiralal Sen, an Indian photographer, made a film of scenes from that show, namely The Flower of Persia (1898).[25] The Wrestlers (1899) by H. S. Bhatavdekar, showing a wrestling match at the Hanging Gardens in Bombay, was the first film to be shot by an Indian and the first Indian documentary film.[citation needed]
",Movie
"The first Indian film released in India was Shree Pundalik, a silent film in Marathi by Dadasaheb Torne on 18 May 1912 at Coronation Cinematograph, Bombay.[26][27] Some have argued that Pundalik was not the first Indian film, because it was a photographic recording of a play, and because the cameraman was a British man named Johnson and the film was processed in London.[28][29]
",Movie
"Advertisement in The Times of India of 25 May 1912 announcing the screening of the first feature film of India, Shree Pundalik by Dadasaheb Torne
",Movie
"A scene from Raja Harishchandra (1913), the first full-length Indian motion picture
",Movie
"Producer-director-screenwriter Dadasaheb Phalke, the ""father of Indian cinema""[30][31][32][33]
",Movie
"AVM Studios in Chennai, India's oldest surviving film studio
",Movie
"The first full-length motion picture in India was produced by Dadasaheb Phalke, Phalke is seen as the pioneer of the Indian film industry and a scholar of India's languages and culture. He employed elements from Sanskrit epics to produce his Raja Harishchandra (1913), a silent film in Marathi. The female characters in the film were played by male actors.[34] Only one print of the film was made, for showing at the Coronation Cinematograph on 3 May 1913. It was a commercial success. The first silent film in Tamil, Keechaka Vadham was made by R. Nataraja Mudaliar in 1916.[35]
",Movie
"The first chain of Indian cinemas, Madan Theatre was owned by Parsi entrepreneur Jamshedji Framji Madan, who oversaw production of 10 films annually and distributed them throughout India beginning in 1902.[34] He founded Elphinstone Bioscope Company in Calcutta. Elphinstone merged into Madan Theatres Limited in 1919, which had brought many of Bengal's most popular literary works to the stage. He also produced Satyawadi Raja Harishchandra in 1917, a remake of Phalke's Raja Harishchandra (1913).
",Movie
"Raghupathi Venkaiah Naidu was an Indian artist and a film pioneer.[36] From 1909, he was involved in many aspects of Indian cinema, travelling across Asia. He was the first to build and own cinemas in Madras. He was credited as the father of Telugu cinema. In South India, the first Tamil talkie Kalidas was released on 31 October 1931.[37] Nataraja Mudaliar established South India's first film studio in Madras.[38]
",Movie
"Film steadily gained popularity across India. Tickets were affordable to the masses (as low as an anna (one-sixteenth of a rupee) in Bombay) with additional comforts available at a higher price.[24]
",Movie
"Young producers began to incorporate elements of Indian social life and culture into cinema. Others brought ideas from across the world. Global audiences and markets soon became aware of India's film industry.[39]
",Movie
"In 1927, the British Government, to promote the market in India for British films over American ones, formed the Indian Cinematograph Enquiry Committee. The ICC consisted of three Brits and three Indians, led by T. Rangachari, a Madras lawyer.[40] This committee failed to support the desired recommendations of supporting British Film, instead recommending support for the fledgling Indian film industry. Their suggestions were shelved.
",Movie
"Ardeshir Irani released Alam Ara, the first Indian talkie, on 14 March 1931.[34] Irani later produced the first south Indian talkie film Kalidas directed by H. M. Reddy released on 31 October 1931.[41][42] Jumai Shasthi was the first Bengali talkie. Chittor V. Nagaiah, was one of the first multilingual film actor/singer/composer/producer/directors in India. He was known as India's Paul Muni.[43][44]
",Movie
"In 1932, the name ""Tollywood"" was coined for the Bengali film industry because Tollygunge rhymed with ""Hollywood"". Tollygunge was then the centre of the Indian film industry. Bombay later overtook Tollygunge as the industry's center, spawning ""Bollywood"" and many other Hollywood-inspired names.[45]
",Movie
"In 1933, East India Film Company produced its first Telugu film, Savitri. Based on a stage play by Mylavaram Bala Bharathi Samajam, the film was directed by C. Pullaiah with stage actors Vemuri Gaggaiah and Dasari Ramathilakam.[46] The film received an honorary diploma at the 2nd Venice International Film Festival.[47]
",Movie
"On 10 March 1935, another pioneer film maker Jyoti Prasad Agarwala made his first film Joymoti in Assamese. Jyoti Prasad went to Berlin to learn more about films. Indramalati is another film he himself produced and directed after Joymoti.  The first film studio in South India, Durga Cinetone was built in 1936 by Nidamarthi Surayya in Rajahmundry, Andhra Pradesh.[48] The 1930s saw the rise of music in Indian cinema with musicals such as Indra Sabha and Devi Devyani marking the beginning of song-and-dance in Indian films.[34] Studios emerged by 1935 in major cities such as Madras, Calcutta and Bombay as filmmaking became an established craft, exemplified by the success of Devdas.[49] directed by an Assamese film maker Pramathesh Baruah. In 1937, Kisan Kanhiya directed by Moti B was released, the first colour film made in India.[50] The 1940 film, Vishwa Mohini, is the first Indian film to depict the Indian movie world. The film was directed by Y. V. Rao and scripted by Balijepalli Lakshmikanta Kavi.[51]
",Movie
"Swamikannu Vincent, who had built the first cinema of South India in Coimbatore, introduced the concept of ""Tent Cinema"" in which a tent was erected on a stretch of open land to screen films. The first of its kind was in Madras, called Edison's Grand Cinemamegaphone. This was due to the fact that electric carbons were used for motion picture projectors.[52] Bombay Talkies opened in 1934 and Prabhat Studios in Pune began production of Marathi films meant.[49] R. S. D. Choudhury produced Wrath (1930), which was banned by the British Raj for its depiction of Indian actors as leaders during the Indian independence movement.[34] Sant Tukaram, a 1936 film based on the life of Tukaram (1608–50), a Varkari Sant and spiritual poet became the first Indian film to be screened at an international film festival, at the 1937 edition of the Venice Film Festival. The film was judged one of the three best films of the year.[53] In 1938, Gudavalli Ramabrahmam, co-produced and directed the social problem film, Raithu Bidda, which was also banned by the British administration, for depicting the peasant uprising among the Zamindars during the British raj.[54][55]
",Movie
"The Indian Masala film—a term used for mixed-genre films that combined song, dance, romance etc.—arose following World War II.[49] During the 1940s cinema in South India accounted for nearly half of India's cinema halls and cinema came to be viewed as an instrument of cultural revival.[49] The partition of India following independence divided the nation's assets and a number of studios moved to Pakistan.[49] Partition became an enduring film subject thereafter.[49]
",Movie
"After Indian independence the film industry was investigated by the S. K. Patil Commission.[56] Patil recommended setting up a Film Finance Corporation (FFC) under the Ministry of Finance.[57] This advice was adopted in 1960 and FFC provide financial support to filmmakers.[57] The Indian government had established a Films Division by 1948, which eventually became one of the world's largest documentary film producers with an annual production of over 200 short documentaries, each released in 18 languages with 9,000 prints for permanent film theatres across the country.[58]
",Movie
"The Indian People's Theatre Association (IPTA), an art movement with a communist inclination, began to take shape through the 1940s and the 1950s.[56] Realist IPTA plays, such as Nabanna (1944, Bijon Bhattacharya) prepared the ground for realism in Indian cinema, exemplified by Khwaja Ahmad Abbas's Dharti Ke Lal (Children of the Earth) in 1946.[56] The IPTA movement continued to emphasize realism and went on to produce Mother India and Pyaasa, among India's most recognizable cinematic productions.[59]
",Movie
"The period from the late 1940s to the early 1960s is regarded by film historians as the Golden Age of Indian cinema.[60][61][62]
",Movie
"This period saw the emergence of the Parallel Cinema movement, mainly led by Bengalis,[69] which then accounted for a quarter of India's film output.[70] The movement emphasized social realism. Early examples include Dharti Ke Lal (1946, Khwaja Ahmad Abbas),[71] Neecha Nagar (1946, Chetan Anand),[72] Nagarik (1952, Ritwik Ghatak)[73][74] and Do Bigha Zamin (1953, Bimal Roy), laying the foundations for Indian neorealism[75] and the Indian New Wave.[76]
",Movie
"The Apu Trilogy (1955–1959, Satyajit Ray) won major prizes at all the major international film festivals and firmly established the Parallel Cinema movement. Pather Panchali (1955), the first part of the trilogy, marked Ray's entry in Indian cinema.[77] The trilogy's influence on world cinema can be felt in the ""youthful coming-of-age dramas that flooded art houses since the mid-fifties"", which ""owe a tremendous debt to the Apu trilogy"".[78]
",Movie
"Cinematographer Subrata Mitra, who debuted in the trilogy, had his own important influence on cinematography globally. One of his most important techniques was bounce lighting, to recreate the effect of daylight on sets. He pioneered the technique while filming Aparajito (1956), the second part of the trilogy.[79] Ray pioneered other effects such as the photo-negative flashbacks and X-ray digressions in Pratidwandi (1972).[80]
",Movie
"During the 1960s, Indira Gandhi's intervention during her reign as the Information and Broadcasting Minister of India supported production of off-beat cinematic by FFC.[57]
",Movie
"Commercial Hindi cinema began thriving, including acclaimed films Pyaasa (1957) and Kaagaz Ke Phool (1959, Guru Dutt) Awaara (1951) and Shree 420 (1955, Raj Kapoor). These films expressed social themes mainly dealing with working-class urban life in India; Awaara presented the city as both a nightmare and a dream, while Pyaasa critiqued the unreality of city life.[69]
",Movie
"Epic film Mother India (1957, Mehboob Khan), a remake of his earlier Aurat (1940), was the first Indian film to be nominated for the Academy Award for Best Foreign Language Film.[81] Mother India defined the conventions of Hindi cinema for decades.[82][83][84] It spawned a new genre of dacoit films.[85] Gunga Jumna (1961, Dilip Kumar) was a dacoit crime drama about two brothers on opposite sides of the law, a theme that became common in Indian films in the 1970s.[86] Madhumati (1958, Bimal Roy) popularised the theme of reincarnation in Western popular culture.[87]
",Movie
"Dilip Kumar (Muhammad Yusuf Khan) debuted in the 1940s and rose to fame in the 1950s and was one of the biggest Indian movie stars. He was a pioneer of method acting, predating Hollywood method actors such as Marlon Brando. Much like Brando's influence on New Hollywood actors, Kumar inspired Indian actors, including Amitabh Bachchan, Naseeruddin Shah, Shah Rukh Khan and Nawazuddin Siddiqui.[88]
",Movie
"Neecha Nagar won the Palme d'Or at Cannes,[72] putting Indian films in competition for the Palme d'Or for nearly every year in the 1950s and early 1960s, with many winning major prizes. Ray won the Golden Lion at the Venice Film Festival for Aparajito (1956) and the Golden Bear and two Silver Bears for Best Director at the Berlin International Film Festival.[89] The films of screenwriter Khwaja Ahmad Abbas were nominated for the Palme d'Or three times. (Neecha Nagar won, with nominations for Awaara and Pardesi (1957)).
",Movie
"Ray's contemporaries Ghatak and Dutt were overlooked in their own lifetimes, but generated international recognition in the 1980s and 1990s.[89][90] Ray is regarded as one of the greatest auteurs of 20th century cinema,[91] with Dutt[92] and Ghatak.[93] In 1992, the Sight & Sound Critics' Poll ranked Ray at No. 7 in its list of ""Top 10 Directors"" of all time,[94] while Dutt ranked No. 73 in the 2002 Sight & Sound poll.[92]
",Movie
"Multiple films from this era are included among the greatest films of all time in various critics' and directors' polls. Multiple Ray films appeared in the Sight & Sound Critics' Poll, including The Apu Trilogy (ranked No. 4 in 1992 if votes are combined),[95] Jalsaghar (ranked No. 27 in 1992), Charulata (ranked No. 41 in 1992)[96] and Aranyer Din Ratri (ranked No. 81 in 1982).[97] The 2002 Sight & Sound critics' and directors' poll also included the Dutt films Pyaasa and Kaagaz Ke Phool (both tied at #160), Ghatak's films Meghe Dhaka Tara (ranked #231) and Komal Gandhar (ranked #346), and Raj Kapoor's Awaara, Vijay Bhatt's Baiju Bawra, Mehboob Khan's Mother India and K. Asif's Mughal-e-Azam all tied at #346.[98] In 1998, the critics' poll conducted by the Asian film magazine Cinemaya included The Apu Trilogy (ranked No. 1 if votes are combined), Ray's Charulata and Jalsaghar (both tied at #11), and Ghatak's Subarnarekha (also tied at #11).[93]
",Movie
"South Indian cinema saw the production works based on the epic Mahabharata, such as Mayabazar (listed by IBN Live's 2013 Poll as the greatest Indian film of all time).[99] Sivaji Ganesan became India's first actor to receive an international award when he won the ""Best Actor"" award at the Afro-Asian film festival in 1960 and was awarded the title of Chevalier in the Legion of Honour by the French Government in 1995.[100] Tamil cinema is influenced by Dravidian politics,[101] with prominent film personalities C N Annadurai, M G Ramachandran, M Karunanidhi and Jayalalithaa becoming Chief Ministers of Tamil Nadu.[102]
",Movie
"Kamal Haasan was introduced as child actor in 1960 Tamil language movie Kalathur Kannamma, Haasan's performance earned him the President's Gold Medal at the age of 6. Kamal Haasan acted all major Indian language Movies within the age of 25. He is one of the most experience actor in Indian cinema, he have more than 58 years of exprience.
",Movie
"Realistic Parallel Cinema continued throughout the 1970s,[103] practiced in many Indian film cultures. The FFC's art film orientation came under criticism during a Committee on Public Undertakings investigation in 1976, which accused the body of not doing enough to encourage commercial cinema.[104]
",Movie
"Hindi commercial cinema continued with films such as Aradhana (1969), Sachaa Jhutha (1970), Haathi Mere Saathi (1971), Anand (1971), Kati Patang (1971) Amar Prem (1972), Dushman (1972) and Daag (1973).
",Movie
"By the early 1970s, Hindi cinema was experiencing thematic stagnation,[107] dominated by musical romance films.[108] The arrival of screenwriter duo Salim-Javed, consisting of Salim Khan and Javed Akhtar, revitalized the industry.[107] They established the genre of gritty, violent, Bombay underworld crime films, with films such as Zanjeer (1973) and Deewaar (1975).[109][110] They reinterpreted the rural themes of Mother India and Gunga Jumna in an urban context reflecting 1970s India,[107][111] channeling the growing discontent and disillusionment among the masses,[107] unprecedented growth of slums[112] and urban poverty, corruption and crime,[113] as well as anti-establishment themes.[114] This resulted in their creation of the ""angry young man"", personified by Amitabh Bachchan,[114] who reinterpreted Kumar's performance in Gunga Jumna,[107][111] and gave a voice to the urban poor.[112]
",Movie
"By the mid-1970s, crime-action films like Zanjeer and Sholay (1975) solidified Bachchan's position as a lead actor.[104] The devotional classic Jai Santoshi Ma (1975) was made on a shoe-string budget and became a box office success and a cult classic.[104] Another important film was Deewaar (1975, Yash Chopra).[86] This crime film pitted ""a policeman against his brother, a gang leader based on the real-life smuggler Haji Mastan"", portrayed by Bachchan. Danny Boyle described it as ""absolutely key to Indian cinema"".[115]
",Movie
"""Bollywood"" was coined in the 70s,[116][117] when the conventions of commercial Bollywood films were established.[118] Key to this was Nasir Hussain and Salim-Javed's creation of the masala film genre, which combines elements of action, comedy, romance, drama, melodrama and musical.[119][118] Another Hussain/Salim-Javed concoction, Yaadon Ki Baarat (1973), was identified as the first masala film and the ""first"" quintessentially ""Bollywood"" film.[120][118] Salim-Javed wrote more successful masala films in the 1970s and 1980s.[118] Masala films made Bachchan the biggest Bollywood movie star of the period. Another landmark was Amar Akbar Anthony (1977, Manmohan Desai).[121][120] Desai further expanded the genre in the 1970s and 1980s.
",Movie
"Commercial Hindi cinema grew in the 1980s, with films such as Ek Duuje Ke Liye (1981), Himmatwala (1983), Tohfa (1984), Naam (1986), Mr India (1987), and Tezaab (1988). 
",Movie
"In the late 1980s, Hindi cinema experienced another period of stagnation, with a decline in box office turnout, due to increasing violence, decline in musical melodic quality, and rise in video piracy, leading to middle-class family audiences abandoning theaters. The turning point came with Qayamat Se Qayamat Tak (1988), directed by Mansoor Khan, written and produced by his father Nasir Hussain, and starring his cousin Aamir Khan with Juhi Chawla. Its blend of youthfulness, wholesome entertainment, emotional quotients and strong melodies lured family audiences back to the big screen.[122][123] It set a new template for Bollywood musical romance films that defined Hindi cinema in the 1990s.[123] Commercial Hindi cinema grew in the 1990s, with the release of Chaalbaaz (1989), Chandni (1989), Maine Pyar Kiya (1989), Saajan (1991), Khalnayak (1993), Darr (1993),[104] Hum Aapke Hain Koun..! (1994), Dilwale Dulhaniya Le Jayenge (1995), Dil To Pagal Hai (1997), Pyar Kiya Toh Darna Kya (1998) and Kuch Kuch Hota Hai (1998). Cult classic Bandit Queen (1994, Shekhar Kapur) received international recognition and controversy.[124][125]
",Movie
"In the late 1990s, Parallel Cinema began a resurgence in Hindi cinema, largely due to the critical and commercial success of crime filmsSatya (1998 and Vaastav). These film's launched a genre known as Mumbai noir,[126] urban films reflecting social problems there.[127]
",Movie
"Since the 1990s, the three biggest Bollywood movie stars have been the ""Three Khans"": Aamir Khan, Shah Rukh Khan, and Salman Khan.[128][129] Combined, they starred in the top ten highest-grossing Bollywood films. The three Khans have had successful careers since the late 1980s,[128] and have dominated the Indian box office since the 1990s.[130][131] Shah Rukh Khan was the most successful for most of the 1990s and 2000s, while Aamir Khan has been the most successful since the late 2000s;[132] according to Forbes, Aamir Khan is ""arguably the world's biggest movie star"" as of 2017, due to his immense popularity in India and China.[133] Other Hindi stars include Anil Kapoor, Madhuri Dixit and Kajol. Haider (2014, Vishal Bhardwaj), the third instalment of the Indian Shakespearean Trilogy after Maqbool (2003) and Omkara (2006),[134] won the People's Choice Award at the 9th Rome Film Festival in the Mondo Genere category making it the first Indian film to achieve this honor.[135]
",Movie
"The 2010s also saw the rise of a new generation of popular actors like Ranbir Kapoor, Ranveer Singh, Varun Dhawan, Sidharth Malhotra, Sushant Singh Rajput, Arjun Kapoor, Aditya Roy Kapur and Tiger Shroff, as well as actresses like Vidya Balan, Kangana Ranaut, Deepika Padukone, Sonam Kapoor, Anushka Sharma, Sonakshi Sinha, Jacqueline Fernandez, Shraddha Kapoor and Alia Bhatt, with Balan and Ranaut gaining wide recognition for successful female-centric films such as The Dirty Picture (2011), Kahaani (2012) and Queen (2014), and Tanu Weds Manu Returns (2015). Kareena Kapoor and Bipasha Basu are among the few working actresses from the 2000s who successfully completed 15 years in the industry.
",Movie
"Kannada film Samskara (1970, Pattabhirama Reddy), pioneered the parallel cinema movement in south Indian cinema. The film won Bronze Leopard at the Locarno International Film Festival.[136]
",Movie
"Telugu film Sankarabharanam (1980) dealt with the revival of Indian classical music and won the Prize of the Public at the 1981 Besancon Film Festival.[137]
",Movie
"Tamil language films appeared at multiple film festivals. Kannathil Muthamittal (Ratnam), Veyyil (Vasanthabalan) and Paruthiveeran. Kanchivaram (2009, Ameer Sultan) premiered at the Toronto International Film Festival. Tamil films were submitted by India for the Academy Award for Best Foreign Language on eight occasions.[138] Nayakan (1987, Kamal Haasan) was included in Time magazine's ""All-TIME"" 100 best movies list.[139] In 1991, Marupakkam directed by K.S. Sethu Madhavan, became the first Tamil film to win the National Film Award for Best Feature Film, the feat was repeated by Kanchivaram in 2007.[140]
",Movie
"Malayalam cinema experienced its own Golden Age in the 1980s and early 1990s. Acclaimed Malayalam filmmakers industry, included Adoor Gopalakrishnan, G. Aravindan, T. V. Chandran and Shaji N. Karun.[141] Gopalakrishnan, is often considered to be Ray's spiritual heir.[142] He directed some of his most acclaimed films during this period, including Elippathayam (1981) which won the Sutherland Trophy at the London Film Festival, as well as Mathilukal (1989) which won major prizes at the Venice Film Festival.[143] Karun's debut film Piravi (1989) won the Camera d'Or at the 1989 Cannes Film Festival, while his second film Swaham (1994) was in competition for the Palme d'Or at the 1994 event.[144] Commercial Malayalam cinema began gaining popularity with the action films of Jayan, a popular stunt actor who died while filming a helicopter stunt.
",Movie
"Salim-Javed was highly influential in South Indian cinema. In addition to writing two Kannada films, many of their Bollywood films had remakes produced in other regions, including Tamil, Telugu and Malayalam cinema. While the Bollywood directors and producers held the rights to their films in Northern India, Salim-Javed retained the rights in South India, where they sold remake rights, usually for around ₹1 lakh (equivalent to ₹27 lakh or US$38,000 in 2017) each, for films such as Zanjeer, Yaadon Ki Baarat and Don.[145] Several of these remakes became breakthroughs for Rajinikanth, who portrayed Bachchan's role for several Tamil remakes.[108][146]
",Movie
"Sridevi is widely considered as the first female superstar of Bollywood cinema due to her pan-Indian appeal and a rare actor who had an equally successful career in the  major Indian film industries: Hindi, Tamil and Telugu  . She's also the only movie star in history of Bollywood to star in the top 10 highest grossers of the year throughout her active period (1983-1997).
",Movie
"Moti Gokulsing and Wimal Dissanayake identify six major influences that have shaped Indian popular cinema:[151]
",Movie
"Sharmistha Gooptu and Bhaumik identify Indo-Persian/Islamicate culture as another major influence. In the early 20th century, Urdu was the lingua franca of popular performances across northern India, established in performance art traditions such as nautch dancing, Urdu poetry and Parsi theater. Urdu and related Hindi dialects were the most widely understood across northern India, thus Hindi-Urdu became the standardized language of early Indian talkies. One Thousand and One Nights (Arabian Nights) had a strong influence on Parsi theater, which adapted ""Persianate adventure-romances"" into films, and on early Bombay cinema where ""Arabian Nights cinema"" became a popular genre.[157] Stadtman identifies foreign influences on commercial Bollywood masala films: New Hollywood, Hong Kong martial arts cinema and Italian exploitation films.[158]
",Movie
"Like mainstream Indian popular cinema, Indian Parallel Cinema was influenced by a combination of Indian theatre and Indian literature (such as Bengali literature and Urdu poetry), but differs when it comes to foreign influences, where it is influenced more by European cinema (particularly Italian neorealism and French poetic realism) than by Hollywood. Ray cited Vittorio De Sica's Bicycle Thieves (1948) and Jean Renoir's The River (1951), on which he assisted, as influences on his debut film Pather Panchali (1955).
",Movie
"During colonial rule Indians bought film equipment from Europe.[39] The British funded wartime propaganda films during World War II, some of which showed the Indian army pitted against the Axis powers, specifically the Empire of Japan, which had managed to infiltrate India.[159] One such story was Burma Rani, which depicted civilian resistance to Japanese occupation by British and Indian forces in Myanmar.[159] Pre-independence businessmen such as J. F. Madan and Abdulally Esoofally traded in global cinema.[34]
",Movie
"Early Indian films made early inroads into the Soviet Union, Middle East, Southeast Asia[160] and China. Mainstream Indian movie stars gained international fame across Asia[161][162][163] and Eastern Europe.[164][165] For example, Indian films were more popular in the Soviet Union than Hollywood films[166][167] and occasionally domestic Soviet films.[168] From 1954 to 1991, 206 Indian films were sent to the Soviet Union, drawing higher average audience figures than domestic Soviet productions,[167][169] Films such as Awaara and Disco Dancer drew more than 60 million viewers.[170][171] Films such as Awaara, 3 Idiots and Dangal,[172][173] were one of the 20 highest-grossing films in China.[174]
",Movie
"Indian films frequently appeared in international fora and film festivals.[160] This allowed Parallel Bengali filmmakers to achieve worldwide fame.[175]
",Movie
"Tamil films gained viewers in South East Asia and other parts of the world. Chandralekha and Muthu were dubbed into Japanese[176] and grossed a record $1.6 million in 1998.[177] In 2010, Enthiran grossed a record $4 million in North America.
",Movie
"Many Asian and South Asian countries increasingly found Indian cinema as more suited to their sensibilities than Western cinema.[160] Jigna Desai holds that by the 21st century, Indian cinema had become 'deterritorialized', spreading to parts of the world where Indian expatriatres were present in significant numbers, and had become an alternative to other international cinema.[178]
",Movie
"Indian cinema more recently began influencing Western musical films, and played a particularly instrumental role in the revival of the genre in the Western world. Ray's work had a worldwide impact, with filmmakers such as Martin Scorsese,[179] James Ivory,[180] Abbas Kiarostami, François Truffaut,[181] Carlos Saura,[182] Isao Takahata and Gregory Nava[183] citing his influence, and others such as Akira Kurosawa praising his work.[184] The ""youthful coming-of-age dramas that have flooded art houses since the mid-fifties owe a tremendous debt to the Apu trilogy"".[78] Since the 1980s,  overlooked Indian filmmakers such as Ghatak[185] and Dutt[186] posthumously gained international acclaim. Baz Luhrmann stated that his successful musical film Moulin Rouge! (2001) was directly inspired by Bollywood musicals.[187] That film's success renewed interest in the then-moribund Western musical genre, subsequently fuelling a renaissance.[188] Danny Boyle's Oscar-winning film Slumdog Millionaire (2008) was directly inspired by Indian films,[115][189] and is considered to be an ""homage to Hindi commercial cinema"".[190]
",Movie
"Indian cinema has been recognised repeatedly at the Academy Awards. Indian films Mother India (1957), Salaam Bombay! (1988) and Lagaan (2001), were nominated for the Academy Award for Best Foreign Language Film. Indian Oscar winners include Bhanu Athaiya (costume designer), Ray (filmmaker), A. R. Rahman (music composer), Resul Pookutty (sound editor) and Gulzar (lyricist), Cottalango Leon and Rahul Thakkar Sci-Tech Award.[191]
",Movie
"Masala is a style of Indian cinema that mix genres in one work, especially in Bollywood, West Bengal and South India. For example, one film can portray action, comedy, drama, romance and melodrama. These films tend to be musicals, with songs filmed in picturesque locations. Plots for such movies may seem illogical and improbable to unfamiliar viewers. The genre is named after masala, a mixture of spices in Indian cuisine.
",Movie
"Parallel Cinema, also known as Art Cinema or the Indian New Wave, is known for its realism and naturalism, addressing the sociopolitical climate. This movement is distinct from mainstream Bollywood cinema and began around the same time as the French and Japanese New Waves. The movement began in Bengal (led by Ray, Sen and Ghatak) and then gained prominence in the regions. The movement was launched by Roy's Do Bigha Zamin (1953), which was both a commercial and critical success, winning the International Prize at the 1954 Cannes Film Festival.[75][76][192]  Ray's films include The Apu Trilogy. Its three films won major prizes at the Cannes, Berlin and Venice Film Festivals, and are frequently listed among the greatest films of all time.[193][194][195][196]
",Movie
"Other neo-realist filmmakers were Shyam Benegal, Karun, Gopalakrishnan[69] and Kasaravalli.[197]
",Movie
"Some Indian films are known as ""multilinguals"", filmed in similar but non-identical versions in different languages. This was done in the 1930s. According to Rajadhyaksha and Willemen in the Encyclopaedia of Indian Cinema (1994), in its most precise form, a multilingual is
",Movie
"a bilingual or a trilingual [that] was the kind of film made in the 1930s in the studio era, when different but identical takes were made of every shot in different languages, often with different leading stars but identical technical crew and music.[198]:15
",Movie
"Rajadhyaksha and Willemen note that in seeking to construct their Encyclopedia, they often found it ""extremely difficult to distinguish multilinguals in this original sense from dubbed versions, remakes, reissues or, in some cases, the same film listed with different titles, presented as separate versions in different languages ... it will take years of scholarly work to establish definitive data in this respect"".[198]:15
",Movie
"Music is a substantial revenue generator, with music rights alone accounting for 4–5% of net revenues.[21] The major film music companies are Saregama and Sony Music.[21] Film music accounts for 48% of net music sales.[21] A typical film may feature 5–6 choreographed songs.[199]
",Movie
"The demands of a multicultural, increasingly globalized Indian audience led to a mixing of local and international musical traditions.[199] Local dance and music remain a recurring theme in India and followed the Indian diaspora.[199] Playback singers such as Mohammad Rafi, Kishore Kumar, Lata Mangeshkar, S. P. Balasubrahmanyam and Yesudas drew crowds to film music stage shows.[199] In the 21st century interaction increased between Indian artists and others.[200]
",Movie
"In filmmaking, a location is any place where acting and dialogue are recorded. Sites where filming without dialog takes place is termed a second unit photography site. Filmmakers often choose to shoot on location because they believe that greater realism can be achieved in a ""real"" place. Location shooting is often motivated by budget considerations.
",Movie
"The most popular locations are the main cities for each regional industry. Other locations include Manali and Shimla in Himachal Pradesh, Srinagar and Ladakh in Jammu and Kashmir, Lucknow, Agra and Varanasi in Uttar Pradesh, Ooty in Tamil Nadu, Amritsar in Punjab, Darjeeling in West Bengal, Udaipur, Jodhpur, Jaisalmer and Jaipur in Rajasthan, Delhi, Kerala and Goa.[201][202]
",Movie
"More than 1000 production organizations operate in the Indian film industry, but few are successful. AVM Productions is the oldest surviving studio in India. Other major production houses include Yash Raj Films, Red Chillies Entertainment, Dharma Productions, Eros International, Ajay Devgn FFilms, Balaji Motion Pictures, UTV Motion Pictures, Raaj Kamal Films International,Wunderbar studios, Indian Movies Limited and Geetha Arts.[203]
",Movie
"Films are made in many cities and regions in India including Andhra Pradesh and Telangana, Assam, Bengal, Bihar, Gujarat, Haryana, Jammu, Kashmir, Jharkhand, Karnataka, Konkan (Goa), Kerala, Maharashtra, Manipur, Odisha, Punjab, Rajasthan, Tamil Nadu and Uttrakhand.
",Movie
"The Assamese language film industry traces its origin to the works of revolutionary visionary Rupkonwar Jyotiprasad Agarwala, who was a distinguished poet, playwright, composer and freedom fighter. He was instrumental in the production of the first Assamese film Joymati[205] in 1935, under the banner of Critrakala Movietone. Due to the lack of trained technicians, Jyotiprasad, while making his maiden film, had to shoulder the added responsibilities as the screenwriter, producer, director, choreographer, editor, set and costume designer, lyricist and music director. The film, completed with a budget of 60,000 rupees, was released on 10 March 1935. The picture failed miserably. Like many early films, the negatives and prints of Joymati are missing. Some effort has been made privately by Altaf Mazid to restore and subtitle what is left of the prints. Despite the significant financial loss from Joymati, a second picture, Indramalati, was released in 1939. The 21st century has produced Bollywood-style Assamese movies.[206]
",Movie
"The Bengali language cinematic tradition of Tollygunge located in West Bengal hosted masters such as Ray, Ghatak and Sen.[207] Recent Bengali films that have captured national attention include Choker Bali.(Rituparno Ghosh)[208] Bengal has produced science fiction and issue films.[209]
",Movie
"Bengali cinema dates to the 1890s, when the first ""bioscopes"" were shown in theatres in Calcutta. Within five years, Hiralal Sen set up the Royal Bioscope Company, producing scenes from the stage productions of a number of popular shows at the Star Theatre, Calcutta, Minerva Theatre and Classic Theatre. Following a long gap after Sen, Dhirendra Nath Ganguly (Known as D.G.) established Indo British Film Co, the first Bengali owned production company, in 1918. The first Bengali Feature film Billwamangal was produced in 1919 under the banner of Madan Theatre. Bilat Ferat (1921) was the IBFC's first production. Madan Theatres production of Jamai Shashthi was the first Bengali talkie.[210]
",Movie
"In 1932, the name ""Tollywood"" was coined for the Bengali film industry because Tollygunge rhymes with ""Hollywood"" and because it was then the centre of the Indian film industry.[45] The 'Parallel Cinema' movement began in Bengal. Bengali stalwarts such as Ray, Mrinal Sen, Ghatak and others earned international acclaim. Actors including Uttam Kumar and Soumitra Chatterjee led the Bengali film industry.
",Movie
"Other Bengali art film directors include Mir Shaani, Buddhadeb Dasgupta, Gautam Ghose, Sandip Ray and Aparna Sen.
",Movie
"Braj Bhasha language films present Brij culture mainly to rural people, predominant in the nebulous Braj region centred around Mathura, Agra, Aligarh and Hathras in Western Uttar Pradesh and Bharatpur and Dholpur in Rajasthan. It is the predominant language in the central stretch of the Ganges-Yamuna Doab in Uttar Pradesh. The first Brij Bhasha movie India was Brij Bhoomi (1982, Shiv Kumar), which was a success throughout the country.[211][212] Later Brij Bhasha cinema saw the production of films like Jamuna Kinare, Brij Kau Birju, Bhakta Surdas and Jesus.[213][214] The culture of Brij is presented in Krishna Tere Desh Main (Hindi), Kanha Ki Braj Bhumi,[215] Brij ki radha dwarika ke shyam[216] and Bawre Nain.[217]
",Movie
"Bhojpuri language films predominantly cater to residents of western Bihar and eastern Uttar Pradesh and also have a large audience in Delhi and Mumbai due to migration of Bhojpuri speakers to these cities. Besides India, markets for these films developed in other Bhojpuri speaking countries of the West Indies, Oceania and South America.[218]
",Movie
"Bhojpuri film history begins with Ganga Maiyya Tohe Piyari Chadhaibo (Mother Ganges, I will offer you a yellow sari, 1962, Kundan Kumar).[219] Throughout the following decades, few films were produced. Films such as Bidesiya (Foreigner, 1963, S. N. Tripathi) and Ganga (Ganges, 1965, Kumar) were profitable and popular, but in general Bhojpuri films were not common in the 1960s and 1970s.
",Movie
"The industry experienced a revival in 2001 with the hit Saiyyan Hamar (My Sweetheart, Mohan Prasad), which shot Ravi Kissan to superstardom.[220] This was followed by several other successes, including Panditji Batai Na Biyah Kab Hoi (Priest, tell me when I will marry, 2005, Prasad), and Sasura Bada Paisa Wala (My father-in-law, the rich guy, 2005.) Both did much better business in Uttar Pradesh and Bihar than mainstream Bollywood hits, and both earned more than ten times their production costs.[221] Although smaller than other Indian film industries, these successes increased Bhojpuri cinema's visibility, leading to an awards show[222] and a trade magazine, Bhojpuri City.[223]
",Movie
"Chhollywood was born in 1965 with the first Chhattisgarhi film Kahi Debe Sandesh (In Black and White, Manu Nayak).[224] Naidu[who?] wrote the lyrics for the film,[225] and two songs were sung by Mohammad Rafi. That film and Ghar Dwar (1971, Niranjan Tiwari) bombed. No Chhollywood movie was produced for nearly 30 years thereafter.[226]
",Movie
"Deepa Mehta, Anant Balani, Homi Adajania, Vijay Singh, Vierendrra Lalit and Sooni Taraporevala have garnered recognition in Indian English cinema.
",Movie
"Before the arrival of talkies, several silent films were closely related to Gujarati culture. Many film directors, producers and actors associated with silent films were Gujarati and Parsi. Twenty leading film company and studios were owned by Gujaratis between 1913 and 1931. They were mostly located in Mumbai. At least forty-four major Gujarati directors worked during this period.[227]
",Movie
"Gujarati cinema dates to 9 April 1932, when the first Gujarati film, Narsinh Mehta, was released.[227][228][229] Leeludi Dharti (1968) was the first colour Gujarati film.[230] After flourishing through the 1960s to 1980s, the industry declined although it later revived. More than one thousand films were released.[231]
",Movie
"Gujarati cinema ranges from mythology to history and from social to political. Gujarati films originally targeted a rural audience, but after its revival catered to an urban audience.[227]
",Movie
"The Hindi language film industry of Bombay—also known as[233] Bollywood—is the largest and most powerful branch.[234] Hindi cinema explored issues of caste and culture in films such as Achhut Kanya (1936) and Sujata (1959).[235] International visibility came to the industry with Raj Kapoor's Awara and later in Shakti Samantha's Aradhana.[236] Hindi cinema grew during the 1990s with the release of as many as 215 films annually.
",Movie
"Many actors signed contracts for simultaneous work in 3–4 films.[21] Institutions such as the Industrial Development Bank of India financed Hindi films.[21] Magazines such as Filmfare, Stardust and Cine Blitz became popular.[237]
",Movie
"In Hindi cinema audiences participate by clapping, singing and reciting familiar dialogue.[238]
",Movie
"Art film directors include Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal,[69] Mira Nair, Nagesh Kukunoor, Sudhir Mishra and Nandita Das.
",Movie
"The Kannada film industry, also referred to as Sandalwood, is based in Bengaluru and caters mostly to Karnataka. Gubbi Veeranna (1891 – 1972) was an Indian theatre director and artist and an awardee of the Padma Shri award conferred by the President of India. He was one of the pioneers and most prolific contributors to Kannada theatre. Kannada actor Rajkumar began working with Veeranna and later became an important actor.
",Movie
"Veeranna founded Karnataka Gubbi Productions. He produced Sadarame (1935, Raja Chandrasekar), in which he acted in the lead role. He then produced Subhadra and Jeevana Nataka (1942). He took the lead role in Hemareddy Mallamma (1945). Karnataka Gubbi Productions was later called Karnataka Films Ltd., and is credited with starting the career of Rajkumar when it offered him the lead role in his debut film Bedara Kannappa. He produced silent movies including His Love Affair, (Raphel Algoet). Veeranna was the lead, accompanied by his wife, Jayamma.
",Movie
"Veeranna produced Bedara Kannappa (1954, H. L. N. Simha) which received the first Certificate of Merit. However, the first ""President's Silver Medal for Best Feature Film in Kannada"" was awarded at the 5th National Film Awards ceremony to Premada Puthri (1957, R. Nagendra Rao).
",Movie
"Vishnuvardhan and Rajkumar were eminent actors along with Ambarish, Anant Nag, Shankar Nag, Prabhakar, Udaya Kumar, Kalyan Kumar, Gangadhar, Ravichandran, Girish Karnad, Prakash Raj, Charan Raj, B Jayamma, Shivaraj kumar, Leelavathi, Kalpana, Bharathi, Jayanthi, Pandari Bai, Aarathi, Jaimala, Tara, Umashri and Ramya.
",Movie
"Kannada directors include H. L. N. Simha, R. Nagendra Rao, B. R. Panthulu, M. S. Sathyu, Puttanna Kanagal, G. V. Iyer, Karnad, T. S. Nagabharana Siddalingaiah, B. V. Karanth, A K Pattabhi, T. V. Singh Thakur, Y. R. Swamy, M. R. Vittal, Sundar Rao Nadkarni, P. S. Moorthy, S. K. A. Chari, Hunsur Krishnamurthy, Prema Karanth, Rajendra Singh Babu, N. Lakshminarayan, Shankar Nag, Girish Kasaravalli, Umesh Kulkarni and Suresh Heblikar. Other noted film personalities in Kannada are, Bhargava, G.K. Venkatesh, Vijaya Bhaskar, Rajan-Nagendra, Geethapriya, Hamsalekha, R. N. Jayagopal, M. Ranga Rao and Yogaraj Bhat.
",Movie
"Kannada cinema contributed to Indian parallel cinema. Influential Kannada films in this genre include Samskara, Chomana Dudi (B. V. Karanth), Tabarana Kathe, Vamshavruksha, Kaadu Kudure, Hamsageethe, Bhootayyana Maga Ayyu, Accident, Maanasa Sarovara, Bara, Chitegoo Chinte, Galige, Ijjodu, Kaneshwara Rama,Ghatashraddha, Tabarana Kathe, Mane, Kraurya, Thaayi Saheba, Bandhana, Muthina Haara, Banker Margayya, Dweepa, Munnudi, Bettada Jeeva, Mysore Mallige and Chinnari Muththa.
",Movie
"The Government Film and Television Institute, Bangalore (formerly a part of S.J. Polytechnic) is believed to be the first government institute in India to start technical film courses.[239]
",Movie
"Konkani language films are mainly produced in Goa. It is one of India's smallest film regions, producing four films in 2009.[240] Konkani language is spoken mainly in the states of Goa, Maharashtra and Karnataka and to a smaller extent in Kerala. The first full length Konkani film was Mogacho Anvddo (1950, Jerry Braganza), under the banner of Etica Pictures.[241][242] The film's release date, 24 April, is celebrated as Konkani Film Day.[243] Karnataka is the hub of many Konkani speaking people. An immense body of Konkani literature and art is a resource for filmmakers. Kazar (Marriage, 2009, Richard Castelino) and Ujvaadu (Shedding New Light on Old Age Issues, Kasaragod Chinna) are major releases. The pioneering Mangalorean Konkani film is Mog Ani Maipas.
",Movie
"The Malayalam film industry, India's fourth largest, is based in Kochi. Malayalam films are known for bridging the gap between parallel cinema and mainstream cinema by portraying thought-provoking social issues with technical flair and low budgets. Filmmakers include Gopalakrishnan, Karun, Aravindan, K. G. George, Padmarajan, Sathyan Anthikad, Chandran and Bharathan.
",Movie
"The first full-length Malayalam feature wasVigathakumaran (1928, J. C. Daniel).[244] This movie is credited as the first Indian social drama feature film. Daniel is considered the father of the Malayalam film industry. Balan (1938, S. Nottani) was the first Malayalam ""talkie"".[245][246]
",Movie
"Malayalam films were mainly produced by Tamil producers until 1947, when the first major film studio, Udaya Studio, opened in Kerala.[247] Neelakkuyil (1954) captured national interest by winning the President's silver medal. Scripted by the well-known Malayalam novelist, Uroob (P. Bhaskaran and Ramu Kariat) is often considered the first authentic Malayali film.[248] Newspaper Boy (1955), made by a group of students, was the first neo-realistic film offering.[249] Chemmeen (1965, Ramu Kariat) based on a story by Thakazhi Sivasankara Pillai, became the first South Indian film to win the National Film Award for Best Feature Film.[250]
",Movie
"The first neorealistic film Newspaper Boy (1955-P. Ramdas),[141] The first CinemaScope film Thacholi Ambu (1978-Navodaya Appachan),[251] The first 70 mm film film Padayottam (1982-Jijo Punnoose),[252] The first 3D film My Dear Kuttichathan (1984-Jijo Punnoose),[253] The first Digital film Moonnamathoral (2006-V. K. Prakash),[254] The first Smartphone film Jalachhayam (2010-Sathish Kalathil),[255] The first 8K resolution film Villain (2017-B. Unnikrishnan)[256] of India were made in Malayalam.
",Movie
"The period from the late 1980s to early 1990s is regarded as the Golden Age of Malayalam cinema[257] with the emergence of actors Mohanlal, Mammootty, Suresh Gopi, Jayaram, Bharath Gopi, Murali, Thilakan and Nedumudi Venu. The major actors who emerged after the Golden Age include Dileep, Jayasurya, Fahadh Faasil, Nivin Pauly, Prithviraj Sukumaran, Dulquer Salmaan, Kunchacko Boban and Asif Ali (actor) and Manju Warrier.
",Movie
"Notable filmmakers such as I. V. Sasi, Bharathan, Padmarajan, K. G. George, Sathyan Anthikad, Priyadarshan, A. K. Lohithadas, Siddique-Lal, T. K. Rajeev Kumar and Sreenivasan. Art film directors include Puttanna Kanagal, Dore Bhagavan, Siddalingaiah in Kannada; Gopalakrishnan, Karun and T.V. Chandran.
",Movie
"K. R. Narayanan National Institute of Visual Science and Arts (KRNNIVSA) is an autonomous institute established by the Government of Kerala at Thekkumthala in Kottayam District in Kerala state as a training-cum-research centre in film/audio-visual technology.[258]
",Movie
"Meitei cinema is a small industry in the state of Manipur. This region's debut was a full-length black and white film Matamgee Manipur ( 1972). Meitei cinema started in the 1980s. Langlen Thadoi (1984) was Meitei cinema's first full-length colour film.
",Movie
"Meitei cinema gained momentum following a ban on the screening of Hindi films in entertainment houses in Manipur. Screening of Hindi movies came to a halt despite reiterated appeals made by successive Chief Ministers. 80-100 movies are made each year. Cinemas opened in Imphal after World War II. The first full-length Meitei movie was made in 1972, followed by a boom in 2002.
",Movie
"Imagi Ningthem (Aribam Syam Sharma) won the Grand Prix in the 1992 Nantes International Film Festival. A nationwide French telecast of Imagi Ningthem expanded the audience. After watching Ishanou (Aribam Syam Sharma), westerners began research on Lai Haraoba and Manipur's rich folklore. Maipak, Son of Manipur (1971) was the first Meitei documentary film.
",Movie
"Among the notable Meitei films are Phijigee Mani, Leipaklei and Pallepfam.
",Movie
"Marathi films are produced in the Marathi language in Maharashtra. It is one of the oldest efforts in Indian cinema. Dadasaheb Phalke made the first indigenous silent film Raja Harishchandra (1913) with a Marathi crew, which is considered by IFFI and NIFD to be part of Marathi cinema.
",Movie
"The first Marathi talkie, Ayodhyecha Raja (1932, Prabhat Films). Shwaas (2004) and Harishchandrachi Factory (2009), became India's official Oscar entries. Today the industry is based in Mumbai, but it began in Kolhapur and then Pune.
",Movie
"Some of the more notable films are Sangte Aika, Ek Gaon Bara Bhangadi, Pinjara, Sinhasan, Pathlaag, Jait Re Jait, Saamana, Santh Wahate Krishnamai, Sant Tukaram and Shyamchi Aai.
",Movie
"Marathi films feature the work of actors including Durga Khote, V. Shantaram, Lalita Pawar, Nanda, Shriram Lagoo, Ramesh Deo, Seema Deo, Nana Patekar, Smita Patil, Sadashiv Amrapurkar, Sonali Kulkarni, Sonali Bendre, Urmila Matondkar, Reema Lagoo, Padmini Kolhapure, Ashok Saraf, Laxmikant Berde and Sachin Khedekar.
",Movie
"Nagpuri films produced in the Nagpuri language in Jharkhand. The first Nagpuri feature film was  Sona Kar Nagpur (1994) which was produced and directed by Dhananjay Nath Tiwari.[259]
",Movie
"Gorkha cinema consists of Nepali language films produced by Nepali-speaking Indians.
",Movie
"The Odia language film industry operates in Bhubaneswar and Cuttack.[260] The first Odia talkie Sita Bibaha (1936) came from Mohan Sunder Deb Goswami. Shreeram Panda, Prashanta Nanda, Uttam Mohanty and Bijay Mohanty started the Oriya film industry by finding an audience and a fresh presentation.[261] The first colour film, Gapa Hele Be Sata (Although a Story, It Is True), was made by Nagen Ray and photographed by Pune Film Institute-trained cinematographer Surendra Sahu. The best year for Odia cinema was 1984 when Maya Miriga (Nirad Mohapatra) and Dhare Alua were showcased in Indian Panorama and Maya Miriga was invited to Critics Week at Cannes. The film received the Best Third World Film award at Mannheim Film Festival, Jury Award at Hawaii and was shown at the London Film Festival.
",Movie
"K.D. Mehra made the first Punjabi film, Sheela (also known as Pind di Kudi (Rustic Girl)). Baby Noor Jehan was introduced as an actress and singer in this film. Sheela was made in Calcutta and released in Lahore; it was a hit across the province. Its success led many more producers to make Punjabi films. As of 2009, Punjabi cinema had produced between 900 and 1,000 movies. The average number of releases per year in the 1970s was nine; in the 1980s, eight; and in the 1990s, six. In the 2000s Punjabi cinema revived with more releases every year featuring bigger budgets.[262] Manny Parmar made the first 3D Punjabi film, Pehchaan 3D (2013).
",Movie
"The Sindhi film industry produces movies at intervals. The first was Abana (1958 ), which was a success throughout the country. Sindhi cinema then produced some Bollywood-style films such as Hal Ta Bhaji Haloon, Parewari, Dil Dije Dil Waran Khe, Ho Jamalo, Pyar Kare Dis: Feel the Power of Love and The Awakening. Numerous Sindhi have contributed in Bollywood, including G P Sippy, Ramesh Sippy, Nikhil Advani, Tarun Mansukhani, Ritesh Sidhwani and Asrani.
",Movie
"Director Songe Dorjee Thongdok introduced the first Sherdukpen-language film Crossing Bridges (2014). Sherdukpen is native to the north-eastern state of Arunachal Pradesh.[263]
",Movie
"Chennai once served as a base for all South Indian films It is the second largest industry in India after Bollywood and is South India's largest production centre.[264]
",Movie
"The first south Indian talkie film Kalidas (H. M. Reddy) was shot in Tamil and Telugu. Sivaji Ganesan became India's first actor to receive an international award when he won Best Actor at the Afro-Asian film festival in 1960 and the title of Chevalier in the Legion of Honour by the French Government in 1995.[100]
",Movie
"Tamil cinema is influenced by Dravidian politics,[101] led by film personalities such as C N Annadurai, M G Ramachandran, M Karunanidhi and Jayalalithaa who became Chief Ministers of Tamil Nadu.[102] K. B. Sundarambal was the first film personality to enter a state legislature in India.[265] She was also the first to command a salary of one lakh rupees.
",Movie
"Tamil films are distributed to various parts of Asia, Southern Africa, Northern America, Europe and Oceania.[266] The industry inspired Tamil film-making in Sri Lanka, Malaysia, Singapore and Canada.
",Movie
"Rajnikanth is referred to as ""Superstar"" and holds matinee idol status in South India.[267] Kamal Haasan debuted in 1960 Kalathur Kannamma, for which he won the President's Gold Medal for Best Child Actor. Kamal Haasan is tied with Mammootty and Bachchan for the most Best Actor National Film Awards, with three. With seven submissions, Kamal Haasan has starred in the highest number of Academy Award submissions.
Critically acclaimed composers such as Ilaiyaraaja and A. R. Rahman work in Tamil cinema. Art film directors include Santosh Sivan.
",Movie
"India's greatest number of theatres are located in Andhra Pradesh / Telangana  and feature films in Telugu. As of 2018, it is the third largest film industry in India after Bollywood and Kollywood in terms of box office collections and footfalls, and in terms of number of theatrical releases. 
.[268][269][270] Ramoji Film City, which holds the Guinness World Record for the world's largest film production facility, is located in Hyderabad.[271] The Prasad IMAX in Hyderabad is the world's largest 3D IMAX screen[147][148] and is the world's most viewed screen.[149] The highest-grossing Telugu movie is Baahubali 2: The Conclusion. Raghupathi Venkaiah Naidu is considered the ""father of Telugu cinema"". The annual Raghupati Venkaiah Award was incorporated into the Nandi Awards to recognize contributions to the industry.[272]
",Movie
"Chittor V. Nagaiah was the first multilingual Indian film actor, thespian, composer, director, producer, writer and playback singer. Nagaiah made significant contributions to Telugu cinema, and starred in some two hundred productions.[273] Regarded as one of the finest Indian method actors, he was Telugu's first matinee idol. His forte was intense characters, often immersing himself in the character's traits and mannerisms.[273] He was the first from South India to be honoured with the Padma Shri.[274] He became known as India's Paul Muni.[43][275] S. V. Ranga Rao was one of the first Indian actors to receive the international award at the Indonesian Film Festival, held in Jakarta, for Narthanasala in 1963.[276] N. T. Rama Rao was one of the most successful Telugu actors of his time.[277]
",Movie
"B. Narsing Rao, K. N. T. Sastry and Pattabhirama Reddy garnered international recognition for their pioneering work in Parallel Cinema.[278][279] Adurthi Subba Rao won ten National Film Awards, Telugu cinema's highest individual awards, for his directorial work.[280] N .T. Rama Rao was an Indian actor, producer, director, editor and politician who earned three National Film Awards. He served as Chief Minister of Andhra Pradesh for seven years over three terms.
",Movie
"Bhanumathi Ramakrishna was a multilingual Indian film actress, drector, music director, singer, producer, author and songwriter.[281][282] Widely known as the first female super star of Telugu cinema, she is also known for her work in Tamil cinema. Ghantasala Venkateswara Rao was an Indian film, composer, playback singer known for his works predominantly in South Indian cinema. S. P. Balasubramanyam holds the Guinness World Record of having sung the most number of songs for any male playback singer; the majority were in Telugu.[283][284][285]
",Movie
"S. V. Ranga Rao, N. T. Rama Rao, Kanta Rao, Bhanumathi Ramakrishna, Savitri, Gummadi and Sobhan Babu received the Rashtrapati Award for best performance in a leading role.[286][287] Sharada, Archana, Vijayashanti, Rohini, and P. L. Narayana received the National Film Award for the best performance in acting. Chiranjeevi was listed among ""the men who changed the face of the Indian Cinema"" by IBN-live India.[288][289]
",Movie
"30 to 40 films are made annually in Tulu. K. N. Tailor and Machchendra nath Pandeshwar are Tulu icons.  Usually Tulu films are released in theatres across the Kanara region of Karnataka.[290]
",Movie
"Enna Thangadi, was the first, released in 1971. The critically acclaimed Suddha won the award for Best Indian Film at the Osian film festival held at New Delhi in 2006.[291][292][293] Oriyardori Asal, released in 2011, is the most successful.[294] Koti Chennaya (1973, Vishu Kumar) was the first history-based. The first colour film was Kariyani Kattandi Kandani (1978, Aroor Bhimarao).
",Movie
"Dadasaheb Phalke is known as the ""Father of Indian cinema"".[30][31][32][33] The Dadasaheb Phalke Award, for lifetime contribution to cinema, was instituted in his honour by the Government of India in 1969, and is the country's most prestigious and coveted film award.[295]
",Movie
"Government-run and private institutes provide formal education in various aspects of filmmaking. Some of the prominent ones include:
",Movie
"Musical film is a film genre in which songs sung by the characters are interwoven into the narrative, sometimes accompanied by dancing.
",Movie
"The songs usually advance the plot or develop the film's characters, but in some cases, they serve merely as breaks in the storyline, often as elaborate ""production numbers.""
",Movie
"The musical film was a natural development of the stage musical after the emergence of sound film technology. Typically, the biggest difference between film and stage musicals is the use of lavish background scenery and locations that would be impractical in a theater. Musical films characteristically contain elements reminiscent of theater; performers often treat their song and dance numbers as if a live audience were watching. In a sense, the viewer becomes the diegetic audience, as the performer looks directly into the camera and performs to it.
",Movie
"The 1930s through the early 1950s are considered to be the golden age of the musical film, when the genre's popularity was at its highest in the Western world. Disney's Snow White and the Seven Dwarfs, the earliest Disney animated feature film, was a musical which won an honorary Oscar for Walt Disney at the 11th Academy Awards.
",Movie
"Musical short films were made by Lee de Forest in 1923–24. Beginning in 1926, thousands of Vitaphone shorts were made, many featuring bands, vocalists, and dancers. The earliest feature-length films with synchronized sound had only a soundtrack of music and occasional sound effects that played while the actors portrayed their characters just as they did in silent films: without audible dialogue.[1] The Jazz Singer, released in 1927 by Warner Brothers, was the first to include an audio track including non-diegetic music and diegetic music, but it had only a short sequence of spoken dialogue. This feature-length film was also a musical, featuring Al Jolson singing ""Dirty Hands, Dirty Face"", ""Toot, Toot, Tootsie"", ""Blue Skies"", and ""My Mammy"". Historian Scott Eyman wrote, ""As the film ended and applause grew with the houselights, Sam Goldwyn's wife Frances looked around at the celebrities in the crowd. She saw 'terror in all their faces', she said, as if they knew that 'the game they had been playing for years was finally over'.""[2] Still, only isolated sequences featured ""live"" sound; most of the film had only a synchronous musical score.[1] In 1928, Warner Brothers followed this up with another Jolson part-talkie, The Singing Fool, which was a blockbuster hit.[1] Theaters scrambled to install the new sound equipment and to hire Broadway composers to write musicals for the screen.[3] The first all-talking feature, Lights of New York, included a musical sequence in a night club. The enthusiasm of audiences was so great that in less than a year all the major studios were making sound pictures exclusively. The Broadway Melody (1929) had a show-biz plot about two sisters competing for a charming song-and-dance man. Advertised by MGM as the first ""All-Talking, All-Singing, All-Dancing"" feature film, it was a hit and won the Academy Award for Best Picture for 1929. There was a rush by the studios to hire talent from the stage to star in lavishly filmed versions of Broadway hits. The Love Parade (Paramount 1929) starred Maurice Chevalier and newcomer Jeanette MacDonald, written by Broadway veteran Guy Bolton.[3]
",Movie
"Warner Brothers produced the first screen operetta, The Desert Song in 1929. They spared no expense and photographed a large percentage of the film in Technicolor. This was followed by the first all-color, all-talking musical feature which was entitled On with the Show (1929). The most popular film of 1929 was the second all-color, all-talking feature which was entitled Gold Diggers of Broadway (1929). This film broke all box office records and remained the highest-grossing film ever produced until 1939. Suddenly, the market became flooded with musicals, revues, and operettas. The following all-color musicals were produced in 1929 and 1930 alone: The Show of Shows (1929), Sally (1929), The Vagabond King (1930), Follow Thru (1930), Bright Lights (1930), Golden Dawn (1930), Hold Everything (1930), The Rogue Song (1930), Song of the Flame (1930), Song of the West (1930), Sweet Kitty Bellairs (1930), Under a Texas Moon (1930), Bride of the Regiment (1930), Whoopee! (1930), King of Jazz (1930), Viennese Nights (1930), and Kiss Me Again (1930). In addition, there were scores of musical features released with color sequences.
",Movie
"Hollywood released more than 100 musical films in 1930, but only 14 in 1931.[4] By late 1930, audiences had been oversaturated with musicals and studios were forced to cut the music from films that were then being released. For example, Life of the Party (1930) was originally produced as an all-color, all-talking musical comedy. Before it was released, however, the songs were cut out. The same thing happened to Fifty Million Frenchmen (1931) and Manhattan Parade (1932) both of which had been filmed entirely in Technicolor. Marlene Dietrich sang songs successfully in her films, and Rodgers and Hart wrote a few well-received films, but even their popularity waned by 1932.[4] The public had quickly come to associate color with musicals and thus the decline in their popularity also resulted in a decline in color productions.
",Movie
"The taste in musicals revived again in 1933 when director Busby Berkeley began to enhance the traditional dance number with ideas drawn from the drill precision he had experienced as a soldier during World War I. In films such as 42nd Street and Gold Diggers of 1933 (1933), Berkeley choreographed a number of films in his unique style. Berkeley's numbers typically begin on a stage but gradually transcend the limitations of theatrical space: his ingenious routines, involving human bodies forming patterns like a kaleidoscope, could never fit onto a real stage and the intended perspective is viewing from straight above.[5]
",Movie
"Musical stars such as Fred Astaire and Ginger Rogers were among the most popular and highly respected personalities in Hollywood during the classical era; the Fred and Ginger pairing was particularly successful, resulting in a number of classic films, such as Top Hat (1935), Swing Time (1936), and Shall We Dance (1937). Many dramatic actors gladly participated in musicals as a way to break away from their typecasting. For instance, the multi-talented James Cagney had originally risen to fame as a stage singer and dancer, but his repeated casting in ""tough guy"" roles and mob films gave him few chances to display these talents. Cagney's Oscar-winning role in Yankee Doodle Dandy (1942) allowed him to sing and dance, and he considered it to be one of his finest moments.
",Movie
"Many comedies (and a few dramas) included their own musical numbers. The Marx Brothers' films included a musical number in nearly every film, allowing the Brothers to highlight their musical talents. Their final film, entitled Love Happy (1949), featured Vera-Ellen, considered to be the best dancer among her colleagues and professionals in the half century.
",Movie
"Similarly, The vaudevillian comedian W. C. Fields joined forces with the comic actress Martha Raye and the young comedian Bob Hope in Paramount Pictures musical anthology The Big Broadcast of 1938. The film also showcased the talents of several internationally recognized musical artists including: Kirsten Flagstad (Norwegian operatic soprano), Wilfred Pelletier (Canadian conductor of the Metropolitan Opera Orchestra, Tito Guizar (Mexican tenor), Shep Fields conducting his Ripppling Rhythm Jazz Orchestra and John Serry Sr. (Italian-American concert accordionist).[6] In addition to the Academy Award for Best Original Song (1938), the film earned an ASCAP Film and Television Award (1989) for Bob Hope's signature song Thanks for the Memory.[7]
",Movie
"During the late 1940s and into the early 1950s, a production unit at Metro-Goldwyn-Mayer headed by Arthur Freed made the transition from old-fashioned musical films, whose formula had become repetitive, to something new. (However, they also produced Technicolor remakes of such musicals as Show Boat, which had previously been filmed in the 1930s.) In 1939, Freed was hired as associate producer for the film Babes in Arms. Starting in 1944 with Meet Me in St. Louis, the Freed Unit worked somewhat independently of its own studio to produce some of the most popular and well-known examples of the genre. The products of this unit include Easter Parade (1948), On the Town (1949), An American in Paris (1951), Singin' in the Rain (1952), The Band Wagon (1953) and Gigi (1958). Non-Freed musicals from the studio include Seven Brides for Seven Brothers in 1954 and High Society in 1956.
",Movie
"This era saw musical stars become household names, including Judy Garland, Gene Kelly, Ann Miller, Donald O'Connor, Cyd Charisse, Mickey Rooney, Vera-Ellen, Jane Powell, Howard Keel, and Kathryn Grayson. Fred Astaire was also coaxed out of retirement for Easter Parade and made a permanent comeback.
",Movie
"The other Hollywood studios proved themselves equally adept at tackling the genre at this time, particularly in the 1950s. Four adaptations of Rodgers and Hammerstein shows - Oklahoma!, The King and I, Carousel, and South Pacific - were all successes, while Paramount Pictures released White Christmas and Funny Face, two films which used previously written music by Irving Berlin and the Gershwins, respectively. Warner Bros. produced Calamity Jane and A Star Is Born; the former film was a vehicle for Doris Day, while the latter provided a big-screen comeback for Judy Garland, who had been out of the spotlight since 1950. Meanwhile, director Otto Preminger, better known for controversial ""message pictures"", made Carmen Jones and Porgy and Bess, both starring Dorothy Dandridge, who is considered the first African American A-list film star. Celebrated director Howard Hawks also ventured into the genre with Gentlemen Prefer Blondes.
",Movie
"In the 1960s, 1970s, and continuing up to today, the musical film became less of a bankable genre that could be relied upon for sure-fire hits. Audiences for them lessened and fewer musical films were produced as the genre became less mainstream and more specialized.
",Movie
"In the 1960s, the critical and box-office success of the films West Side Story, Gypsy, The Music Man, Bye Bye Birdie, My Fair Lady, Mary Poppins, The Sound of Music, A Funny Thing Happened on the Way to the Forum, The Jungle Book, Thoroughly Modern Millie, Oliver!, and Funny Girl suggested that the traditional musical was in good health, while French filmmaker Jacques Demy's jazz musicals The Umbrellas of Cherbourg and The Young Girls of Rochefort were popular with international critics. However popular musical tastes were being heavily affected by rock and roll and the freedom and youth associated with it, and indeed Elvis Presley made a few films that have been equated with the old musicals in terms of form, though A Hard Day's Night and Help!, starring the Beatles, were more technically audacious. Most of the musical films of the 1950s and 1960s such as Oklahoma! and The Sound of Music were straightforward adaptations or restagings of successful stage productions. The most successful musicals of the 1960s created specifically for film were Mary Poppins and The Jungle Book, two of Disney's biggest hits of all time.
",Movie
"The phenomenal box-office performance of The Sound of Music gave the major Hollywood studios more confidence to produce lengthy, large-budget musicals. Despite the resounding success of some of these films, Hollywood also produced a large number of musical flops in the late 1960s and early 1970s which appeared to seriously misjudge public taste. The commercially and/or critically unsuccessful films included Camelot, Finian's Rainbow, Hello Dolly!, Sweet Charity, Doctor Dolittle, Star!, Darling Lili, Goodbye, Mr. Chips, Paint Your Wagon, Song of Norway, On a Clear Day You Can See Forever, Man of La Mancha, Lost Horizon, and Mame. Collectively and individually these failures crippled several of the major studios.
",Movie
"In the 1970s, film culture and the changing demographics of filmgoers placed greater emphasis on gritty realism, while the pure entertainment and theatricality of classical-era Hollywood musicals was seen as old-fashioned. Despite this, Fiddler on the Roof and Cabaret were more traditional musicals closely adapted from stage shows and were strong successes with critics and audiences. Changing cultural mores and the abandonment of the Hays Code in 1968 also contributed to changing tastes in film audiences. The 1973 film of Andrew Lloyd Webber and Tim Rice's Jesus Christ Superstar was met with some criticism by religious groups but was well received. By the mid-1970s, filmmakers avoided the genre in favor of using music by popular rock or pop bands as background music, partly in hope of selling a soundtrack album to fans. The Rocky Horror Picture Show was originally released in 1975 and was a critical failure until it started midnight screenings in the 1980s where it achieved cult status. 1976 saw the release of the low-budget comic musical, The First Nudie Musical, released by Paramount. The 1978 film version of Grease was a smash hit; its songs were original compositions done in a 1950s pop style. However, the sequel Grease 2 (released in 1982) bombed at the box-office. Films about performers which incorporated gritty drama and musical numbers interwoven as a diegetic part of the storyline were produced, such as Lady Sings the Blues, All That Jazz, and New York, New York. Some musicals made in Britain experimented with the form, such as Richard Attenborough's Oh! What a Lovely War (released in 1969), Alan Parker's Bugsy Malone and Ken Russell's Tommy and Lisztomania.
",Movie
"A number of film musicals were still being made that were financially and/or critically less successful than in the musical's heyday. They include 1776, The Wiz, At Long Last Love, Mame, Man of La Mancha, Lost Horizon, Godspell, Phantom of the Paradise, Funny Lady (Barbra Streisand's sequel to Funny Girl), A Little Night Music, and Hair amongst others. The critical wrath against At Long Last Love, in particular, was so strong that it was never released on home video. Fantasy musical films Scrooge, The Blue Bird, The Little Prince, Willy Wonka & the Chocolate Factory, Pete's Dragon, and Disney's Bedknobs and Broomsticks were also released in the 1970s, the latter winning the Academy Award for Best Visual Effects.
",Movie
"By the 1980s, financiers grew increasingly confident in the musical genre, partly buoyed by the relative health of the musical on Broadway and London's West End. Productions of the 1980s and 1990s included The Apple, Xanadu, The Blues Brothers, Annie, Monty Python's The Meaning of Life, The Best Little Whorehouse in Texas, Victor Victoria, Footloose, Fast Forward, A Chorus Line, Little Shop of Horrors, Forbidden Zone, Absolute Beginners, Labyrinth, Evita, and Everyone Says I Love You. However, Can't Stop the Music, starring the Village People, was a calamitous attempt to resurrect the old-style musical and was released to audience indifference in 1980. Little Shop of Horrors was based on an off-Broadway musical adaptation of a 1960 Roger Corman film, a precursor of later film-to-stage-to-film adaptations, including The Producers.
",Movie
"Many animated films of the period – predominately from Disney – included traditional musical numbers. Howard Ashman, Alan Menken, and Stephen Schwartz had previous musical theatre experience and wrote songs for animated films during this time, supplanting Disney workhorses the Sherman Brothers. Starting with 1989's The Little Mermaid, the Disney Renaissance gave new life to the musical film. Other successful animated musicals included Aladdin, The Hunchback of Notre Dame, and Pocahontas from Disney proper, The Nightmare Before Christmas from Disney division Touchstone Pictures, The Prince of Egypt from DreamWorks, Anastasia from Fox and Don Bluth, and South Park: Bigger, Longer & Uncut from Paramount. (Beauty and the Beast and The Lion King were adapted for the stage after their blockbuster success.)
",Movie
"In the 21st century, movie musicals were reborn with darker musicals, epic drama musicals and comedy-drama musicals such as Moulin Rouge!, Chicago, Dreamgirls, Sweeney Todd: The Demon Barber of Fleet Street, Les Misérables, and La La Land all of which won the Golden Globe Award for Best Motion Picture – Musical or Comedy in their respective years, while such films as The Phantom of the Opera, Hairspray, Mamma Mia!, Nine, Into the Woods, and The Greatest Showman were only nominated. Chicago was also the first musical since Oliver! to win Best Picture at the Academy Awards.
",Movie
"Joshua Oppenheimer's Academy Award-nominated documentary The Act of Killing may be considered a nonfiction musical.[8]
",Movie
"One specific musical trend was the rising number of jukebox musicals based on music from various pop/rock artists on the big screen, some of which based on Broadway shows. Examples of Broadway-based jukebox musical films included Mamma Mia! (ABBA), Rock of Ages, and Sunshine on Leith (The Proclaimers). Original ones included Across the Universe (The Beatles), Moulin Rouge! (various pop hits), and Idlewild (Outkast).
",Movie
"Disney also returned to musicals with Enchanted, The Princess and the Frog, Tangled, Winnie the Pooh, The Muppets, Frozen, Muppets Most Wanted, Into the Woods, Moana, and Mary Poppins Returns. Following a string of successes with live action fantasy adaptations of several of their animated features, Disney produced a live action version of Beauty and the Beast, the first of this live action fantasy adaptation pack to be an all-out musical, and features new songs as well as new lyrics to both the Gaston number and the reprise of the title song. Pixar also produced Coco, the very first computer-animated musical film by the company.
",Movie
"Other animated musical films include Rio, Trolls, and Sing.[citation needed]
",Movie
"Biopics about music artists and showmen were also big in the 21st century. Examples include 8 Mile (Eminem), Ray (Ray Charles), Walk the Line (Johnny Cash and June Carter), La Vie en Rose (Édith Piaf), Notorious (Biggie Smalls), Jersey Boys (The Four Seasons) Love & Mercy (Brian Wilson), CrazySexyCool: The TLC Story (TLC), Aaliyah: The Princess of R&B (Aaliyah), Get on Up (James Brown), Whitney (Whitney Houston), Straight Outta Compton (N.W.A), The Greatest Showman (P. T. Barnum), Bohemian Rhapsody (Freddie Mercury).[citation needed]
",Movie
"Director Damien Chazelle created a musical film called La La Land, starring Ryan Gosling and Emma Stone. It was meant to reintroduce the traditional jazz style of song numbers with influences from the Golden Age of Hollywood and Jacques Demy's French musicals while incorporating a contemporary/modern take on the story and characters with balances in fantasy numbers and grounded reality. It received 14 nominations at the 89th Academy Awards, tying the record for most nominations with All About Eve (1950) and Titanic (1997), and won the awards for Best Director, Best Actress, Best Cinematography, Best Original Score, Best Original Song, and Best Production Design.
",Movie
"An exception to the decline of the musical film is Indian cinema, especially the Bollywood film industry based in Mumbai (formerly Bombay), where the majority of films have been and still are musicals. The majority of films produced in the Tamil industry based in Chennai (formerly Madras), Sandalwood based in Bangalore, Telugu industry based in Hyderabad, and Malayalam industry are also musicals.
",Movie
"Bollywood musicals have their roots in the traditional musical theatre of India, such as classical Indian musical theatre, Sanskrit drama, and Parsi theatre. Early Bombay filmmakers combined these Indian musical theatre traditions with the musical film format that emerged from early Hollywood sound films.[9] Other early influences on Bombay filmmakers included Urdu literature and the Arabian Nights.[10]
",Movie
"The first Indian sound film, Ardeshir Irani's Alam Ara (1931), was a major commercial success.[11] There was clearly a huge market for talkies and musicals; Bollywood and all the regional film industries quickly switched to sound filming.
",Movie
"In 1937, Ardeshir Irani, of Alam Ara fame, made the first colour film in Hindi, Kisan Kanya. The next year, he made another colour film, a version of Mother India. However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were the staple fare at the cinema.
",Movie
"Following India's independence, the period from the late 1940s to the early 1960s is regarded by film historians as the ""Golden Age"" of Hindi cinema.[15][16][17] Some of the most critically acclaimed Hindi films of all time were produced during this period. Examples include Pyaasa (1957) and Kaagaz Ke Phool (1959) directed by Guru Dutt and written by Abrar Alvi, Awaara (1951) and Shree 420 (1955) directed by Raj Kapoor and written by Khwaja Ahmad Abbas, and Aan (1952) directed by Mehboob Khan and starring Dilip Kumar. These films expressed social themes mainly dealing with working-class life in India, particularly urban life in the former two examples; Awaara presented the city as both a nightmare and a dream, while Pyaasa critiqued the unreality of city life.[18]
",Movie
"Mehboob Khan's Mother India (1957), a remake of his earlier Aurat (1940), was the first Indian film to be nominated for the Academy Award for Best Foreign Language Film, which it lost by a single vote.[19] Mother India was also an important film that defined the conventions of Hindi cinema for decades.[20][21][22]
",Movie
"In the 1960s and early 1970s, the industry was dominated by musical romance films with ""romantic hero"" leads, the most popular being Rajesh Khanna.[23] Other actors during this period include Shammi Kapoor, Jeetendra, Sanjeev Kumar, and Shashi Kapoor, and actresses like Sharmila Tagore, Mumtaz, Saira Banu, Helen and Asha Parekh.
",Movie
"By the start of the 1970s, Hindi cinema was experiencing thematic stagnation,[24] dominated by musical romance films.[23] The arrival of screenwriter duo Salim-Javed, consisting of Salim Khan and Javed Akhtar, marked a paradigm shift, revitalizing the industry.[24] They began the genre of gritty, violent, Bombay underworld crime films in the early 1970s, with films such as Zanjeer (1973) and Deewaar (1975).[25][26]
",Movie
"The 1970s was also when the name ""Bollywood"" was coined,[27][28] and when the quintessential conventions of commercial Bollywood films were established.[29] Key to this was the emergence of the masala film genre, which combines elements of multiple genres (action, comedy, romance, drama, melodrama, musical). The masala film was pioneered in the early 1970s by filmmaker Nasir Hussain,[30] along with screenwriter duo Salim-Javed,[29] pioneering the Bollywood blockbuster format.[29] Yaadon Ki Baarat (1973), directed by Hussain and written by Salim-Javed, has been identified as the first masala film and the ""first"" quintessentially ""Bollywood"" film.[31][29] Salim-Javed went on to write more successful masala films in the 1970s and 1980s.[29] Masala films launched Amitabh Bachchan into the biggest Bollywood movie star of the 1970s and 1980s. A landmark for the masala film genre was Amar Akbar Anthony (1977),[32][31] directed by Manmohan Desai and written by Kader Khan. Manmohan Desai went on to successfully exploit the genre in the 1970s and 1980s.
",Movie
"Along with Bachchan, other popular actors of this era included Feroz Khan,[33] Mithun Chakraborty, Naseeruddin Shah, Jackie Shroff, Sanjay Dutt, Anil Kapoor and Sunny Deol. Actresses from this era included Hema Malini, Jaya Bachchan, Raakhee, Shabana Azmi, Zeenat Aman, Parveen Babi, Rekha, Dimple Kapadia, Smita Patil, Jaya Prada and Padmini Kolhapure.[34]
",Movie
"In the late 1980s, Hindi cinema experienced another period of stagnation, with a decline in box office turnout, due to increasing violence, decline in musical melodic quality, and rise in video piracy, leading to middle-class family audiences abandoning theaters. The turning point came with Qayamat Se Qayamat Tak (1988), directed by Mansoor Khan, written and produced by his father Nasir Hussain, and starring his cousin Aamir Khan with Juhi Chawla. Its blend of youthfulness, wholesome entertainment, emotional quotients and strong melodies lured family audiences back to the big screen.[35][36] It set a new template for Bollywood musical romance films that defined Hindi cinema in the 1990s.[36]
",Movie
"The period of Hindi cinema from the 1990s onwards is referred to as ""New Bollywood"" cinema,[37] linked to economic liberalisation in India during the early 1990s.[38] By the early 1990s, the pendulum had swung back toward family-centric romantic musicals. Qayamat Se Qayamat Tak was followed by blockbusters such as Maine Pyar Kiya (1989), Chandni (1989), Hum Aapke Hain Kaun (1994), Dilwale Dulhania Le Jayenge (1995), Raja Hindustani (1996), Dil To Pagal Hai (1997), Pyaar To Hona Hi Tha (1998) and Kuch Kuch Hota Hai (1998). A new generation of popular actors emerged, such as Aamir Khan, Aditya Pancholi, Ajay Devgan, Akshay Kumar, Salman Khan (Salim Khan's son), and Shahrukh Khan, and actresses such as Madhuri Dixit, Sridevi, Juhi Chawla, Meenakshi Seshadri, Manisha Koirala, Kajol, and Karisma Kapoor.[34]
",Movie
"Since the 1990s, the three biggest Bollywood movie stars have been the ""Three Khans"": Aamir Khan, Shah Rukh Khan, and Salman Khan.[39][40] Combined, they have starred in most of the top ten highest-grossing Bollywood films. The three Khans have had successful careers since the late 1980s,[39] and have dominated the Indian box office since the 1990s,[41] across three decades.[42]
",Movie
"In the 2000s, Bollywood musicals played an instrumental role in the revival of the musical film genre in the Western world.[43] Baz Luhrmann stated that his successful musical film Moulin Rouge! (2001) was directly inspired by Bollywood musicals.[44] The film thus pays homage to India, incorporating an Indian-themed play based on the ancient Sanskrit drama The Little Clay Cart and a Bollywood-style dance sequence with a song from the film China Gate. The critical and financial success of Moulin Rouge! renewed interest in the then-moribund Western musical genre, and subsequently films such as Chicago, The Producers, Rent, Dreamgirls, Hairspray, Sweeney Todd, Across the Universe, The Phantom of the Opera, Enchanted and Mamma Mia! were produced, fuelling a renaissance of the genre.[45][43]
",Movie
"The Guru and The 40-Year-Old Virgin also feature Indian-style song-and-dance sequences; the Bollywood musical Lagaan (2001) was nominated for the Academy Award for Best Foreign Language Film; two other Bollywood films Devdas (2002) and Rang De Basanti (2006) were nominated for the BAFTA Award for Best Film Not in the English Language; and Danny Boyle's Academy Award winning Slumdog Millionaire (2008) also features a Bollywood-style song-and-dance number during the film's end credits.
",Movie
"Spain has a history and tradition of musical films that were made independent of Hollywood influence. The first films arise during the Second Spanish Republic of the 1930s and the advent of sound films. A few zarzuelas (Spanish operetta) were even adapted as screenplays during the silent era. The beginnings of the Spanish musical were focused on romantic Spanish archetypes: Andalusian villages and landscapes, gypsys, ""bandoleros"", and copla and other popular folk songs included in story development. These films had even more box-office success than Hollywood premieres in Spain. The first Spanish film stars came from the musical genre: Imperio Argentina, Estrellita Castro, Florián Rey (director) and, later, Lola Flores, Sara Montiel and Carmen Sevilla. The Spanish musical started to expand and grow. Juvenile stars appear and top the box-office. Marisol, Joselito, Pili & Mili, and Rocío Dúrcal were the major figures of musical films from 1960s to 1970s. Due to Spanish transition to democracy and rise of ""Movida culture"", the musical genre felt into a decadence of production and box-office, only saved by Carlos Saura and his flamenco musical films.
",Movie
"Unlike the musical films of Hollywood and Bollywood, popularly identified with escapism, the Soviet musical was first and foremost a form of propaganda. Vladimir Lenin said that cinema was ""the most important of the arts."" His successor, Joseph Stalin, also recognized the power of cinema in efficiently spreading Communist Party doctrine. Films were widely popular in the 1920s, but it was foreign cinema that dominated the Soviet filmgoing market. Films from Germany and the U.S. proved more entertaining than Soviet director Sergei Eisenstein's historical dramas.[46] By the 1930s it was clear that if the Soviet cinema was to compete with its Western counterparts, it would have to give audiences what they wanted: the glamour and fantasy they got from Hollywood.[47] The musical film, which emerged at that time, embodied the ideal combination of entertainment and official ideology.
",Movie
"A struggle between laughter for laughter's sake and entertainment with a clear ideological message would define the golden age of the Soviet musical of the 1930s and 1940s. Then-head of the film industry Boris Shumyatsky sought to emulate Hollywood's conveyor belt method of production, going so far as to suggest the establishment of a Soviet Hollywood.[48]
",Movie
"In 1930, the esteemed Soviet film director Sergei Eisenstein went to the United States with fellow director Grigori Aleksandrov to study Hollywood's filmmaking process. The American films greatly impacted Aleksandrov, particularly the musicals.[49] He returned in 1932, and in 1934 directed The Jolly Fellows, the first Soviet musical. The film was light on plot and focused more on the comedy and musical numbers. Party officials at first met the film with great hostility. Aleksandrov defended his work by arguing the notion of laughter for laughter's sake.[50] Finally, when Aleksandrov showed the film to Stalin, the leader decided that musicals were an effective means of spreading propaganda. Messages like the importance of collective labor and rags-to-riches stories would become the plots of most Soviet musicals.
",Movie
"The success of The Jolly Fellows ensured a place in Soviet cinema for the musical format, but immediately Shumyatsky set strict guidelines to make sure the films promoted Communist values. Shumyatsky's decree ""Movies for the Millions"" demanded conventional plots, characters, and montage to successfully portray Socialist Realism (the glorification of industry and the working class) on film.[51]
",Movie
"The first successful blend of a social message and entertainment was Aleksandrov's Circus (1936). It starred his wife, Lyubov Orlova (an operatic singer who had also appeared in The Jolly Fellows) as an American circus performer who has to immigrate to the USSR from the U.S. because she has a mixed-race child, whom she had with a black man. Amidst the backdrop of lavish musical productions, she finally finds love and acceptance in the USSR, providing the message that racial tolerance can only be found in the Soviet Union.
",Movie
"The influence of Busby Berkeley's choreography on Aleksandrov's directing can be seen in the musical number leading up to the climax. Another, more obvious reference to Hollywood is the Charlie Chaplin impersonator who provides comic relief throughout the film. Four million people in Moscow and Leningrad went to see Circus during its first month in theaters.[52]
",Movie
"Another of Aleksandrov's more-popular films was The Bright Path (1940). This was a reworking of the fairytale Cinderella set in the contemporary Soviet Union. The Cinderella of the story was again Orlova, who by this time was the most popular star in the USSR.[53] It was a fantasy tale, but the moral of the story was that a better life comes from hard work. Whereas in Circus, the musical numbers involved dancing and spectacle, the only type of choreography in Bright Path is the movement of factory machines. The music was limited to Orlova's singing. Here, work provided the spectacle.
",Movie
"The other director of musical films was Ivan Pyryev. Unlike Aleksandrov, the focus of Pyryev's films was life on the collective farms. His films, Tractor Drivers (1939), The Swineherd and the Shepherd (1941), and his most famous, Cossacks of the Kuban (1949) all starred his wife, Marina Ladynina. Like in Aleksandrov's Bright Path, the only choreography was the work the characters were doing on film. Even the songs were about the joys of working.
",Movie
"Rather than having a specific message for any of his films, Pyryev promoted Stalin's slogan ""life has become better, life has become more joyous.""[54] Sometimes this message was in stark contrast with the reality of the time. During the filming of Cossacks of the Kuban, the Soviet Union was going through a postwar famine. In reality, the actors who were singing about a time of prosperity were hungry and malnourished.[55] The films did, however, provide escapism and optimism for the viewing public.
",Movie
"The most popular film of the brief era of Stalinist musicals was Alexandrov's 1938 film Volga-Volga. The star, again, was Lyubov Orlova and the film featured singing and dancing, having nothing to do with work. It is the most unusual of its type. The plot surrounds a love story between two individuals who want to play music. They are unrepresentative of Soviet values in that their focus is more on their music than their jobs. The gags poke fun at the local authorities and bureaucracy. There is no glorification of industry since it takes place in a small rural village. Work is not glorified either, since the plot revolves around a group of villagers using their vacation time to go on a trip up the Volga to perform in Moscow.
",Movie
"Volga-Volga followed the aesthetic principles of Socialist Realism rather than the ideological tenets. It became Stalin's favorite film and he gave it as a gift to President Roosevelt during WWII. It is another example of one of the films that claimed life is better. Released at the height of Stalin's purges, it provided escapism and a comforting illusion for the public.[49][citation needed]
",Movie
"Romance films or romance movies are romantic love stories recorded in visual media for broadcast in theaters and on TV that focus on passion, emotion, and the affectionate romantic involvement of the main characters and the journey that their genuinely strong, true and pure romantic love takes them through dating, courtship or marriage. Romance films make the romantic love story or the search for strong and pure love and romance the main plot focus. Occasionally, romance lovers face obstacles such as finances, physical illness, various forms of discrimination, psychological restraints or family that threaten to break their union of love. As in all quite strong, deep, and close romantic relationships, tensions of day-to-day life, temptations (of infidelity), and differences in compatibility enter into the plots of romantic films.[1]
",Movie
"Romantic films often explore the essential themes of love at first sight, young with older love, unrequited romantic love, obsessive love, sentimental love, spiritual love, forbidden love/romance, platonic love, sexual and passionate love, sacrificial love, explosive and destructive love, and tragic love. Romantic films serve as great escapes and fantasies for viewers, especially if the two people finally overcome their difficulties, declare their love, and experience life ""happily ever after"", implied by a reunion and final kiss. In romantic television series, the development of such romantic relationships may play out over many episodes, and different characters may become intertwined in different romantic arcs.
",Movie
"
",Movie
"A horror film is a film that seeks to elicit fear.[1] Initially inspired by literature from authors like Edgar Allan Poe, Bram Stoker, and Mary Shelley,[2] horror has existed as a film genre for more than a century. The macabre and the supernatural are frequent themes. Horror may also overlap with the fantasy, supernatural fiction, and thriller genres.
",Movie
"Horror films often aim to evoke viewers' nightmares, fears, revulsions and terror of the unknown. Plots within the horror genre often involve the intrusion of an evil force, event, or personage into the everyday world. Prevalent elements include ghosts, extraterrestrials, vampires, werewolves, demons, Satanism, evil clowns, gore, torture, vicious animals, evil witches, monsters, zombies, cannibalism, psychopaths, natural or man-made disasters, and serial killers.[3]
",Movie
"Some sub-genres of horror film include low-budget horror, action horror, comedy horror, body horror,[4] disaster horror, found footage,[5] holiday horror, horror drama, psychological horror,[6] science fiction horror, slasher, supernatural horror, gothic horror, natural horror, zombie horror, first-person horror, and teen horror.
",Movie
"The first depictions of the supernatural on screen appear in several of the short silent films created by the French pioneer filmmaker Georges Méliès in the late 1890s. The best known of these supernatural-based works is the 3-minute short film Le Manoir du Diable (1896), also known in English as The Haunted Castle or The House of the Devil. The film is sometimes credited as being the first ever horror film.[7] In The Haunted Castle, a mischievous devil appears inside a medieval castle and harasses the visitors. Méliès' other popular horror film is La Caverne maudite (1898), which translates literally to ""the accursed cave"". The film, also known for its English title The Cave of the Demons, tells the story of a woman stumbling over a cave that is populated by the spirits and skeletons of people who died there.[7] Méliès would also make other short films that historians consider now as horror-comedies. Une nuit terrible (1896), which translates to A Terrible Night, tells a story of a man who tries to get a good night's sleep but ends up wrestling a giant spider. His other film, L'auberge ensorcelée (1897), or The Bewitched Inn, features a story of a hotel guest getting pranked and tormented by an unseen presence.[8]
",Movie
"In 1897, the accomplished American photographer-turned director George Albert Smith created The X-Ray Fiend (1897), a horror-comedy that came out a mere two years after x-rays were invented. The film shows a couple of skeletons courting each other. An audience full of people unaccustomed to the idea would have found it frightening and otherworldly.[9] The next year, Smith created the short film Photographing a Ghost (1898), consisdered a precursor to the paranormal investigation subgenre. The film portrays three men attempting to photograph a ghost, only to fail time and again as the ghost eludes them and throws chairs. 
",Movie
"Japan also made early forays into the horror genre. In 1898, Japanese film company Konishi Honten released two horror films both written by Ejiro Hatta; Shinin No Sosei (Resurrection of a Corpse), and Bake Jizo (Jizo the Spook)[10] Shinin No Sosei told the story of a dead man who comes back to life after having fallen from a coffin that two men were carrying. (the writer Hatta played the dead man, while the coffin-bearers were played by Konishi employees.) Though there are no records of the cast, crew, or plot of Bake Jizo, it was likely based on the Japanese legend of Jizo statues, believed to provide safety and protection to children. The presence of the word bake—which can be translated to “spook,” “ghost,” or “phantom”—may imply a haunted or possessed statue.[11] (In Japan, Jizō, or respectfully as Ojizō-sama, is a beloved divinity who is seen as the guardian of children, and in particular, children who died before their parents. He has been worshipped as the guardian of the souls of mizuko, the souls of stillborn, miscarried, or aborted fetuses.)
",Movie
"Spanish filmmaker Segundo de Chomón is regarded as the most significant Spanish silent film director in an international context.[12] He was popular for his frequent camera tricks and optical illusions, an innovation that contributed heavily to the popularity of trick films in the period. His famous works include Satán se divierte (1907), which translates to Satan Having Fun, or Satan at Play; La casa hechizada (1908), or The House of Ghosts, considered to be one of the earliest cinematic depictions of a haunted house premise; and Le spectre rouge (1907) or The Red Spectre, a collaboration film with French director Ferdinand Zecca about a demonic magician who attempts to perform his act in a mysterious grotto.
",Movie
"Horror films of early cinema featured a slew of literary adaptations, adapting the tonally dark works of Edgar Allan Poe and Dante Alighieri, among others. The first of these literary adaptations was Selig Polyscope Company's production of Dr. Jekyll and Mr. Hyde (1908), now a lost film, based on the classic gothic novella Strange Case of Dr Jekyll and Mr Hyde, published 15 years prior, about a man who transforms between two contrasting personas. (The book tells the classic story of a man with an unpredictably dual nature: usually very good, but sometimes shockingly evil instead.)
",Movie
"Georges Méliès also liked adapting the Faust legend into his films. In fact, the French filmmaker produced at least six variations of the German legend of the man who made a pact with the devil. Among his notable Faust films include Faust aux enfers (1903), known primarily for its English title The Damnation of Faust, or Faust in Hell. It is the filmmaker's third film adaptation of the Faust legend. In it, Méliès took inspiration from Hector Berlioz's Faust opera, but it pays less attention to the story and more to the special effects that represent a tour of hell. Méliès then made a sequel called Damnation du docteur Faust (1904), released in the U.S. as Faust and Marguerite. This time, the film was based on the opera by Charles Gounod. Méliès' other devil-inspired films in this period include Les quat'cents farces du diable (1906), known in English as The Merry Frolics of Satan or The 400 Tricks of the Devil, a tale about an engineer who barters with the Devil for superhuman powers and is forced to face the consequences. Méliès would also make other horror-based short films that aren't inspired by Faust, most notably the fantastical and unsettling Le papillon fantastique (1909), where a magician turns a butterfly woman into a spider beast.
",Movie
"In 1910, Edison Studios produced the first filmed version of Mary Shelley's classic gothic 1818 novel Frankenstein, the popular story of a scientist creating a hideous, sapient creature after a successful scientific experiment. Adapted to the screen for the first time by J. Searle Dawley for Edison, Frankenstein (1910) was deliberately designed to de-emphasize the horrific aspects of the story and focus on the story's mystical and psychological elements.[13] Yet, the macabre nature of its source material made the film synonymous with the horror film genre.[14]
",Movie
"In March 1911, the hour-long Italian silent film epic L'Inferno was screened in the Teatro Mercadante in Naples.[15] The film, which was adapted from the first part of Dante Alighieri's Divine Comedy and based visually on Gustave Doré's haunting illustrations, became an international success and is arguably the first true blockbuster in all of cinema. It is also regarded by many scholars as the finest film adaptation of any of Dante's works to date. The film, which was directed by three artists; Francesco Bertolini, Adolfo Padovan, and Giuseppe de Liguoro, is worth noting and well-remembered for its stunning visualization of the nine circles of Hell. Its stunning special effects is also very well noted especially for its presentation of a massive Lucifer with wings that stretch out behind him in front of a black void. He is seen devouring the Roman figures Brutus and Gaius in a disgusting display of double exposure and scale manipulation. The film is also able to capture some of the manic, tortuous, and bizarre imagery and themes of Dante's complex masterwork.[16]
",Movie
"In the 1910s Georges Méliès would continue producing his Faustian films, the most significant of this period was Le Chevalier des Neiges (The Knight of the Snows) (1912). The film was Méliès' last film with Faustian themes,[17] and the last of many films in which the filmmaker appeared as the Devil.[18] The film tells a story of a princess kidnapped by Satan and thrown into a dungeon. Her lover, the brave Knight of the Snows, must then go on a journey to rescue her. Special effects in the film were created with stage machinery, pyrotechnics, substitution splices, superimpositions, and dissolves.[19] It is again, among few of the best examples of trick films that Georges Méliès and Segundo de Chomón helped popularized.
",Movie
"In 1912, French director Abel Gance released his short film Le masque d'horreur (The Mask of Horror).  The film tells a story of a mad sculptor who searches for the perfect realization of ""the mask of horror"". He places himself in front of a mirror after smearing blood over himself with the glass of an oil lamp. He then swallows a virulent poison to observe the effects of pain.[20]
",Movie
"In 1913, German directors Stellan Rye and Paul Wegener made the silent horror film Der Student von Prag, popularly known for its English title as The Student of Prague, loosely based on a short story by Edgar Allan Poe. The film tells a story of a student who inadvertently makes a Faustian bargain. In the film, the student tells a stranger to make him a rich man. So the stranger visits the student in his dorm room where he conjures up a hundred thousand pieces of gold and a contract for him to sign. In return, the stranger is granted to take anything he wants from the room and chooses to take the student's mirror. Upon moving it from the wall, a doppelgänger steps out and causes trouble. (In Western culture, a doppelgänger is a supernatural or ghostly double or look-alike of a specific person and usually seen as a harbinger of bad luck.) Cinematographer Guido Seeber utilized groundbreaking camera tricks to create the effect of the doppelgänger by using a mirror double which produces a seamless double exposure. The film was written by Hanns Heinz Ewers, a noted writer of horror and fantasy stories and his involvement with the screenplay lent a much needed air of respectability to the fledgling art form of horror film and German Expressionism [21]
",Movie
"Meanwhile, from November 1915 to June 1916, French writer/director Louis Feuillade released a weekly serial entitled Les Vampires where the filmmaker exploited the power of horror imagery to great effect. Consisting of 10 parts or episodes and roughly 7 hours long if combined, Les Vampires is considered to be one of the longest films ever made. The series tells a story of a criminal gang called the Vampires, who play upon their supernatural name and style to instill fear in the public and the police who desperately want to put a stop to them. [22] Marked as Feuillade's legendary opus, Les Vampires is considered a precursor to movie thrillers. The series is also a close cousin to the surrealist movement.[23]
",Movie
"Paul Wegener followed up his success with The Student of Prague by adapting a story inspired by the ancient Jewish legend of the golem, an anthropomorphic being that is magically created entirely from clay or mud. Wegener teamed up with Henrik Galeen to make  Der Golem (1915). The film, which is still partially lost, tells a story of an antiques dealer who finds a golem, a clay statue, brought to life centuries before. The dealer resurrects the golem as a servant, but the golem falls in love with the antiques dealer's wife. As she does not return his love, the golem commits a series of murders. Wegener then made a sequel, this time he teamed up with co-director Rochus Gliese, and made Der Golem und die Tänzerin (1917), or The Golem and the Dancing Girl as it is known in English. It is now considered a lost film. Wegener would make a third golem film three years later to round up his Der Golem trilogy.
",Movie
"In 1919, Austrian director Richard Oswald released a German silent anthology horror film called Unheimliche Geschichten, also known as Eerie Tales or Uncanny Tales. In the film, a bookshop closes and the portraits of the Strumpet, Death, and the Devil come to life and amuse themselves by reading stories--about themselves, of course, in various guises and eras. The film is split into five stories: The Apparition, The Hand, The Black Cat (based on the Edgar Allan Poe short story), The Suicide Club (based on the Robert Louis Stevenson short story collection) and Der Spuk (which translates to The Spectre in English). The film is described as the ""critical link between the more conventional German mystery and detective films of the mid 1910s and the groundbreaking fantastic cinema of the early 1920s.""[24]
",Movie
"Robert Wiene's Das Cabinet des Dr. Caligari (The Cabinet of Dr. Caligari) (1920) became a worldwide success and had a lasting impact on the film world, particularly for horror. It was not so much the story but the style that made it distinguishable from other films, ""Caligari's settings, some simply painted on canvas backdrops, are weirdly distorted, with caricatures of narrow streets, misshapen walls, odd rhomboid windows, and leaning doorframes. Effects of light and shadow were rendered by painting black lines and patterns directly on the floors and walls of sets.”[25] Critic Roger Ebert called it arguably ""the first true horror film"", and film reviewer Danny Peary called it cinema's first cult film and a precursor to arthouse films. Considered a classic, The Cabinet of Dr. Caligari helped draw worldwide attention to the artistic merit of German cinema and had a major influence on American films, particularly in the genres of horror and film noir, introducing techniques such as the twist ending and the unreliable narrator to the language of narrative film.
",Movie
"In October 1920, Paul Wegener teamed up with co-director Carl Boese to make the final Golem film entitled Der Golem, wie er in die Welt kam, known in English as The Golem: How He Came into the World. The final film in the Der Golem trilogy, The Golem: How He Came into the World (1920) is a prequel to Der Golem from 1915. In this film, Wegener stars as the golem who frightens a young lady with whom he is infatuated. The film is the best known of the series, as it is the only film that is completely preserved. It is also a leading example of early German Expressionism.
",Movie
"F. W. Murnau arguably made The first vampire-themed movie, Nosferatu (1922). It was an unauthorized adaptation of Bram Stoker's gothic horror novel Dracula. In Nosferatu, Murmau created some of cinema's most lasting and haunting imagery which famously involve shadows of the creeping Count Orlok. This helped popularized the expressionism style in filmmaking. Many expressionist works of this era emphasize a distorted reality, stimulating the human psyche and have influenced the horror film genre.
",Movie
"For most of the 1920s, German filmmakers like Wegener, Murmau, and Wiene would significantly influence later productions not only in horror films but in filmmaking in general. They would become the leading innovators of the German Expressionist movement. The plots and stories of the German Expressionist films often dealt with madness and insanity. Arthur Robison's film, Schatten – Eine nächtliche Halluzination (1923), literally Shadows - a Nocturnal Hallucination, also known as Warning Shadows in English is also one of the leading German Expressionist films. It tells the story of house guests inside a manor given visions of what might happen if the manor's host, the count played by Fritz Kortner, stays jealous and the guests do not reduce their advances towards his beautiful wife. Kortner's bulging eyes and twisted features are facets of a classic Expressionist performance style, as his unnatural feelings contort his face and body into something that appears other than human.[26]
",Movie
"In 1924, German filmmaker Paul Leni made another representative German Expressionist film with Das Wachsfigurenkabinett, or Waxworks as it is commonly known. The horror film tells a story of a writer who accepts a job from a wax museum to write a series of stories about the exhibits on Harun al-Rashid, the Caliph of Baghdad, Ivan the Terrible, the Tsar of All Rus' and Jack the Ripper, the unknown serial killer of London in order to boost business. Although Waxworks is often credited as a horror film, it is an anthology film that goes through several genres including a fantasy adventure, a historical film, and a horror film through its various episodes. Waxworks contain many elements present in a German Expressionist movie. The film features deep shadows, moving shapes, and warped staircases. The director said of the film, ""I have tried to create sets so stylized that they evidence no idea of reality."" Waxworks was director Paul Leni’s last film in Germany before he heading to Hollywood to make some of the most important horror films of the late silent era.[27]
",Movie
"Though the word horror to describe the film genre would not be used until the 1930s (when Universal Pictures began to release their initial monster films), earlier American productions often relied on horror themes. Many of these early films were considered dark melodramas because of their stock characters and emotion-heavy plots that focused on romance, violence, suspense, and sentimentality.[28]
",Movie
"In 1923, Universal Pictures started producing films with horror elements. This would become the first phase of the studio's Universal Classic Monsters series that would continue for three more decades. The first of these films was  The Hunchback of Notre Dame (1923) starring Lon Chaney as the hunchback Quasimodo. The film was adapted from the classic French gothic novel of the same name written by Victor Hugo in 1833, about a horribly deformed bell ringer in the cathedral of Notre-Dame. The film elevated Chaney, already a well-known character actor, to full star status in Hollywood, and also helped set a standard for many later horror films.
",Movie
"Other notable examples of Universal Studios' horror films include The Phantom of the Opera (1925), The Cat and the Canary (1927), The Unknown (1927), and The Man Who Laughs (1928). 
",Movie
"The trend of inserting an element of macabre into American pre-horror melodramas continued into the 1920s. Directors known for relying on macabre in their films during the 1920s were Maurice Tourneur, Rex Ingram, and Tod Browning. Ingram's The Magician (1926) contains one of the first examples of a ""mad doctor"" and is said to have had a large influence on James Whale's version of Frankenstein.[29] The Unholy Three (1925) is an example of Browning's use of macabre and unique style of morbidity; he remade the film in 1930 as a talkie, though The Terror (1928) was the first horror film with sound.
",Movie
"Other European countries also, contributed to the genre during this period. Victor Sjöström's The Phantom Carriage (Sweden, 1920) is a cautionary tale about a supernatural legend, Benjamin Christensen's Häxan (Denmark/Sweden, 1922) is a documentary-style, horror film, about witchcraft and superstition, and in 1928, Frenchman, Jean Epstein produced an influential film, The Fall of the House of Usher, based on the Poe tale.
",Movie
"In the 1930s Universal Pictures continued making movies with horror elements as they began to churn in a successful film series based on Gothic horror. Bela Lugosi stars in Tod Browning's Dracula (1931) and then Boris Karloff stars in James Whale's Frankenstein (1931), both films considered classic adaptations of the popular horror characters. The next year, Karloff  would play the titular character in The Mummy (1932). Make-up artist Jack Pierce would be responsible for creating the make-up of many of the classic 1930's monsters for Universal Studios. He would also come to design the Satanic make-up for Lugosi in the independently produced White Zombie (1932). 
",Movie
"After the release of The Mummy, Universal followed a trilogy of films based on the tales of Edgar Allan Poe; Murders in the Rue Morgue (1932), The Black Cat (1934), and The Raven (1935), the latter two of which teamed Lugosi with Karloff. Universal then began releasing sequels including Bride of Frankenstein (1935) and Dracula's Daughter (1936). They also made the first mainstream werewolf picture, Werewolf of London (1935) which wasn't a box office triumph during release despite being revered by audiences today.
",Movie
"Other studios followed Universal's lead. MGM's controversial Freaks (1932), which featured characters played by people who had real deformities, frightened audiences at the time. The studio even disowned the film, and it remained banned, in the United Kingdom, for 30 years.[30] Paramount Pictures' Dr. Jekyll and Mr. Hyde (1931) is remembered for its innovative use of photographic filters to create Jekyll's transformation before the camera.[31] And RKO created the highly successful and influential monster movie, King Kong (1933). With the progression of the genre, actors like Boris Karloff and Bela Lugosi were beginning to build entire careers in horror. 
",Movie
"Early in the decade also, Danish director Carl Theodor Dreyer created the horror fantasy film Vampyr (1932) based on elements from J. Sheridan Le Fanu's collection of supernatural stories In a Glass Darkly. The German-produced sound film tells the story of Allan Gray, a student of the occult who enters a village under the curse of a vampire. According to the book 1001 Movies You Must See Before You Die, Vampyr's ""greatness derives partly  from Dreyer's handling of the vampire theme in terms of sexuality and eroticism, and partly from its highly distinctive, dreamy look.""
",Movie
"In the 1940s, Val Lewton became a well known figure in early B-horror cinema for making low-budget movies for RKO Pictures, including I Walked with a Zombie (1943), The Body Snatcher (1945), and Cat People (1942), a film deemed by the United States' National Film Registry  as being ""culturally, historically, or aesthetically significant"".
",Movie
"The decade also sees the continuation of Universal Pictures' consistent releases of horror, suspense and science fiction films. This comes to be later known as the cult classic Universal Classic Monsters series which began in the 1920s and would later dissipate in the 1950s. In this decade Lon Chaney Jr. became the studio's leading monster movie actor supplanting the previous decades' leading stars Karloff and Lugosi by a wide margin in terms of the number of leading roles that he played. Chaney is best known playing Larry Talbot in The Wolf Man (1941) and its sequels and crossover films. He also played Frankenstein's monster in The Ghost of Frankenstein (1942), taking over Boris Karloff in the main role. The Mummy series was also continued with The Mummy's Tomb (1942), The Mummy's Ghost and The Mummy's Curse (both 1944) all starring Chaney Jr. as the Mummy.
",Movie
"Paramount Pictures also made horror films in the 1940s, the most popular of which is The Uninvited. The film has been noted by contemporary film scholars as being the first film in history to portray ghosts as legitimate entities rather than illusions or misunderstandings played for comedy. It depicts various supernatural phenomena, including disembodied voices, apparitions, and possession. MGM's best horror genre contribution of the 1940s would be Albert Lewin's The Picture of Dorian Gray, which was popularly known for its interesting use of color insert to show Dorian's haunting portrait. 
",Movie
"In 1945, Great Britain contributed the anthology horror film Dead of Night. In the film house guests tell at least five supernatural tales, the last of which being the most remembered. The film's last story, titled The Ventriloquist's Dummy features a ventriloquist tormented by a malevolent puppet.
",Movie
"The popularity of movie genres of the 1940s were mostly film noir, melodrama and mystery. It would then arguably be a stretch to point out that some mystery and thriller films can be considered horror genre contributions of the decade. These movies include The Spiral Staircase (1946) which tells the story of a serial killer targeting women with ""afflictions"", like the mute and blind; The Seventh Victim (1943), a horror/film noir story of a woman stumbling upon a Satanic cult while looking for her missing sister; and John Brahm's The Lodger (1944), where a landlady suspects her new lodger to be Jack the Ripper.
",Movie
"The Queen of Spades (1949) is a fantasy/horror film about an elderly countess who strikes a bargain with the devil and exchanges her soul for the ability to always win at cards. Wes Anderson ranked it as the sixth best British film.[32] Martin Scorsese said that The Queen of Spades is a ""stunning film"" and one of ""the few true classics of supernatural cinema.""[33] And Dennis Schwartz of Ozus' World Movie Reviews called it ""A masterfully filmed surreal atmospheric supernatural tale"".[34]
",Movie
"With advances in technology, the tone of horror films shifted from the Gothic towards contemporary concerns. A popular horror subgenre began to emerge: the Doomsday film.[35] Low-budget productions featured humanity overcoming threats such as alien invasions and deadly mutations to people, plants, and insects. Popular films of this genre include Creature from the Black Lagoon (1954) and The Blob (1958). 
",Movie
"1956's science fiction/horror film Invasion of the Body Snatchers concerns an xtraterrestrial invasion where aliens are capable of reproducing a duplicate replacement copy of each human. It is considered to be the most popular and most paranoid films from the golden age of American sci-fi cinema.
",Movie
"Japan's experience with Hiroshima and Nagasaki bore the well-known Godzilla (1954) and its many sequels, featuring mutation from the effects of nuclear radiation. This kickstarted the tokusatsu trend known as Kaiju films, a Japanese film genre that features giant monsters, usually attacking major cities and engaging the military and other monsters in battle. Other films in this genre that isn't about Godzilla include Rodan (1956) and The Mysterians (1957). Besides Kaiju films, Japan was also into ghost cat/feline ghost movies in the 50's. These include Ghost-Cat of Gojusan-Tsugi (1956), and Black Cat Mansion (1958), which tells a story of a samurai tormented by a cat possessed by the spirits of the people she killed. 
",Movie
"The 1950s is also well known for creature feature and Ray Harryhausen's visual effects work in many of the genre's films. His work on The Beast from 20,000 Fathoms (1953) is considered to be the film which kick started the 50s wave of monster movies and the concept of combining nuclear paranoia with the genre. The film was about a dinosaur that was awakened from frozen ice in the Arctic Circle by an atomic bomb test. Other creature films include It Came from Beneath the Sea (1955), Tarantula (1955), and The Giant Behemoth (1959). 
",Movie
"Hollywood directors and producers found ample opportunity for audience exploitation through gimmicks. House of Wax (1953) used the advent of 3-D film to draw audiences, while The Tingler (1959) used electric buzzers to scare audiences in their theater seats. Filmmakers continued to merge elements of science fiction and horror over the following decades. Considered a ""pulp masterpiece""[36] of the era was The Incredible Shrinking Man (1957), based on Richard Matheson's existentialist novel. The film conveyed the fears of living in the Atomic Age and the terror of social alienation.
",Movie
"Across the pond, the United Kingdom emerged as a major producer of horror films.[39] The Hammer company focused on the genre for the first time, enjoying huge international success from films involving classic horror characters which were shown in color for the first time.[40] Drawing on Universal's precedent, many films produced were Frankenstein and Dracula remakes, followed by many sequels. Christopher Lee starred in a number of Hammer Horror films, including The Curse of Frankenstein (1957), which Professor Patricia MacCormac called the ""first really gory horror film, showing blood and guts in colour"".[41] Other British companies contributed to a boom in horror film production in the United Kingdom in the 1960s and 1970s.
",Movie
"In television, the anthology series The Twilight Zone (1959-1964) has become a staple in horror fiction.[42]  Each episode presents a standalone story in which characters find themselves dealing with often disturbing or unusual events, an experience described as entering ""the Twilight Zone"". Although predominantly science-fiction, the show's paranormal and Kafkaesque events leaned the show towards fantasy and horror. The phrase “twilight zone,” is used today to describe surreal experiences.
",Movie
"Released in May 1960, the British psychological thriller film, Peeping Tom (1960) by Michael Powell, is a progenitor of the contemporary ""slasher film"",[43] though Alfred Hitchcock cemented the subgenre with Psycho released also in the same year.[44] Hitchcock, considered to be the ""Master of Suspense"" didn't set out to frighten fans the way many other traditional horror filmmakers do. Instead, he helped pioneer the art of psychological suspense. As a result, he managed to frighten his viewers by getting to the root of their deepest fears.[45] One of his most frightening films besides Psycho is The Birds (1963), where a seemingly idyllic town is overrun by violent birds. 
",Movie
"France continued the mad scientist theme with the film Eyes Without a Face (1960), a must-watch for the lover of mad scientist stories. The story follows Parisian police in search of the culprit responsible for the deaths of young women whose faces have been mutilated.[46] In Criterion's description of the film, they say it include ""images of terror, of gore, [and] of inexplicable beauty"".[47]
",Movie
"Meanwhile, Italian horror films became internationally notable thanks to Mario Bava's contributions. His film La Maschera del Demonio (1960), marketed in English as The Mask of Satan then wound up being known as Black Sunday in the United States and Revenge of the Vampire in the United Kingdom. In this film, Bava turned a Russian folk legend into a beguilling fairly tale about a young doctor who finds himself stranded in a haunted community and falls for a woman whose body become possessed by a woman executed for witchcraft. Three years later, Bava went on to make the horror anthology film Black Sabbath (1963) known in Italy as I tre volti della paura, literally 'The Three Faces of Fear'. 
",Movie
"The American International Pictures (AIP), in the early 60s, made a series of films based on stories by Edgar Allan Poe, most of which star Vincent Price, who became well known for his performances in subsequent horror films of the time. His success in House of Usher (1960) led him to do other Poe adaptions like 
Tales of Terror (1962) and The Masque of the Red Death (1964). Other popular Vincent Price horror films include House on Haunted Hill (1959) and The Last Man on Earth (1964) where Price becomes a reluctant Vampire hunter after becoming the last man on earth.
",Movie
"The British horror film The Haunting (1963) was directed and produced by Robert Wise. It is an adaptation of the 1959 horror novel The Haunting of Hill House by famed horror writer Shirley Jackson. Robert Wise's The Haunting is considered by a great many critics, aficionados, and casual fans of the horror genre to be one of the scariest films of all time. The film is best known for its brilliant use of canted frames, mirror reflections, fish-eye lenses and uncanny sound and image editing.
",Movie
"Roman Polanski made his first film in English with Repulsion (1965), which is considered to be his scariest and most disturbing work. Polanski's ""evocations of sexual panic and masterful use of sound puts the audiences' imagination to work in numerous ways"".[48] This psychological horror film tells the story of a young withdrawn woman who finds sexual advances repulsive and who, after she is left alone, becomes even more isolated and detached from reality.
",Movie
"Horror films of the 1960s used the supernatural premise to express the horror of the demonic. Jack Clayton's The Innocents (1961) tell the story of a governess who fears that the children she is watching over are possessed by ghosts haunting the estate they are staying. The story was based on Henry James' 1898 horror novella The Turn of the Screw. A few years later, Roman Polanski wrote and directed Rosemary's Baby (1968), based on the bestselling horror novel by Ira Levin. The highly influential film tells the story of a pregnant woman who suspects that an evil cult wants to take her baby for use in their rituals. Meanwhile, ghosts were a dominant theme in Japanese horror, in such films as Kwaidan, Onibaba (both 1964) and Kuroneko (1968).
",Movie
"Another influential American horror film of the 60s was George A. Romero's Night of the Living Dead (1968). Produced and directed by Romero on a budget of $114,000, it grossed $30 million internationally. Considered to be the first true zombie movie, the film began to combine psychological insights with gore. Distancing the era from earlier gothic trends, late 1960s films brought horror into everyday life. 
",Movie
"Low-budget splatter films from the likes of Herschell Gordon Lewis also gained prominence in the 1960s.[49] It's the precursor to ""torture porn"" movies that would become popular in the following decades. Some of Lewis' notorious works include Two Thousand Maniacs! (1964) which follows a group of Northern tourists  savagely tortured and murdered during a Confederate celebration of a small southern community's centennial; and Color Me Blood Red (1965), a story about a psychotic painter who murders civilians and uses their blood as red paint.
",Movie
"In television, the animated mystery Hanna-Barbera series Scooby-Doo, Where Are You! was broadcast from 1969 to 1970. The series centers on a group of teenagers and their dog who go to abandoned places to solve mysteries involving supposedly supernatural creatures through a series of antics and missteps. The animated series' simple formula had a major impact on future slasher films especially of its portrayal of villains in masks.[50]
",Movie
"The financial successes of the low-budget gore films of the ensuing years, and the critical and popular success of Rosemary's Baby, led to the release of more films with occult themes in the 1970s. The Exorcist (1973), the first of these movies, was a significant commercial success and was followed by scores of horror films in which a demon entity is represented as the supernatural evil, often by impregnating women or possessing children.
",Movie
"""Evil children"" and reincarnation became popular subjects. Robert Wise's film Audrey Rose (1977) for example, deals with a man who claims that his daughter is the reincarnation of another dead person. Alice, Sweet Alice (1977), is another Catholic-themed horror slasher about a little girl's murder and her sister being the prime suspect. Another popular occult horror movie was The Omen (1976), where a man realizes that his five-year-old adopted son is the Antichrist. Invincible to human intervention, Demons became villains in many horror films with a postmodern style and a dystopian worldview. Another example is The Sentinel (1977), in which a fashion model discovers that her new brownstone residence may actually be a portal to Hell. Notable examples of this subject from the late 1980s are Child's Play (1988), Night of the Demons (1988), and Pet Sematary (1989).
",Movie
"Don't Look Now (1973), a independent British-Italian film directed by Nicolas Roeg, was also notable. Its focus on the psychology of grief was unusually strong for a film featuring a supernatural horror plot. Another notable film is The Wicker Man (1973), a British mystery horror film dealing with the practice of ancient pagan rituals in the modern era.
",Movie
"In the 1970s, Italian filmmakers Mario Bava, Riccardo Freda, Antonio Margheriti, and Dario Argento developed giallo horror films that became classics and influenced the genre in other countries. Representative films include: Twitch of the Death Nerve, Deep Red and Suspiria.
",Movie
"The ideas of the 1960s began to influence horror films in the 70s, as the youth involved in the counterculture began exploring the medium. Wes Craven's The Hills Have Eyes (1977) and The Last House on the Left (1972) along with Tobe Hooper's The Texas Chain Saw Massacre (1974)[51] (based on the Ed Gein case) recalled the Vietnam War; while George A. Romero satirized the consumer society in his zombie sequel, Dawn of the Dead (1978). Meanwhile, the subgenre of comedy horror re-emerged in the cinema with The Abominable Dr. Phibes (1971), Young Frankenstein (1974), The Rocky Horror Picture Show (1975), and An American Werewolf in London (1981) among others.
",Movie
"Also in the 1970s, the works of the horror author Stephen King began to be adapted for the screen, beginning with Brian De Palma's adaptation of Carrie (1976), King's first published novel, for which the two female leads (Sissy Spacek and Piper Laurie) gained Oscar nominations. Next, was his third published novel, The Shining (1980), directed by Stanley Kubrick, which was a sleeper at the box office. At first, many critics and viewers had negative feedback toward The Shining. However, the film is now known as one of Hollywood's most classic horror films.
",Movie
"This psychological horror film has a variety of themes; ""evil children"", alcoholism, telepathy, and insanity. This type of film is an example of how Hollywood's idea of horror started to evolve. Murder and violence were no longer the main themes of horror films. In the 1970s and 1980s, psychological and supernatural horror started to take over cinema. Another classic Hollywood horror film is Tobe Hooper's Poltergeist (1982). Poltergeist is ranked the 20th scariest movie ever made by the Chicago, Illinois Film Critics Association. Both The Shining and Poltergeist involve horror being based on real-estate values. The evil and horror throughout the films come from where the movies are taking place.[52][53]
",Movie
"The Amityville Horror is a 1979 supernatural horror film directed by Stuart Rosenberg, based on Jay Anson's 1977 book of the same name. It stars James Brolin and Margot Kidder as a young couple who purchase a home they come to find haunted by combative supernatural forces. The Changeling is a 1980 Canadian psychological horror film directed by Peter Medak.
",Movie
"Steven Spielberg's shark horror film, Jaws (1975), began a new wave of killer animal stories, such as Orca (1977) and Up from the Depths (1979). Jaws is often credited as being one of the first films to use traditionally B movie elements such as horror and mild gore in a big-budget Hollywood film. In 1979, Don Coscarelli's Phantasm was the first of the Phantasm franchise.
",Movie
"A cycle of slasher films began in the 1970s and 1980s with the creation of Halloween by John Carpenter. “Halloween” was a significant influence on the horror industry and has become one of the quintessential foreunners of commercial horror films, grossing 70 Million usd on a shoestring budget of $300,000-325,000.[54] Its influence and inspiration can still be seen in films today.
",Movie
"Another notable 1970s slasher films are Bob Clark's Black Christmas (1974). Sleepaway Camp (1983) is known for its twist ending, which is considered by some to be one of the most shocking endings among horror films. My Bloody Valentine (1981) is a slasher film dealing with Valentine's Day fiction.
",Movie
"The boom in slasher films provided enough material for numerous comedic spoofs of the genre including Saturday the 14th (1981), Student Bodies (1981), National Lampoon's Class Reunion (1982), and Hysterical (1983).
",Movie
"This subgenre would be mined by dozens of increasingly violent movies throughout the subsequent decades. Sean S. Cunningham made Friday the 13th (1980), Wes Craven directed A Nightmare on Elm Street (1984), and Clive Barker made Hellraiser (1987). 
",Movie
"Some films explored urban legends such as ""The babysitter and the man upstairs"".
A notable example is When a Stranger Calls (1979), an American psychological horror film directed by Fred Walton starring Carol Kane and Charles Durning.
",Movie
"Alien (1979), a British-American science-fiction horror film directed by Ridley Scott was very successful, receiving both critical acclaim and being a box office success. John Carpenter's movie The Thing (1982) was also a mix of horror and sci-fi, but it was neither a box-office nor critical hit, but soon became a cult classic. However, nearly 20 years after its release, it was praised for using ahead-of-its-time special effects and paranoia.
",Movie
"The 1980s saw a wave of gory ""B movie"" horror films – although most of them were poorly reviewed by critics, many became cult classics and later saw success with critics. A significant example is Sam Raimi's Evil Dead movies, which were low-budget gorefests but had a very original plotline which was later praised by critics. In the Philippines, the first Shake, Rattle & Roll (1984) was released. The horror anthology film became a behemoth franchise in the country in the next decade.
",Movie
"Day of the Dead is a 1985 horror film written and directed by George A. Romero and the third film in Romero's Night of the Living Dead series.
",Movie
"Vampire horror was also popular in the 1980s, including cult vampire classics such as Fright Night (1985), The Lost Boys (1987), and Near Dark (also 1987). In 1984, Joe Dante's seminal monster comedy horror Gremlins became a box office hit with critics and audiences, and inspired a trend of ""little monster"" films such as Critters and Ghoulies.[citation needed]
",Movie
"David Cronenberg's films such as Shivers (1975), Rabid (1977), The Brood (1979), The Dead Zone (1983), and The Fly (1986) dealt with ""body horror"" and ""mad scientist"" themes.[55]
",Movie
"Several science fiction action horror movies were released in the 1980s, notably Aliens (1986) and Predator (1987). Notable comedy horror films of the 1980s include Re-Animator (1985), and Night of the Creeps (1986).
",Movie
"Henry: Portrait of a Serial Killer is a 1986 psychological horror crime film directed and co-written by John McNaughton about the random crime spree of a serial killer who seemingly operates with impunity. Pumpkinhead (1988) is a dark fantasy horror film, which is the directorial debut of special effects artist Stan Winston.
",Movie
"In the first half of the 1990s, the genre still contained many of the themes from the 1980s. The slasher films, A Nightmare on Elm Street, Friday the 13th, Halloween, and Child's Play, all saw sequels in the 1990s, most of which met with varied amounts of success at the box office but all were panned by critics, with the exception of Wes Craven's New Nightmare (1994) and the hugely successful film, The Silence of the Lambs (1991). The latter, which stars Jodie Foster and Anthony Hopkins, is considered a major horror movie of all times.[56] Misery (1990) also deals with a psychopath, and the film received critical acclaim for Kathy Bates's performance as the psychopathic Annie Wilkes.
",Movie
"New Nightmare, with In the Mouth of Madness (1995), The Dark Half (1993), and Candyman (1992), were part of a mini-movement of self-reflexive or metafictional horror films. Each film touched upon the relationship between fictional horror and real-world horror. Candyman, for example, examined the link between an invented urban legend and the realistic horror of the racism that produced its villain. In the Mouth of Madness took a more literal approach, as its protagonist actually hopped from the real world into a novel created by the madman he was hired to track down. This reflective style became more overt and ironic with the arrival of Scream (1996).
",Movie
"In Interview with the Vampire (1994), the ""Theatre de Vampires"" (and the film itself, to some degree) invoked the Grand Guignol style, perhaps to further remove the undead performers from humanity, morality and class. The horror movie soon continued its search for new and effective frights. In the 1985 novel, The Vampire Lestat, by the author Anne Rice (who penned Interview with the Vampire's screenplay and the 1976 novel of the same name) suggests that its antihero Lestat inspired and nurtured the Grand Guignol style and theatre.
",Movie
"Two main problems pushed horror backward during this period: firstly, the horror genre wore itself out with the proliferation of nonstop slasher and gore films in the eighties. Secondly, the adolescent audience which feasted on the blood and morbidity of the previous decade grew up, and the replacement audience for films of an imaginative nature were being captured instead by the explosion of science-fiction and fantasy films, courtesy of the special effects possibilities with advances made in computer-generated imagery.[57]  Examples of these CGI include movies like Species (1995), Anaconda (1997), Mimic (1997), Blade (1998), Deep Rising (1998), House on Haunted Hill (1999), Sleepy Hollow (1999), and The Haunting (1999).
",Movie
"To re-connect with its audience, horror became more self-mockingly ironic and outright parodic, especially in the latter half of the 1990s. Peter Jackson's Braindead (1992) (known as Dead Alive in the United States) took the splatter film to ridiculous excesses for comic effect. Wes Craven's Scream (written by Kevin Williamson) movies, starting in 1996, featured teenagers who were fully aware of, and often made reference to, the history of horror movies, and mixed ironic humour with the shocks (despite Scream 2 and Scream 3 utilising less use of the humour of the original, until Scream 4 in 2011, and rather more references to horror film conventions). Along with I Know What You Did Last Summer (1997) (also written by Williamson) and Urban Legend (1998), they re-ignited the dormant slasher film genre.
",Movie
"Event Horizon (1997) is a British-American science fiction horror film directed by Paul W. S. Anderson. The Sixth Sense (1999) is a supernatural horror film written and directed by M. Night Shyamalan, which tells the story of Cole Sear (Haley Joel Osment), a troubled, isolated boy who is able to see and talk to the dead, and an equally troubled child psychologist named Malcolm Crowe (Bruce Willis) who tries to help him.
",Movie
"House on Haunted Hill is a 1999 horror film directed by William Malone which follows a group of strangers who are invited to a party at an abandoned asylum, where they are offered $1 million each by an amusement park mogul if they are able to survive the night. It is a remake of the 1959 film of the same title. Other horror films of the late 1990s include Cube (1997), The Faculty (1998), Disturbing Behavior (1998), Stir of Echoes (1999), Stigmata (1999), and Existenz (1999).
",Movie
"Monster horror was quite popular in the 1990s. Tremors (1990) is
the first installment of the Tremors franchise. Lake Placid (1999) is another monster horror film, written by David E. Kelley and directed by Steve Miner.
",Movie
"Another successful horror film is Audition, a 1999 Japanese film based on the novel of the same name, directed by Takashi Miike. Around this period, Japanese horror started becoming popular in English speaking countries.
",Movie
"The film The Last Broadcast (1998) served as inspiration for the highly successful The Blair Witch Project (1999), which popularized the found footage horror subgenre. The theme of witchcraft was also addressed in The Witches (1990), starring Anjelica Huston, and The Craft (1996), a supernatural horror film directed by Andrew Fleming. Wolf is a 1994 romantic horror film following the transformation of a man (Jack Nicholson) into a werewolf.
",Movie
"Ravenous (1999) starring Guy Pearce and directed by Antonia Bird is a ""quirky""[58] and gruesome movie based on the real life horror story of the Donner party that got stranded in the Sierra Nevada mountains in 1847 due to snow.[59]
",Movie
"The decade started with American Psycho (2000) directed by Mary Harron starring Christian Bale as a charismatic serial killer and Manhattan business mogul. The movie was highly controversial when released and remains a cult classic today.[60] Scary Movie (2000), a comedy horror directed by Keenen Ivory Wayans parodied of the horror, slasher, and mystery genres. The film received mixed reviews from critics. By contrast, Valentine (2001) was a conventional horror film. It had some success at the box office, but was derided by critics for being formulaic and relying on foregone horror film conventions. The Others (2001) was hugely successful, winning and being further nominated for many awards. It is a 2001 Spanish-American supernatural gothic horror film with elements of psychological horror. It was written, directed, and scored by Alejandro Amenábar. It stars Nicole Kidman and Fionnula Flanagan.
",Movie
"Franchise films such as Jason X (2001) and Freddy vs. Jason (2003) also made a stand in theaters. Final Destination (2000) marked a successful revival of teen-centered horror and spawned five installments. Jeepers Creepers series was also successful. Films such as Hollow Man (2000), Cabin Fever (2002), House of 1000 Corpses (2003) (the latter an exploitation horror film written, co-scored and directed by Rob Zombie in his directorial debut) and the previous mentions helped bring the genre back to Restricted ratings in theaters. Van Helsing (2004) and Underworld series had huge box office success, despite mostly negative reviews by critics.  Ginger Snaps (2000) is a Canadian film dealing with the tragic transformation of a teenage girl who is bitten by a werewolf. Signs (2002) revived the science fiction alien theme. The Descent, a 2005 British adventure horror film written and directed by Neil Marshall was also successful. Another notable film is Drag Me to Hell, a 2009 American supernatural horror film co-written and directed by Sam Raimi. The Strangers (2008) deals with unprovoked stranger-on-stranger violence. The House of the Devil (2009) is inspired by the ""satanic panic"" of the 1980s. Trick 'r Treat is a 2007 anthology horror film written and directed by Michael Dougherty and produced by Bryan Singer. Black Water (2007) is a British-Australian natural horror film. Another natural adventure horror film is The Ruins (2008), which is based on the novel of the same name by Scott Smith.
",Movie
"Several horror film adaptations from comic books and video games were produced. 30 Days of Night (2007) is based on the comic book miniseries of the same name. The story focuses on an Alaskan town beset by vampires as it enters into a 30-day long polar night. Comic book adaptations like the Blade series, Constantine (2005), and Hellboy (2004) also became box office successes. The Resident Evil video game franchise was adapted into a film released in March 2002, and several sequels followed.  Other video game adaptations like Doom (2005) and Silent Hill (2006) also had moderate box office success.
",Movie
"Some pronounced trends have marked horror films. Films from non-English language countries have become successful. The Devil's Backbone (2001) is such an example. It is a 2001 Spanish-Mexican gothic horror film directed by Guillermo del Toro, and written by del Toro, David Muñoz, and Antonio Trashorras. A French horror film Brotherhood of the Wolf (2001) became the second-highest-grossing French language film in the United States in the last two decades. The Swedish film Let the Right One In (2008) was also successful. REC is a 2007 Spanish zombie horror film, co-written and directed by Jaume Balagueró and Paco Plaza. Martyrs (2008), a French-Canadian horror film, was controversial upon its release, receiving polarizing reviews. Another notable film is The Orphanage (2007), a Spanish horror film and the debut feature of Spanish filmmaker J. A. Bayona. A Tale of Two Sisters is a 2003 South Korean psychological drama horror film written and directed by Kim Jee-woon. Shutter (2004) is a Thai horror film which focuses on mysterious images seen in developed pictures. Cold Prey is a 2006 Norwegian slasher film directed by Roar Uthaug.
",Movie
"Another trend is the emergence of psychology to scare audiences, rather than gore. The Others (2001) proved to be a successful example of a psychological horror film. A minimalist approach which was equal parts Val Lewton's theory of ""less is more"", usually employing the low-budget techniques utilized on The Blair Witch Project (1999), has been evident, particularly in the emergence of Asian horror movies which have been remade into successful Americanized versions, such as The Ring (2002), The Grudge (2004), Dark Water (2005), and Pulse (2006). In March 2008, China banned the movies from its market.[61] Credo (2008) and Triangle (2009) are two British psychological horror films. What Lies Beneath (2000) is a supernatural horror film directed by Robert Zemeckis, starring Harrison Ford and Michelle Pfeiffer as a couple who experience a strange haunting of their home. Orphan (2009) is a notable psychological horror film. Another psychological horror film is 1408 (2007), based on Stephen King's 1999 short story of the same name. Two Australian horror films that deal with teenagers are Lake Mungo (2008) and The Loved Ones (2009).
",Movie
"The films I Am Legend (2007), Quarantine (2008), Zombieland (2009), and 28 Days Later (2002) featured an update of the apocalyptic and aggressive zombie genre. The latter film spawned a sequel: 28 Weeks Later (2007). An updated remake of Dawn of the Dead (2004) soon appeared as well as the zombie comedy Shaun of the Dead (2004) and Spanish -Cuban comedy zombie film Juan of the Dead (2012). This resurgence led George A. Romero to return to his Living Dead series with Land of the Dead (2005), Diary of the Dead (2007) and Survival of the Dead (2009).[62] Cannibals were present in horror films such as  Dahmer (2002), Wrong Turn (2003), Tooth and Nail (2007), and Dying Breed (2008). Jennifer's Body (2009) starring Megan Fox and Amanda Seyfried, written by Diablo Cody and directly by Karyn Kusama brings a succubus into a suburban American high school. 
",Movie
"The Australian film Wolf Creek (2005) written, co-produced, and directed by Greg McLean revolves around three backpackers who find themselves taken captive and after a brief escape, hunted down by Mick Taylor in the Australian outback. The film was ambiguously marketed as being ""based on true events""; the plot bore elements reminiscent of the real-life murders of tourists by Ivan Milat in the 1990s, and Bradley Murdoch in 2001; and contained more extreme violence. An extension of this trend was the emergence of a type of horror with emphasis on depictions of torture, suffering, and violent deaths, (variously referred to as ""horror porn"", ""torture porn"", ""splatterporn"" and ""gore-nography"") with films such as Ghost Ship (2002), The Collector (2009), Saw (2004), Hostel (2005), and their respective sequels, frequently singled out as examples of emergence of this subgenre.[63] The Saw film series holds the Guinness World Record of the highest-grossing horror franchise in history.[64] Finally, with the arrival of Paranormal Activity (2007), which was well received by critics and an excellent reception at the box office, minimalist horror approach started by The Blair Witch Project was reaffirmed. Cloverfield (2008) is another found footage horror film. The Mist (2007) is a science-fiction horror film based on the 1980 novella of the same name by Stephen King. Antichrist (2009) is an English-language Danish experimental horror film written and directed by Lars von Trier, and starring Willem Dafoe and Charlotte Gainsbourg. The Exorcism of Emily Rose is a 2005 legal drama horror film directed by Scott Derrickson, loosely based on the story of Anneliese Michel. The Children (2008) is British horror film focusing on the mayhem created by several children. Another 2008 British horror film is Eden Lake.
",Movie
"Remakes of earlier horror movies became routine in the 2000s. In addition to the remake of Dawn of the Dead (2004), as well as the remake of both Herschell Gordon Lewis' cult classic, 2001 Maniacs (2003), and the remake of Tobe Hooper's classic, The Texas Chainsaw Massacre (2003), there was also the 2007 Rob Zombie-written and -directed remake of John Carpenter's Halloween.[65] The film focused more on Michael's backstory than the original did, devoting the first half of the film to Michael's childhood. It was critically panned by most,[66][67] but was a success in its theatrical run, spurring its own sequel. This film helped to start a ""reimagining"" riot in horror filmmakers. Among the many remakes or ""reimaginings"" of other popular horror films and franchises are films such as Thirteen Ghosts (2001), The Texas Chainsaw Massacre (2003), The Hills Have Eyes (2006), Friday the 13th (2009),[68] Children of the Corn (2009),[69] Halloween (2007),  Prom Night (2008), The Omen (2006), Carrie (2002), The Wicker Man (2006), Day of the Dead (2008), Night of the Demons (2009), My Bloody Valentine (2009), Willard (2003), Black Christmas (2006), The Amityville Horror (2005), April Fool's Day (2008), The Fog (2005), The Hitcher (2007), It's Alive (2009), When a Stranger Calls (2006), and The Last House on the Left (2009).
",Movie
"Remakes remain popular, with films such as A Nightmare on Elm Street (2010),[70] The Crazies (2010), I Spit on Your Grave (2010), Don't Be Afraid of the Dark (2010), Fright Night (2011), Maniac (2012), Poltergeist (2015), and Suspiria (2018). The 1976 film, Carrie, saw its second remake in 2013, which is the third film adaptation of Stephen King's 1974 novel of the same name. Child's Play saw a sequel with Curse of Chucky (2013), while Hellraiser: Judgment (2018) become the tenth installment in the Hellraiser film series. Halloween is a 2018 slasher film which is the eleventh installment in the Halloween film series, and a direct sequel to the  1978 film of the same name, while effecting a retcon of all previous sequels. The 2013 Evil Dead is the fourth installment in the Evil Dead franchise, and serves as a soft reboot of the original 1981 film and as a continuation to the original film trilogy.
",Movie
"Serialized, found footage style web videos featuring Slender Man became popular on YouTube in the beginning of the decade. Such series included TribeTwelve, EverymanHybrid, and Marble Hornets, the latter of which has been adapted into a feature film. Slender Man (2018) is supernatural horror film, based on the character of the same name. The character as well as the multiple series is credited with reinvigorating interest in found footage as well as urban folklore. Horror has become prominent on television with The Walking Dead, American Horror Story, and The Strain, and on online streaming services like Netflix's Stranger Things and Haunting of Hill House. Also, many popular horror films have had successful television series made: Psycho spawned Bates Motel, The Silence of the Lambs spawned Hannibal, and both Scream and Friday the 13th had TV series in development.[71][72]
",Movie
"You're Next (2011) and The Cabin in the Woods (2012) led to a return to the slasher genre; the latter was intended also as a critical satire of torture porn.[73] The Green Inferno (2015) pays homage to the controversial horror film, Cannibal Holocaust (1980). The Australian psychological horror film, The Babadook (2014) directed by Jennifer Kent received critical acclaim and won many awards.[74] It Follows (2014) subverted traditional horror tropes of sexuality and slasher films and enjoyed commercial and critical success. The Conjuring Universe is a series of horror films which deal with the paranormal. The series include The Conjuring (2013), The Conjuring 2 (2016), Annabelle (2014), Annabelle: Creation (2017) and The Nun (2018). Sinister (2012) is a British-American supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill. Another notable supernatural horror film is Insidious (2010). The Witch (2015) is a historical period supernatural horror film written and directed by Robert Eggers in his directorial debut, which follows a Puritan family encountering forces of evil in the woods beyond their New England farm. Get Out (2017) received universal acclaim from critics and audiences alike. Its plot follows a black man who uncovers a disturbing secret when he meets the family of his white girlfriend. Adapted from the Stephen King novel, It (2017) set a box office record for horror films by grossing $123.1 million on opening weekend in the United States and nearly $185 million globally.[75] Gerald's Game (2017) is a psychological horror film based on Stephen King's novel of the same name. Other horror films include Frozen (2010), Black Swan (2010), Devil (2010), The Innkeepers (2011), Oculus (2013), Under the Skin (2013), Mama (2013), Green Room (2015), The Invitation (2015), Hush (2016), Lights Out (2016), Don't Breathe (2016), Mother! (2017), It Comes at Night (2017), and Unsane (2018). 
",Movie
"A Quiet Place (2018) is a critically acclaimed post-apocalyptic science-fiction horror film with a plot that follows a family who must live life in silence while hiding from extraterrestrial creatures that arrived on earth on fragments from their exploded home planet, and which hunt exclusively by sound. Annihilation (2018) is another successful science-fiction horror film. Hereditary (2018) follows a family haunted after the death of their secretive grandmother.
",Movie
"Several notable found footage horror films were produced, including The Last Exorcism (2010), V/H/S (2012), Unfriended (2014), The Taking of Deborah Logan (2014), The Visit (2015). Various themes were addressed in the horror of this period. Horror films which deal with troubled teens include  Excision (2012) and Split (2016). The Autopsy of Jane Doe (2016) depicts coroners who experience supernatural phenomena while examining the body of an unidentified woman. The Purge is an action horror film franchise, consisting of four films and a television series, which are based on a future dystopian United States, where all crime is made legal once a year. Contracted (2013), Starry Eyes (2014), American Mary (2012) deal with body horror. Kill List (2011) is a British crime drama psychological horror film which deals with contract killers. The Hallow (2015) follows a family who go to a remote rural place in Ireland and have to deal with demonic creatures living in the woods. Prometheus (2012) and Alien: Covenant (2017) address extraterrestrial themes. Friend Request (2016) and The Den (2013) are examples of cyber horror. The Neon Demon (2016) follows an aspiring model in Los Angeles whose beauty and youth generate intense fascination and jealousy within the industry.  #Horror (2015) depicts a group of wealthy 7th grade girls who face a night of terror together after a social network game spirals out of control. The Other Side of the Door (2016) deals with a mother who attempts to use a ritual to meet her dead son for a last time to say goodbye, but misuses the ritual. Truth or Dare (2018) follows a group of college students who play a game of truth or dare? while on vacation in Mexico, only to realize it has deadly consequences if they don't follow through on their tasks. Ouija: Origin of Evil (2016) focuses on a widow and her family adding a Ouija board to their phony seance business where, unbeknownst to them, they invite a spirit that possesses the youngest daughter. The Blackcoat's Daughter (also known as February) is a 2015 American-Canadian supernatural psychological horror film which follows two Catholic schoolgirls who get left behind at their boarding school over winter break, where  the nuns are rumored to be satanists.
",Movie
"The success of non-English language films continued with the Swedish film, Marianne (2011), while Let the Right One In (2008)  was the subject of a Hollywood remake, Let Me In (2010). South Korean horror produced I Saw the Devil (2010) and Train to Busan (2016). Raw is a 2016 French-Belgian horror drama written and directed by Julia Ducournau, and starring Garance Marillier. Goodnight Mommy (2014) (German: Ich seh, Ich seh) is an Austrian horror film. Verónica is a 2017 Spanish horror film loosely based on real events. ""A Girl Walks Home Alone at Night"" (2014) directed by Ana Lily Amirpour is vampire film in Farsi that transcends simple vampire and horror categorization.[76] Untamed (2016) directed by  Amat Escalante is a unique psychological-sexual thriller.[77]
",Movie
"Horror films' evolution throughout the years has given society a new approach to resourcefully utilize their benefits. The horror film style has changed over time, but, in 1996, Scream set off a ""chain of copycats"", leading to a new variety of teenage, horror movies.[83] This new approach to horror films began to gradually earn more and more revenue as seen in the progress of Scream movies; the first movie earned $6 million and the third movie earned $101 million.[83] The importance that horror films have gained in the public and producers’ eyes is one obvious effect on our society.
",Movie
"Horror films' income expansion is only the first sign of the influences of horror flicks. The role of women and how women see themselves in the movie industry has been altered by the horror genre. Early horror films such as My Bloody Valentine (1981), Halloween (1978), and Friday the 13th (1980) were produced mostly for male audiences in order to ""feed the fantasies of young men"".[84] This idea is no longer prevalent in horror films, as women have become not only the main audience and fans of horror films but also the main protagonists of contemporary horror films.[85] Movie makers have also begun to integrate topics more broadly associated with other genres into their films in order to grow audience appeal.[84]
",Movie
"Many early horror films created great social and legal controversy. In the U.S., the Motion Picture Production Code which was implemented in 1930, set moral guidelines for film content, restraining movies containing controversial themes, graphic violence, explicit sexuality and/or nudity. The gradual abandonment of the Code, and its eventual formal repeal in 1968 (when it was replaced by the MPAA film rating system) offered more freedom to the movie industry. Nevertheless, controversy continued to surround horror movies, and many continued to face censorship issues around the world. For example, 1978's I Spit on Your Grave, an American rape-and-revenge exploitation horror film written, co-produced, directed, and edited by Meir Zarchi, was received negatively by critics, but it attracted a great deal of national and international attention due to its explicit scenes of rape, murder and prolonged nudity, which led to bans in countries such as Ireland, Norway, Iceland, and West Germany. Many of these countries later removed the ban, but the film remains prohibited in Ireland.[86]
",Movie
"While horror is only one genre of film, the influence it presents to the international community is large. Horror movies tend to be a vessel for showing eras of audiences issues across the globe visually and in the most effective manner. Jeanne Hall, a film theorist, agrees with the use of horror films in easing the process of understanding issues by making use of their optical elements.[87] The use of horror films to help audiences understand international prior historical events occurs, for example, to depict the horrors of the Vietnam War, the Holocaust and the worldwide AIDS epidemic.[88] However, horror movies do not always present positive endings. In fact, in many occurrences the manipulation of horror presents cultural definitions that are not accurate, yet set an example to which a person relates to that specific cultural from then on in their life.[89]
",Movie
"The visual interpretations of films can be lost in the translation of their elements from one culture to another, like in the adaptation of the Japanese film Ju on into the American film The Grudge. The cultural components from Japan were slowly ""siphoned away"" to make the film more relatable to a western audience.[90] This deterioration that can occur in an international remake happens by over-presenting negative cultural assumptions that, as time passes, sets a common ideal about that particular culture in each individual.[89] Holm's discussion of The Grudge remakes presents this idea by stating, ""It is, instead, to note that The Grudge films make use of an un-theorized notion of Japan... that seek to directly represent the country.""
",Movie
"
",Movie
"Science fiction (often shortened to Sci-Fi or SF) is a genre of speculative fiction, typically dealing with imaginative concepts such as advanced science and technology, spaceflight, time travel, and extraterrestrial life. Science fiction often explores the potential consequences of scientific and other innovations, and has been called a ""literature of ideas"".[1][2]
",Movie
"""Science fiction"" is difficult to define, as it includes a wide range of subgenres and themes. James Blish wrote: ""Wells used the term originally to cover what we would today call ‘hard’ science fiction, in which a conscientious attempt to be faithful to already known facts (as of the date of writing) was the substrate on which the story was to be built, and if the story was also to contain a miracle, it ought at least not to contain a whole arsenal of them.""[3]
",Movie
"Isaac Asimov said:  ""Science fiction can be defined as that branch of literature which deals with the reaction of human beings to changes in science and technology.""[4]  According to Robert A. Heinlein, ""a handy short definition of almost all science fiction might read: realistic speculation about possible future events, based solidly on adequate knowledge of the real world, past and present, and on a thorough understanding of the nature and significance of the scientific method.""[5]
",Movie
"Lester del Rey wrote, ""Even the devoted aficionado or fan—has a hard time trying to explain what science fiction is"", and that the reason for there not being a ""full satisfactory definition"" is that ""there are no easily delineated limits to science fiction.""[6] Author and editor Damon Knight summed up the difficulty, saying ""science fiction is what we point to when we say it"",[7] while author Mark C. Glassy argues that the definition of science fiction is like the definition of pornography: you do not know what it is, but you know it when you see it.[8]
",Movie
"Forrest J Ackerman is credited with first using the term ""Sci-Fi"" (analogous to the then-trendy ""hi-fi"") in 1954.[9]  as science fiction entered popular culture, writers and fans active in the field came to associate the term with low-budget, low-tech ""B-movies"" and with low-quality pulp science fiction.[10][11][12] By the 1970s, critics within the field such as Knight and Terry Carr were using sci-fi to distinguish hack-work from serious science fiction.[13] Peter Nicholls writes that ""SF"" (or ""sf"") is ""the preferred abbreviation within the community of sf writers and readers.""[14] Robert Heinlein found even ""science fiction"" insufficient, and suggested the term speculative fiction to be used instead, which has continued to be applied to ""serious"" or ""thoughtful"" science fiction.
",Movie
"Science fiction  had its beginnings in the time when the line between myth and fact was blurred. Written in the 2nd century AD by the Hellenized Syrian satirist Lucian, A True Story contains many themes and tropes that are characteristic of modern science fiction, including travel to other worlds, extraterrestrial lifeforms, interplanetary warfare, and artificial life. Some consider it the first science fiction novel.[15] Some of the stories from The Arabian Nights,[16][17] along with the 10th century The Tale of the Bamboo Cutter[17] and Ibn al-Nafis's 13th century Theologus Autodidactus[18] also contain elements of science fiction.
",Movie
"Products of the Age of Reason and the development of modern science itself, Johannes Kepler's Somnium (1620–1630), Francis Bacon's The New Atlantis (1627),[19] Cyrano de Bergerac's Comical History of the States and Empires of the Moon (1657) and The States and Empires of the Sun (1662), Margaret Cavendish's ""The Blazing World"" (1666),[20] Jonathan Swift's Gulliver's Travels (1726), Ludvig Holberg's novel Nicolai Klimii Iter Subterraneum (1741) and Voltaire's Micromégas (1752) are some of the first true science fantasy works.[21][22] Isaac Asimov and Carl Sagan considered Somnium the first science fiction story.  It depicts a journey to the Moon and how the Earth's motion is seen from there.[23]
",Movie
"Following the 18th-century development of the novel as a literary form, Mary Shelley's books Frankenstein (1818) and The Last Man (1826) helped define the form of the science fiction novel.  Brian Aldiss has argued that Frankenstein was the first work of science fiction.[24] Edgar Allan Poe wrote several stories considered science fiction, including one about a trip to the Moon.[25][26] Jules Verne was noted for his attention to detail and scientific accuracy, especially Twenty Thousand Leagues Under the Sea (1870) which predicted the modern nuclear submarine.[27][28][29][30] In 1887 the novel El anacronópete by Spanish author Enrique Gaspar y Rimbau introduced the first time machine.[31][32]
",Movie
"Many critics consider H. G. Wells one of science fiction's most important authors,[33][34] or even ""the Shakespeare of science fiction”.[35]  His most notable science fiction works include The Time Machine (1895), The Island of Doctor Moreau (1896), The Invisible Man (1897), and The War of the Worlds (1898).  His science fiction imagined time travel, alien invasion, invisibility, and biological engineering.  In his non-fiction futurologist works he predicted the advent of airplanes, military tanks, space travel, nuclear weapons, satellite television and something resembling the World Wide Web.[36]
",Movie
"In 1912 Edgar Rice Burroughs published A Princess of Mars, the first of his three-decade-long planetary romance series of Barsoom novels, set on Mars and featuring John Carter as the hero.[37]
",Movie
"In 1926 Hugo Gernsback published the first American science fiction magazine, Amazing Stories, in which he wrote:
",Movie
"By 'scientifiction' I mean the Jules Verne, H. G. Wells and Edgar Allan Poe type of story—a charming romance intermingled with scientific fact and prophetic vision... Not only do these amazing tales make tremendously interesting reading—they are always instructive.  They supply knowledge... in a very palatable form... New adventures pictured for us in the scientifiction of today are not at all impossible of realization tomorrow... Many great science stories destined to be of historical interest are still to be written... Posterity will point to them as having blazed a new trail, not only in literature and fiction, but progress as well.[38][39][40]
",Movie
"In 1928 E. E. ""Doc"" Smith’s first published work, The Skylark of Space written in collaboration with Lee Hawkins Garby, appeared in Amazing Stories.  It is often called the first great space opera.[41] In 1928 Philip Francis Nowlan's original Buck Rogers story, Armageddon 2419, appeared in Amazing Stories. This was followed by a Buck Rogers comic strip, the first serious science fiction comic.[42]
",Movie
"In 1937 John W. Campbell became editor of Astounding Science Fiction, an event which is sometimes considered the beginning of the Golden Age of Science Fiction  characterized by stories celebrating scientific achievement and progress.[43]  In 1942, Isaac Asimov started his Foundation series, which chronicles the rise and fall of galactic empires and introduced psychohistory.[44][45]  The ""Golden Age"" is often said to have ended in 1946, but sometimes the late 1940s and the 1950s are included.[46]
",Movie
"Theodore Sturgeon’s  1953 novel More Than Human explored possible future human evolution.[47][48][49]  In 1957 Andromeda: A Space-Age Tale by the Russian writer and paleontologist Ivan Yefremov presented a view of a future interstellar communist civilization and is considered one of the most important Soviet science fiction novels.[50][51]  In 1959 Robert A. Heinlein's Starship Troopers marked a departure from his earlier juvenile stories and novels.[52]   It is one of the first and most influential examples of military science fiction,[53][54] and introduced the concept of powered armor exoskeletons.[55][56][57]  The German space opera series Perry Rhodan, by various authors, started in 1961 with an account of the first Moon landing and has since expanded to the entire Universe and billions of years; becoming the most popular science fiction book series of all time.[58]
",Movie
"In the 1960s and 1970s  New Wave science fiction was known for its embrace of a high degree of experimentation, both in form and in content, and a highbrow and self-consciously ""literary"" or artistic sensibility.[21][59][60]   In 1961 Solaris by Stanisław Lem was published in Poland.[61] The novel dealt with the theme of human limitations as its characters attempted to study a seemingly intelligent ocean on a newly discovered planet.[62][63]   1965's Dune by Frank Herbert featured a much more complex and detailed imagined future society than had been common in science fiction before.[64] In 1968 Philip K. Dick’s best-known novel Do Androids Dream of Electric Sheep? was published. It is the literary source of the film Blade Runner.[65] 1969’s The Left Hand of Darkness by Ursula K. Le Guin was set on a planet in which the inhabitants have no fixed gender.  It is one of the most influential examples of social science fiction, feminist science fiction, and anthropological science fiction.[66][67][68]
",Movie
"In 1976 C. J. Cherryh published  Gate of Ivrel and Brothers of Earth, which began her Alliance-Union universe future history series.[69][70][71]  In 1979 Science Fiction World began publication in the People's Republic of China.[72] It dominates the Chinese science fiction magazine market, at one time claiming a circulation of 300,000 copies per issue, with an estimate of 3-5 readers per copy (giving it a total readership of at least 1 million) making it the world's most popular science fiction periodical.[73]
",Movie
"In 1984  William Gibson’s first novel Neuromancer helped popularize cyberpunk, and the word ""cyberspace"" --  a term he coined in his 1982 short story Burning Chrome.[74][75][76]  In 1986 Shards of Honor by Lois McMaster Bujold began her Vorkosigan Saga.[77][78] 1992’s Snow Crash by Neal Stephenson predicted immense social upheaval due to the information revolution.[79] In 2007 Liu Cixin's novel, The Three-Body Problem, was published in China. It was translated into English by Ken Liu and published by Tor Books in 2014, and won the 2015 Hugo Award for Best Novel.[80] Liu was the first Asian writer to win ""Best Novel.""[81]
",Movie
"Emerging themes in late Twentieth and early Twenty-first century science fiction include environmental issues, the implications of the global Internet and the expanding information universe, questions about biotechnology and nanotechnology, as well as a post-Cold War interest in post-scarcity societies.  Recent trends and sub-genres  include steampunk,[82] biopunk,[83][84] and mundane science fiction.[85][86]
",Movie
"The first known science fiction film is 1902's A Trip to the Moon, directed by French filmmaker Georges Méliès.[87] It was profoundly influential on later filmmakers, bringing creativity to the cinematic medium and offering fantasy for pure entertainment, a rare goal in film at the time. In addition, Méliès's innovative editing and special effects techniques were widely imitated and became important elements of the medium.[88] The film also spurred on the development of cinematic science fiction and fantasy by demonstrating that scientific themes worked on the screen and that reality could be transformed by the camera.[87][89]
",Movie
"1927's Metropolis, directed by Fritz Lang, is the first feature-length science fiction film.[90] Though not well received in its time, it is now considered a great and influential film.[91][92][93]
",Movie
"In 1954 Godzilla, directed by Ishirō Honda, began the  kaiju subgenre of science fiction film, which feature large creatures of any form, usually attacking a major city or engaging other monsters in battle.[94][95]
",Movie
"1968's 2001: A Space Odyssey, directed by Stanley Kubrick and based on the work of Arthur C. Clarke, rose above the mostly B-movie offerings up to that time in scope and quality and greatly influenced later science fiction films.[96][97][98][99]  That same year Planet of the Apes, directed by Franklin J. Schaffner and  based on the 1963 French novel La Planète des Singes by Pierre Boulle, was also popular and critically acclaimed for its vivid depiction of a post-apocalyptic world in which intelligent apes dominate humans.[100]
",Movie
"In 1977 George Lucas began the Star Wars film series with the film now identified as ""Star Wars: Episode IV – A New Hope"". The series went on to become a worldwide popular culture phenomenon,[101] and the third highest-grossing film series.[102]   From the 1980s science fiction films along with fantasy, horror, and superhero films have dominated Hollywood's big-budget productions.[103]  Science fiction films often ""crossover"" with other genres including animation (WALL-E), gangster (Sky Racket), Western (Serenity), comedy (Spaceballs), war (Enemy Mine), sports (Rollerball), mystery (Minority Report), film noir (Blade Runner), and romantic comedy (Eternal Sunshine of the Spotless Mind).[104] Science fiction action films feature science fiction elements weaved into action film premises.[105]
",Movie
"Science fiction and television have always had a close relationship.  Television or television-like technologies frequently appeared in science fiction long before television itself became widely available in the late 1940s and early 1950s; perhaps most famously in George Orwell’s Nineteen Eighty-Four.[106]  The first known science fiction television program was produced by the BBC's pre-war BBC Television service. On 11 February 1938 a thirty-five-minute adapted extract of the play RUR, written by the Czech playwright Karel Čapek, was broadcast live from the BBC's Alexandra Palace studios.[107]  The first popular science fiction program on American television was the children's adventure serial Captain Video and His Video Rangers, which ran from June 1949 to April 1955.[108]
",Movie
"The Twilight Zone, produced and narrated by Rod Serling, who also wrote or co-wrote most of the episodes, ran from 1959 to 1964.  It featured fantasy and horror as well as science fiction, with each episode being a complete story.[109][110] Critics have ranked it as one of the best TV programs of any genre.[111][112] The Jetsons, while intended as comedy and only running for one season (1962-1963), predicted many inventions now in common use: flatscreen television, newspapers on a computer-like screen, computer viruses, video chat,  tanning beds, home treadmills and more.[113]
",Movie
"In 1963 the time travel themed Doctor Who premiered on BBC Television. The original series ran until 1989 and was revived in 2005.  It has been extremely popular worldwide and has greatly influenced later TV science fiction programs, as well as popular culture.[114][115] Star Trek, produced by Gene Roddenberry, premiered in 1966 on NBC Television and ran through the 1969 season. It combined elements of space opera and space Western. Although only mildly successful it gained popularity through later syndication and eventually spawned a very popular and influential franchise through films, later programs, and novels; as well as by intense fan interest.[116][117][118]  Other programs in the 1960s included The Prisoner,[119] The Outer Limits,[120] and Lost in Space.[121][122]
",Movie
"In 1987 Star Trek: The Next Generation began a torrent of new shows, including three further Star Trek continuation shows (Deep Space 9, Voyager and Enterprise) and Babylon 5.[123] Red Dwarf, a comic science fiction series aired on BBC Two between 1988 and 1999, and on Dave since 2009, gaining a cult following.[124] To date, eleven full series of the show plus one ""special"" miniseries have aired. The latest series, dubbed Red Dwarf XII, started airing in October 2017.[125] The X-Files, which featured UFOs and conspiracy theories, was created by Chris Carter and broadcast by Fox Broadcasting Company from 1993 to 2002.[126][127] Stargate, a film about ancient astronauts and interstellar teleportation, was released in 1994. Stargate SG-1 premiered in 1997 and ran  for 10 seasons. Spin-off series included Stargate Infinity, Stargate Atlantis, and Stargate Universe.[128]
",Movie
"One of the great benefits of science fiction is that it can convey bits and pieces, hints and phrases, of knowledge unknown or inaccessible to the reader ... works you ponder over as the water is running out of the bathtub or as you walk through the woods in an early winter snowfall.",Movie
"Science fiction's great rise in popularity during the first half of the twentieth century was closely tied to the respect paid to science at that time, as well as the rapid pace of technological innovation and new inventions.[130]  Science fiction has almost always predicted scientific and technological progress.  Some works predict this leading to improvements in life and society, for instance the stories of Arthur C. Clarke and the Star Trek series.  While others warn about possible negative consequences, for instance H.G. Wells' The Time Machine and Aldous Huxley’s Brave New World.[131]
",Movie
"Brian Aldiss described science fiction as ""cultural wallpaper.""[132] Carl Sagan wrote that ""Many scientists deeply involved in the exploration of the solar system (myself among them) were first turned in that direction by science fiction. And the fact that some of that science fiction was not of the highest quality is irrelevant. Ten year‐olds do not read the scientific literature"".[129] Evidence for this widespread influence can be found in a trend for academic researchers to employ science fiction as a tool for advocacy, generating cultural insights, and assisting teaching and learning across a range of academic disciplines not limited to the natural sciences.[133] The National Science Foundation conducted surveys of ""Public Attitudes and Public Understanding"" of ""Science Fiction and Pseudoscience.""[134] They write that ""Interest in science fiction may affect the way people think about or relate to science....one study found a strong relationship between preference for science fiction novels and support for the space program...The same study also found that students who read science fiction are much more likely than other students to believe that contacting extraterrestrial civilizations is both possible and desirable.""[135]
",Movie
"Science fiction is often said to generate a ""sense of wonder.""  Science fiction editor and critic David Hartwell writes: ""Science fiction’s appeal lies in combination of the rational, the believable, with the miraculous. It is an appeal to the sense of wonder.""[136] Isaac Asimov in 1967 commenting on the changes then occurring in SF wrote:  ""And because today’s real life so resembles day-before-yesterday’s fantasy, the old-time fans are restless. Deep within, whether they admit it or not, is a feeling of disappointment and even outrage that the outer world has invaded their private domain. They feel the loss of a 'sense of wonder' because what was once truly confined to 'wonder' has now become prosaic and mundane.""[137]
",Movie
"Science fiction has sometimes been used as a means of social protest. James Cameron’s film Avatar was intended as a protest against imperialism, and specifically against the European colonization of the Americas.[138]  Its images were used by, among others, Palestinians in their protest against Israel.[139]
",Movie
"Robots, artificial humans, human clones, intelligent computers, and their possible conflicts with humans has been a major theme of science fiction since the publication of Frankenstein.  Some critics have seen this as reflecting authors’ concerns over the social alienation seen in modern society.[140]
",Movie
"Feminist science fiction poses questions about social issues such as how society constructs gender roles, the role reproduction plays in defining gender and the unequal political and personal power of men over women. Some of the most notable feminist science fiction works have illustrated these themes using utopias to explore a society in which gender differences or gender power imbalances do not exist, or dystopias to explore worlds in which gender inequalities are intensified, thus asserting a need for feminist work to continue.[141]
",Movie
"Libertarian science fiction focuses on the politics and the social order implied by right libertarian philosophies with an emphasis on individualism and private property, and in some cases anti-statism.[142]
",Movie
"Climate fiction, or ""cli-fi"" deals with issues concerning climate change and global warming.[143][144]  University courses on literature and environmental issues may include climate change fiction in their syllabi,[145] as well as it being discussed by the media, outside of SF fandom.[146]
",Movie
"Comic science fiction often satirizes and criticizes present-day society, as well as sometimes making fun of the conventions and cliches of serious science fiction.[147][148]
",Movie
"The study of science fiction, or science fiction studies, is the critical assessment, interpretation, and discussion of science fiction literature, film, new media, fandom, and fan fiction. Science fiction scholars study science fiction to better understand it and its relationship to science, technology, politics, and culture-at-large. Science fiction studies has a long history, dating back to the turn of the 20th century, but it was not until later that science fiction studies solidified as a discipline with the publication of the academic journals Extrapolation (1959), Foundation: The International Review of Science Fiction (1972), and Science Fiction Studies (1973), and the establishment of the oldest organizations devoted to the study of science fiction, the Science Fiction Research Association and the Science Fiction Foundation, in 1970. The field has grown considerably since the 1970s with the establishment of more journals, organizations, and conferences with ties to the science fiction scholarship community, and science fiction degree-granting programs such as those offered by the University of Liverpool and Kansas University.
",Movie
"Scholar and science fiction critic George Edgar Slusser said that science fiction ""is the one real international literary form we have today, and as such has branched out to visual media, interactive media and on to whatever new media the world will invent in the 21st century... crossover issues between the sciences and the humanities are crucial for the century to come.""[149]
",Movie
"Science Fiction has historically been sub-divided between hard science fiction and soft science fiction - with the division centering on the feasibility of the science central to the story.[150] However, this distinction has come under increasing scrutiny in the 21st century. Authors including Tade Thompson and Jeff VanderMeer have pointed out that stories that focus explicitly on physics, astronomy, mathematics, and engineering tend to be considered ""hard"", while stories that focus on botany, mycology, zoology or the social sciences tend to be categorized as, ""soft,"" regardless of the relative rigor of the science.[151]
",Movie
"Max Gladstone defined hard SF as being, ""SF where the math works,"" but pointed out that this ends up with stories that seem, ""weirdly dated,"" as scientific paradigms shift over time. Aliette de Bodard argued that there was a risk in the categorization that authors' work would be dismissed as not being, ""proper,"" SF. Michael Swanwick dismissed the traditional definition of hard SF altogether, instead saying that it was defined by characters striving to solve problems, ""in the right way - with determination, a touch of stoicism, and the consciousness that the universe is not on his or her side.""[151]
",Movie
"Ursula K. Leguin took a more traditional view on the difference between ""hard"" and ""soft"" SF but arrived at a divergent value-judgment from the one implied by de Bodard, saying, ""The “hard” science fiction writers dismiss everything except, well, physics, astronomy, and maybe chemistry. Biology, sociology, anthropology—that's not science to them, that's soft stuff. They're not that interested in what human beings do, really. But I am. I draw on the social sciences a great deal."" [152]
",Movie
"Respected authors of main-stream literature have written science fiction. Mary Shelley wrote a number of science fiction novels including Frankenstein, and is considered a major writer of the Romantic Age.[154] Aldous Huxley’s  Brave New World (1932) is often listed as one of England’s most important novels, both for its criticism of modern culture and its prediction of future trends including reproductive technology and social engineering.[155][156][157][158] Doris Lessing, who was later awarded the Nobel Prize in literature, wrote a series of SF novels, Canopus in Argos, which depict the efforts of more advanced species and civilizations to influence those less advanced including humans on Earth.[159][160][161][162] Kurt Vonnegut was a highly respected American author whose works contain science fiction premises or themes.[163][164][165]  Science fiction authors whose works are considered to be serious literature include Ray Bradbury,[166] Arthur C. Clarke (especially for Childhood's End),[167][168] and Paul Myron Anthony Linebarger, writing under the name Cordwainer Smith.[169]
",Movie
"In her much reprinted essay ""Science Fiction and Mrs Brown,""[170] Ursula K. Le Guin first asks: ""Can a science fiction writer write a novel?""; and answers: ""I believe that all novels, ... deal with character, and that it is to express character – not to preach doctrines, sing songs, or celebrate the glories of the British Empire, that the form of the novel, so clumsy, verbose, and undramatic, so rich, elastic, and alive, has been evolved ... The great novelists have brought us to see whatever they wish us to see through some character. Otherwise they would not be novelists, but poets, historians, or pamphleteers.""
",Movie
"Tom Shippey asks: ""What is its relationship to fantasy fiction, is its readership still dominated by male adolescents, is it a taste which will appeal to the mature but non-eccentric literary mind?""[171] He compares George Orwell's Coming Up for Air with Frederik Pohl and C. M. Kornbluth's The Space Merchants and concludes that the basic building block and distinguishing feature of a science fiction novel is the presence of the novum, a term Darko Suvin adapts from Ernst Bloch and defines as ""a discrete piece of information recognizable as not-true, but also as not-unlike-true, not-flatly- (and in the current state of knowledge) impossible.""[172]
",Movie
"Orson Scott Card, best known for his 1985 science fiction novel Ender's Game and also an author of non-SF fiction, has postulated that in science fiction the message and intellectual significance of the work is contained within the story itself and, therefore, there need not be stylistic gimmicks or literary games; but that some writers and critics confuse clarity of language with lack of artistic merit. In Card's words: ""...a great many writers and critics have based their entire careers on the premise that anything that the general public can understand without mediation is worthless drivel. [...] If everybody came to agree that stories should be told this clearly, the professors of literature would be out of a job, and the writers of obscure, encoded fiction would be, not honored, but pitied for their impenetrability.""[173]
",Movie
"Science fiction author and physicist Gregory Benford has declared that: ""SF is perhaps the defining genre of the twentieth century, although its conquering armies are still camped outside the Rome of the literary citadels.""[174] Jonathan Lethem in an essay published in the Village Voice entitled ""Close Encounters: The Squandered Promise of Science Fiction"" suggests that the point in 1973 when Thomas Pynchon's Gravity's Rainbow was nominated for the Nebula Award and was passed over in favor of Arthur C. Clarke's Rendezvous with Rama stands as ""a hidden tombstone marking the death of the hope that SF was about to merge with the mainstream.""[175] Among the responses to Lethem was one from the editor of the Magazine of Fantasy and Science Fiction who asked: ""When is it [the SF genre] ever going to realize it can't win the game of trying to impress the mainstream?""[176]
",Movie
"David Barnett has remarked:[177] ""The ongoing, endless war between ""literary"" fiction and ""genre"" fiction has well-defined lines in the sand. Genre's foot soldiers think that literary fiction is a collection of meaningless but prettily drawn pictures of the human condition. The literary guard consider genre fiction to be crass, commercial, whizz-bang potboilers. Or so it goes.""  He has also pointed out that there are books such as The Road by Cormac McCarthy, Cloud Atlas by David Mitchell, The Gone-Away World by Nick Harkaway, The Stone Gods by Jeanette Winterson and Oryx and Crake by Margaret Atwood, which use recognizable science fiction tropes, but whose authors and publishers do not market them as science fiction.[178]
",Movie
"Science fiction is being written worldwide by a diverse population of authors. According to 2013 statistics by the science fiction publisher Tor Books, men outnumber women by 78% to 22% among submissions to the publisher.[179] A controversy about voting slates in the 2015 Hugo Awards highlighted tensions in the science fiction community between a trend of increasingly diverse works and authors being honored by awards, and a backlash by groups of authors and fans who preferred what they considered more traditional science fiction.[180]
",Movie
"Among the most respected awards for science fiction are the Hugo Award, presented by the World Science Fiction Society at Worldcon; the Nebula Award, presented by the SFWA and voted on by the community of authors; and the John W. Campbell Memorial Award for Best Science Fiction Novel and Theodore Sturgeon Memorial Award for short fiction. One notable award for science fiction films is the Saturn Award. It is presented annually by The Academy of Science Fiction, Fantasy, and Horror Films.
",Movie
"There are national awards, like Canada's Prix Aurora Awards, regional awards, like the Endeavour Award presented at Orycon for works from the Pacific Northwest, special interest or subgenre awards like the Chesley Award for art or the World Fantasy Award for fantasy. Magazines may organize reader polls, notably the Locus Award.
",Movie
"Conventions (in fandom, shortened as ""cons""), are held in cities around the world, catering to a local, regional, national, or international membership. General-interest conventions cover all aspects of science fiction, while others focus on a particular interest like media fandom, filking, etc. Most are organized by volunteers in non-profit groups, though most media-oriented events are organized by commercial promoters. The convention's activities are called the program, which may include panel discussions, readings, autograph sessions, costume masquerades, and other events. Activities occur throughout the convention that are not part of the program. These commonly include a dealer's room, art show, and hospitality lounge (or ""con suites"").[181]
",Movie
"Conventions may host award ceremonies; Worldcons present the Hugo Awards each year. SF societies, referred to as ""clubs"" except in formal contexts, form a year-round base of activities for science fiction fans. They may be associated with an ongoing science fiction convention, or have regular club meetings, or both. Most groups meet in libraries, schools and universities, community centers, pubs or restaurants, or the homes of individual members. Long-established groups like the New England Science Fiction Association and the Los Angeles Science Fantasy Society have clubhouses for meetings and storage of convention supplies and research materials.[182] The Science Fiction and Fantasy Writers of America (SFWA) was founded by Damon Knight in 1965 as a non-profit organization to serve the community of professional science fiction authors,[183]
",Movie
"Science fiction fandom is the ""community of the literature of ideas... the culture in which new ideas emerge and grow before being released into society at large.""[2] Members of this community, ""fans"", are in contact with each other at conventions or clubs, through print or online fanzines, or on the Internet using web sites, mailing lists, and other resources.  SF fandom emerged from the letters column in Amazing Stories magazine. Soon fans began writing letters to each other, and then grouping their comments together in informal publications that became known as fanzines.[184] Once they were in regular contact, fans wanted to meet each other, and they organized local clubs. In the 1930s, the first science fiction conventions gathered fans from a wider area.[185]
",Movie
"The first science fiction fanzine, The Comet, was published in 1930.[186] Fanzine printing methods have changed over the decades, from the hectograph, the mimeograph, and the ditto machine, to modern photocopying. Distribution volumes rarely justify the cost of commercial printing. Modern fanzines are printed on computer printers or at local copy shops, or they may only be sent as email. The best known fanzine (or ""'zine"") today is Ansible, edited by David Langford, winner of numerous Hugo awards. Other fanzines to win awards in recent years include File 770, Mimosa, and Plokta.[187] Artists working for fanzines have risen to prominence in the field, including Brad W. Foster, Teddy Harvia, and Joe Mayhew; the Hugos include a category for Best Fan Artists.[187] The earliest organized fandom online was the SF Lovers community, originally a mailing list in the late 1970s with a text archive file that was updated regularly.[188] In the 1980s, Usenet groups greatly expanded the circle of fans online. In the 1990s, the development of the World-Wide Web exploded the community of online fandom by orders of magnitude, with thousands and then literally millions of web sites devoted to science fiction and related genres for all media.[182] Most such sites are small, ephemeral, and/or very narrowly focused, though sites like SF Site and SFcrowsnest offer a broad range of references and reviews about science fiction.
",Movie
"Science fiction elements can include:
",Movie
"Epic films are a style of filmmaking with large scale, sweeping scope, and spectacle. The usage of the term has shifted over time, sometimes designating a film genre and at other times simply synonymous with big budget filmmaking. Like epics in the classical literary sense it is often focused on a heroic character. An epic's ambitious nature helps to set it apart from other types of film such as the period piece or adventure film.
",Movie
"Epic historical films would usually take a historical or a mythical event and add an extravagant setting and lavish costumes, accompanied by an expansive musical score with an ensemble cast, which would make them among the most expensive of films to produce. The most common subjects of epic films are royalty,  and important figures from various periods in world history.[1]
",Movie
"The term ""epic"" originally came from the poetic genre exemplified by such works as the Iliad, Epic of Gilgamesh, or the Odyssey. In classical literature, epics are considered works focused on deeds or journeys of heroes upon which the fate of a large number of people depend. Similarly, films described as ""epic"" typically take a historical character, or a mythic heroic figure. Common subjects of epics are royalty, gladiators, great military leaders, or leading personalities from various periods in world history. However, there are some films described as ""epic"" almost solely on the basis of their enormous scope and the sweeping panorama of their settings such as How the West Was Won or East of Eden that do not have the typical substance of classical epics but are directed in an epic style.
",Movie
"When described as ""epic"" because of content, an epic movie is often set during a time of war or other societal crisis, while usually covering a longer span of time sometimes throughout entire generations coming and passing away, in terms of both the events depicted and the running time of the film. Such films usually have a historical setting, although fantasy or science fiction settings have become common in recent decades. The central conflict of the film is usually seen as having far-reaching effects, often changing the course of history. The main characters' actions are often central to the resolution of the societal conflict.
",Movie
"In its classification of films by genre, the American Film Institute limits the genre to historical films such as Ben-Hur. However, film scholars such as Constantine Santas are willing to extend the label to science-fiction films such as 2001: A Space Odyssey and Star Wars.[2] Lynn Ramey suggests that ""Surely one of the hardest film genres to define is that of the ""epic"" film, encompassing such examples as Ben-Hur, Gone with the Wind....and more recently, 300 and the Star Wars films...none of these comes from literary epics per se, and there is little that links them with one another. Among those who espouse film genre studies, epic is one of the most despised and ignored genres""[3] Finally, although the American Movie Channel formally defines epic films as historical films, they nonetheless state the epic film may be combined with the genre of science-fiction and cite Star Wars as an example.[4]
",Movie
"Stylistically, films classed as epic usually employ spectacular settings and specially designed costumes, often accompanied by a sweeping musical score, and an ensemble cast of bankable stars. Epics are usually among the most expensive of films to produce. They often use on-location filming, authentic period costumes, and action scenes on a massive scale. Biographical films may be less lavish versions of this genre.
",Movie
"Many writers may refer to any film that is ""long"" (over two hours) as an epic, making the definition epic a matter of dispute, and raise questions as to whether it is a ""genre"" at all. As Roger Ebert put it, in his ""Great Movies"" article on Lawrence of Arabia:[5]
",Movie
"The word epic in recent years has become synonymous with big budget B picture. What you realize watching Lawrence of Arabia is that the word epic refers not to the cost or the elaborate production, but to the size of the ideas and vision. Werner Herzog's Aguirre: The Wrath of God didn't cost as much as the catering in Pearl Harbor, but it is an epic, and Pearl Harbor is not.
",Movie
"The comedy film Monty Python and the Holy Grail had the joking tagline ""Makes Ben-Hur look like an epic.""
",Movie
"The epic is among the oldest of film genres, with one early notable example being Giovanni Pastrone's Cabiria, a three-hour silent film, about the Punic Wars, that laid the groundwork for the subsequent silent epics of D. W. Griffith.
",Movie
"The genre reached a peak of popularity in the early 1960s,[6] when  Hollywood frequently collaborated with foreign film studios (such as Rome's Cinecittà) to use relatively exotic locations in Spain, Morocco, and elsewhere for the production of epic films such as El Cid (1961) or Lawrence of Arabia (1962). This boom period of international co-productions is generally considered to have ended with Cleopatra (1963), The Fall of the Roman Empire (1964), and Doctor Zhivago (1965). Nevertheless, films in this genre continued to appear, with one notable example being War and Peace, which was released in the former Soviet Union during 1967-1968 and, directed by Sergei Bondarchuk, and said to be the most expensive film ever made.
",Movie
"Epic films continue to be produced, although since the development of CGI they typically use computer effects instead of an actual cast of thousands. Since the 1950s, such films have regularly been shot with a wide aspect ratio for a more immersive and panoramic theatrical experience.
",Movie
"Epic films were recognized in a montage at the 2006 Academy Awards.
",Movie
"The enduring popularity of the epic is often accredited to their ability to appeal to a wide audience. Many of the highest-grossing films of all-time have been epics.[7] The 1997 film Titanic, which is cited as helping to revive the genre, grossed $658 million domestically and over $2.1 billion worldwide, making it the second highest-grossing film of all-time behind the 2009 film Avatar,[8] another epic which grossed $2.7 billion worldwide. If inflation is taken into account, then the historical epic Gone with the Wind becomes the highest-grossing film ever in the United States.[9] Adjusted for inflation it earned the equivalent of $1.6 billion in the United States alone.[7] Adjusted for ticket price inflation, the science fiction/fantasy epic Star Wars stands at number 2, with an inflated gross of $1.4 billion in the United States.[7]
",Movie
"So far the most Academy Awards ever won by a single film stands at 11. This feat has only been achieved by 3 movies (Ben-Hur, Titanic and The Lord of the Rings: The Return of the King) all of which are considered epics.
",Movie
"
",Technology
"Technology (""science of craft"", from Greek τέχνη, techne, ""art, skill, cunning of hand""; and -λογία, -logia[2]) is the collection of techniques, skills, methods, and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation. Technology can be the knowledge of techniques, processes, and the like, or it can be embedded in machines to allow for operation without detailed knowledge of their workings.
",Technology
"The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times, including the printing press, the telephone, and the Internet, have lessened physical barriers to communication and allowed humans to interact freely on a global scale.
",Technology
"Technology has many effects. It has helped develop more advanced economies (including today's global economy) and has allowed the rise of a leisure class. Many technological processes produce unwanted by-products known as pollution and deplete natural resources to the detriment of Earth's environment. Innovations have always influenced the values of a society and raised new questions of the ethics of technology.  Examples include the rise of the notion of efficiency in terms of human productivity, and the challenges of bioethics.
",Technology
"Philosophical debates have arisen over the use of technology, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticize the pervasiveness of technology, arguing that it harms the environment and alienates people; proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition.
",Technology
"The use of the term ""technology"" has changed significantly over the last 200 years. Before the 20th century, the term was uncommon in English, and it was used either to refer to the description or study of the useful arts[3] or to allude to technical education, as in the Massachusetts Institute of Technology (chartered in 1861).[4]
",Technology
"The term ""technology"" rose to prominence in the 20th century in connection with the Second Industrial Revolution. The term's meanings changed in the early 20th century when American social scientists, beginning with Thorstein Veblen, translated ideas from the German concept of Technik into ""technology."" In German and other European languages, a distinction exists between technik and technologie that is absent in English, which usually translates both terms as ""technology."" By the 1930s, ""technology"" referred not only to the study of the industrial arts but to the industrial arts themselves.[5]
",Technology
"In 1937, the American sociologist Read Bain wrote that ""technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them.""[6] Bain's definition remains common among scholars today, especially social scientists.  Scientists and engineers usually prefer to define technology as applied science, rather than as the things that people make and use.[7] More recently, scholars have borrowed from European philosophers of ""technique"" to extend the meaning of technology to various forms of instrumental reason, as in Foucault's work on technologies of the self (techniques de soi).
",Technology
"Dictionaries and scholars have offered a variety of definitions. The Merriam-Webster Learner's Dictionary offers a definition of the term: ""the use of science in industry, engineering, etc., to invent useful things or to solve problems"" and ""a machine, piece of equipment, method, etc., that is created by technology.""[8] Ursula Franklin, in her 1989 ""Real World of Technology"" lecture, gave another definition of the concept; it is ""practice, the way we do things around here.""[9] The term is often used to imply a specific field of technology, or to refer to high technology or just consumer electronics, rather than technology as a whole.[10] Bernard Stiegler, in Technics and Time, 1, defines technology in two ways: as ""the pursuit of life by means other than life,"" and as ""organized inorganic matter.""[11]
",Technology
"Technology can be most broadly defined as the entities, both material and immaterial, created by the application of mental and physical effort in order to achieve some value. In this usage, technology refers to tools and machines that may be used to solve real-world problems. It is a far-reaching term that may include simple tools, such as a crowbar or wooden spoon, or more complex machines, such as a space station or particle accelerator. Tools and machines need not be material; virtual technology, such as computer software and business methods, fall under this definition of technology.[12] W. Brian Arthur defines technology in a similarly broad way as ""a means to fulfill a human purpose.""[13]
",Technology
"The word ""technology"" can also be used to refer to a collection of techniques. In this context, it is the current state of humanity's knowledge of how to combine resources to produce desired products, to solve problems, fulfill needs, or satisfy wants; it includes technical methods, skills, processes, techniques, tools and raw materials. When combined with another term, such as ""medical technology"" or ""space technology,"" it refers to the state of the respective field's knowledge and tools. ""State-of-the-art technology"" refers to the high technology available to humanity in any field.
",Technology
"Technology can be viewed as an activity that forms or changes culture.[14] Additionally, technology is the application of math, science, and the arts for the benefit of life as it is known. A modern example is the rise of communication technology, which has lessened barriers to human interaction and as a result has helped spawn new subcultures; the rise of cyberculture has at its basis the development of the Internet and the computer.[15] Not all technology enhances culture in a creative way; technology can also help facilitate political oppression and war via tools such as guns. As a cultural activity, technology predates both science and engineering, each of which formalize some aspects of technological endeavor.
",Technology
"The distinction between science, engineering, and technology is not always clear. Science is systematic knowledge of the physical or material world gained through observation and experimentation.[16] Technologies are not usually exclusively products of science, because they have to satisfy requirements such as utility, usability, and safety.[citation needed]
",Technology
"Engineering is the goal-oriented process of designing and making tools and systems to exploit natural phenomena for practical human means, often (but not always) using results and techniques from science. The development of technology may draw upon many fields of knowledge, including scientific, engineering, mathematical, linguistic, and historical knowledge, to achieve some practical result.
",Technology
"Technology is often a consequence of science and engineering, although technology as a human activity precedes the two fields. For example, science might study the flow of electrons in electrical conductors by using already-existing tools and knowledge. This new-found knowledge may then be used by engineers to create new tools and machines such as semiconductors, computers, and other forms of advanced technology. In this sense, scientists and engineers may both be considered technologists; the three fields are often considered as one for the purposes of research and reference.[17]
",Technology
"The exact relations between science and technology in particular have been debated by scientists, historians, and policymakers in the late 20th century, in part because the debate can inform the funding of basic and applied science. In the immediate wake of World War II, for example, it was widely considered in the United States that technology was simply ""applied science"" and that to fund basic science was to reap technological results in due time. An articulation of this philosophy could be found explicitly in Vannevar Bush's treatise on postwar science policy, Science – The Endless Frontier: ""New products, new industries, and more jobs require continuous additions to knowledge of the laws of nature ... This essential new knowledge can be obtained only through basic scientific research.""[18] In the late-1960s, however, this view came under direct attack, leading towards initiatives to fund science for specific tasks (initiatives resisted by the scientific community). The issue remains contentious, though most analysts resist the model that technology simply is a result of scientific research.[19][20]
",Technology
"The use of tools by early humans was partly a process of discovery and of evolution. Early humans evolved from a species of foraging hominids which were already bipedal,[21] with a brain mass approximately one third of modern humans.[22] Tool use remained relatively unchanged for most of early human history. Approximately 50,000 years ago, the use of tools and complex set of behaviors emerged, believed by many archaeologists to be connected to the emergence of fully modern language.[23]
",Technology
"Hominids started using primitive stone tools millions of years ago. The earliest stone tools were little more than a fractured rock, but approximately 75,000 years ago,[24] pressure flaking provided a way to make much finer work.
",Technology
"The discovery and utilization of fire, a simple energy source with many profound uses, was a turning point in the technological evolution of humankind.[25] The exact date of its discovery is not known; evidence of burnt animal bones at the Cradle of Humankind suggests that the domestication of fire occurred before 1 Ma;[26] scholarly consensus indicates that Homo erectus had controlled fire by between 500 and 400 ka.[27][28] Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.[29]
",Technology
"Other technological advances made during the Paleolithic era were clothing and shelter; the adoption of both technologies cannot be dated exactly, but they were a key to humanity's progress. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 ka, humans were constructing temporary wood huts.[30][31] Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate
out of Africa by 200 ka and into other continents such as Eurasia.[32]
",Technology
"Human's technological ascent began in earnest in what is known as the Neolithic Period (""New Stone Age""). The invention of polished stone axes was a major advance that allowed forest clearance on a large scale to create farms. This use of polished stone axes increased greatly in the Neolithic, but were originally used in the preceding Mesolithic in some areas such as Ireland.[33] Agriculture fed larger populations, and the transition to sedentism allowed simultaneously raising more children, as infants no longer needed to be carried, as nomadic ones must. Additionally, children could contribute labor to the raising of crops more readily than they could to the hunter-gatherer economy.[34][35]
",Technology
"With this increase in population and availability of labor came an increase in labor specialization.[36] What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war amongst adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.[37]
",Technology
"Continuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge of gold, copper, silver, and lead  – native metals found in relatively pure form in nature.[38] The advantages of copper tools over stone, bone, and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 ka).[39] Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4000 BCE). The first uses of iron alloys such as steel dates to around 1800 BCE.[40][41]
",Technology
"Meanwhile, humans were learning to harness other forms of energy. The earliest known use of wind power is the sailing ship; the earliest record of a ship under sail is that of a Nile boat dating to the 8th millennium BCE.[42] From prehistoric times, Egyptians probably used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and ""catch"" basins. The ancient Sumerians in Mesopotamia used a complex system of canals and levees to divert water from the Tigris and Euphrates rivers for irrigation.[43]
",Technology
"According to archaeologists, the wheel was invented around 4000 BCE probably independently and nearly simultaneously in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture) and Central Europe.[44] Estimates on when this may have occurred range from 5500 to 3000 BCE with most experts putting it closer to 4000 BCE.[45] The oldest artifacts with drawings depicting wheeled carts date from about 3500 BCE;[46] however, the wheel may have been in use for millennia before these drawings were made. More recently, the oldest-known wooden wheel in the world was found in the Ljubljana marshes of Slovenia.[47]
",Technology
"The invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. The ancient Sumerians used the potter's wheel and may have invented it.[48] A stone pottery wheel found in the city-state of Ur dates to around 3429 BCE,[49] and even older fragments of wheel-thrown pottery have been found in the same area.[49] Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois[50] and were first used in Mesopotamia and Iran in around 3000 BCE.[50]
",Technology
"The oldest known constructed roadways are the stone-paved streets of the city-state of Ur, dating to circa 4000 BCE[51] and timber roads leading through the swamps of Glastonbury, England, dating to around the same time period.[51] The first long-distance road, which came into use around 3500 BCE,[51] spanned 1,500 miles from the Persian Gulf to the Mediterranean Sea,[51] but was not paved and was only partially maintained.[51] In around 2000 BCE, the Minoans on the Greek island of Crete built a fifty-kilometer (thirty-mile) road leading from the palace of Gortyn on the south side of the island, through the mountains, to the palace of Knossos on the north side of the island.[51] Unlike the earlier road, the Minoan road was completely paved.[51]
",Technology
"Ancient Minoan private homes had running water.[53] A bathtub virtually identical to modern ones was unearthed at the Palace of Knossos.[53][54] Several Minoan private homes also had toilets, which could be flushed by pouring water down the drain.[53] The ancient Romans had many public flush toilets,[54] which emptied into an extensive sewage system.[54] The primary sewer in Rome was the Cloaca Maxima;[54] construction began on it in the sixth century BCE and it is still in use today.[54]
",Technology
"The ancient Romans also had a complex system of aqueducts,[52] which were used to transport water across long distances.[52] The first Roman aqueduct was built in 312 BCE.[52] The eleventh and final ancient Roman aqueduct was built in 226 CE.[52] Put together, the Roman aqueducts extended over 450 kilometers,[52] but less than seventy kilometers of this was above ground and supported by arches.[52]
",Technology
"Innovations continued through the Middle Ages with innovations such as silk, the horse collar and horseshoes in the first few hundred years after the fall of the Roman Empire. Medieval technology saw the use of simple machines (such as the lever, the screw, and the pulley) being combined to form more complicated tools, such as the wheelbarrow, windmills and clocks. The Renaissance brought forth many of these innovations, including the printing press (which facilitated the greater communication of knowledge), and technology became increasingly associated with science, beginning a cycle of mutual advancement. The advancements in technology in this era allowed a more steady supply of food, followed by the wider availability of consumer goods.
",Technology
"Starting in the United Kingdom in the 18th century, the Industrial Revolution was a period of great technological discovery, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, driven by the discovery of steam power. Technology took another step in a second industrial revolution with the harnessing of electricity to create such innovations as the electric motor, light bulb, and countless others. Scientific advancement and the discovery of new concepts later allowed for powered flight and advancements in medicine, chemistry, physics, and engineering. The rise in technology has led to skyscrapers and broad urban areas whose inhabitants rely on motors to transport them and their food supply. Communication was also greatly improved with the invention of the telegraph, telephone, radio and television. The late 19th and early 20th centuries saw a revolution in transportation with the invention of the airplane and automobile.
",Technology
"The 20th century brought a host of innovations. In physics, the discovery of nuclear fission has led to both nuclear weapons and nuclear power. Computers were also invented and later miniaturized utilizing transistors and integrated circuits. Information technology subsequently led to the creation of the Internet, which ushered in the current Information Age. Humans have also been able to explore space with satellites (later used for telecommunication) and in manned missions going all the way to the moon. In medicine, this era brought innovations such as open-heart surgery and later stem cell therapy along with new medications and treatments.
",Technology
"Complex manufacturing and construction techniques and organizations are needed to make and maintain these new technologies, and entire industries have arisen to support and develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have been created to support them, including engineering, medicine, and computer science, and other fields have been made more complex, such as construction, transportation, and architecture.
",Technology
"Generally, technicism is the belief in the utility of technology for improving human societies.[55] Taken to an extreme, technicism ""reflects a fundamental attitude which seeks to control reality, to resolve all problems with the use of scientific–technological methods and tools.""[56] In other words, human beings will someday be able to master all problems and possibly even control the future using technology. Some, such as Stephen V. Monsma,[57] connect these ideas to the abdication of religion as a higher moral authority.
",Technology
"Optimistic assumptions are made by proponents of ideologies such as transhumanism and singularitarianism, which view technological development as generally having beneficial effects for the society and the human condition. In these ideologies, technological development is morally good.
",Technology
"Transhumanists generally believe that the point of technology is to overcome barriers, and that what we commonly refer to as the human condition is just another barrier to be surpassed.
",Technology
"Singularitarians believe in some sort of ""accelerating change""; that the rate of technological progress accelerates as we obtain more technology, and that this will culminate in a ""Singularity"" after artificial general intelligence is invented in which progress is nearly infinite; hence the term. Estimates for the date of this Singularity vary,[58] but prominent futurist Ray Kurzweil estimates the Singularity will occur in 2045.
",Technology
"Kurzweil is also known for his history of the universe in six epochs: (1) the physical/chemical epoch, (2) the life epoch, (3) the human/brain epoch, (4) the technology epoch, (5) the artificial intelligence epoch, and (6) the universal colonization epoch. Going from one epoch to the next is a Singularity in its own right, and a period of speeding up precedes it. Each epoch takes a shorter time, which means the whole history of the universe is one giant Singularity event.[59]
",Technology
"Some critics see these ideologies as examples of scientism and techno-utopianism and fear the notion of human enhancement and technological singularity which they support. Some have described Karl Marx as a techno-optimist.[60]
",Technology
"On the somewhat skeptical side are certain philosophers like Herbert Marcuse and John Zerzan, who believe that technological societies are inherently flawed. They suggest that the inevitable result of such a society is to become evermore technological at the cost of freedom and psychological health.
",Technology
"Many, such as the Luddites and prominent philosopher Martin Heidegger, hold serious, although not entirely, deterministic reservations about technology (see ""The Question Concerning Technology""[61]). According to Heidegger scholars Hubert Dreyfus and Charles Spinosa, ""Heidegger does not oppose technology. He hopes to reveal the essence of technology in a way that 'in no way confines us to a stultified compulsion to push on blindly with technology or, what comes to the same thing, to rebel helplessly against it.' Indeed, he promises that 'when we once open ourselves expressly to the essence of technology, we find ourselves unexpectedly taken into a freeing claim.'[62] What this entails is a more complex relationship to technology than either techno-optimists or techno-pessimists tend to allow.""[63]
",Technology
"Some of the most poignant criticisms of technology are found in what are now considered to be dystopian literary classics such as Aldous Huxley's Brave New World, Anthony Burgess's A Clockwork Orange, and George Orwell's Nineteen Eighty-Four. In Goethe's Faust, Faust selling his soul to the devil in return for power over the physical world is also often interpreted as a metaphor for the adoption of industrial technology. More recently, modern works of science fiction such as those by Philip K. Dick and William Gibson and films such as Blade Runner and Ghost in the Shell project highly ambivalent or cautionary attitudes toward technology's impact on human society and identity.
",Technology
"The late cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called ""technopolies,"" societies that are dominated by the ideology of technological and scientific progress to the exclusion or harm of other cultural practices, values, and world-views.[64]
",Technology
"Darin Barney has written about technology's impact on practices of citizenship and democratic culture, suggesting that technology can be construed as (1) an object of political debate, (2) a means or medium of discussion, and (3) a setting for democratic deliberation and citizenship. As a setting for democratic culture, Barney suggests that technology tends to make ethical questions, including the question of what a good life consists in, nearly impossible because they already give an answer to the question: a good life is one that includes the use of more and more technology.[65]
",Technology
"Nikolas Kompridis has also written about the dangers of new technology, such as genetic engineering, nanotechnology, synthetic biology, and robotics. He warns that these technologies introduce unprecedented new challenges to human beings, including the possibility of the permanent alteration of our biological nature. These concerns are shared by other philosophers, scientists and public intellectuals who have written about similar issues (e.g. Francis Fukuyama, Jürgen Habermas, William Joy, and Michael Sandel).[66]
",Technology
"Another prominent critic of technology is Hubert Dreyfus, who has published books such as On the Internet and What Computers Still Can't Do.
",Technology
"A more infamous anti-technological treatise is Industrial Society and Its Future, written by the Unabomber Ted Kaczynski and printed in several major newspapers (and later books) as part of an effort to end his bombing campaign of the techno-industrial infrastructure. There are also subcultures that disapprove of some or most technology, such as self-identified off-gridders.[67]
",Technology
"The notion of appropriate technology was developed in the 20th century by thinkers such as E.F. Schumacher and Jacques Ellul to describe situations where it was not desirable to use very new technologies or those that required access to some centralized infrastructure or parts or skills imported from elsewhere. The ecovillage movement emerged in part due to this concern.[68]
",Technology
"This section mainly focuses on American concerns even if it can reasonably be generalized to other Western countries. 
",Technology
The inadequate quantity and quality of American jobs is one of the most fundamental economic challenges we face. [...] What's the linkage between technology and this fundamental problem?,Technology
"In his article, Jared Bernstein, a Senior Fellow at the Center on Budget and Policy Priorities,[69] questions the widespread idea that automation, and more broadly, technological advances, have mainly contributed to this growing labor market problem.
His thesis appears to be a third way between optimism and skepticism. Essentially, he stands for a neutral approach of the linkage between technology and American issues concerning unemployment and declining wages.
",Technology
"He uses two main arguments to defend his point.
First, because of recent technological advances, an increasing number of workers are losing their jobs. Yet, scientific evidence fails to clearly demonstrate that technology has displaced so many workers that it has created more problems than it has solved. Indeed, automation threatens repetitive jobs but higher-end jobs are still necessary because they complement technology and manual jobs that ""requires flexibility judgment and common sense""[70] remain hard to replace with machines. Second, studies have not shown clear links between recent technology advances and the wage trends of the last decades.
",Technology
"Therefore, according to Bernstein, instead of focusing on technology and its hypothetical influences on current American increasing unemployment and declining wages, one needs to worry more about ""bad policy that fails to offset the imbalances in demand, trade, income, and opportunity.""[70]
",Technology
"For people who use both the Internet and mobile devices in excessive quantities it is likely for them to experience fatigue and over exhaustion as a result of disruptions in their sleeping patterns. Continuous studies have shown that increased BMI and weight gain are associated with people who spend long hours online and not exercising frequently.[71]  Heavy Internet use is also displayed in the school lower grades of those who use it in excessive amounts.[72]  It has also been noted that the use of mobile phones whilst driving has increased the occurrence of road accidents — particularly amongst teen drivers. Statistically, teens reportedly have fourfold the number of road traffic incidents as those who are 20 years or older, and a very high percentage of adolescents write (81%) and read (92%) texts while driving.[73] In this context, mass media and technology have a negative impact on people, on both their mental and physical health.
",Technology
"Thomas P. Hughes stated that because technology has been considered as a key way to solve problems, we need to be aware of its complex and varied characters to use it more efficiently.[74] What is the difference between a wheel or a compass and cooking machines such as an oven or a gas stove? Can we consider all of them, only a part of them, or none of them as technologies?
",Technology
"Technology is often considered too narrowly; according to Hughes, ""Technology is a creative process involving human ingenuity"".[75] This definitio's emphasis on creativity avoids unbounded definitions that may mistakenly include cooking ""technologies,"" but it also highlights the prominent role of humans and therefore their responsibilities for the use of complex technological systems.
",Technology
"Yet, because technology is everywhere and has dramatically changed landscapes and societies, Hughes argues that engineers, scientists, and managers have often believed that they can use technology to shape the world as they want. They have often supposed that technology is easily controllable and this assumption has to be thoroughly questioned.[74] For instance, Evgeny Morozov particularly challenges two concepts: ""Internet-centrism"" and ""solutionism.""[76] Internet-centrism refers to the idea that our society is convinced that the Internet is one of the most stable and coherent forces. Solutionism is the ideology that every social issue can be solved thanks to technology and especially thanks to the internet. In fact, technology intrinsically contains uncertainties and limitations. According to Alexis Madrigal's review of Morozov's theory, to ignore it will lead to ""unexpected consequences that could eventually cause more damage than the problems they seek to address.""[77] Benjamin R. Cohen and Gwen Ottinger also discussed the multivalent effects of technology.[78]
",Technology
"Therefore, recognition of the limitations of technology, and more broadly, scientific knowledge, is needed – especially in cases dealing with environmental justice and health issues. Ottinger continues this reasoning and argues that the ongoing recognition of the limitations of scientific knowledge goes hand in hand with scientists and engineers’ new comprehension of their role. Such an approach of technology and science ""[require] technical professionals to conceive of their roles in the process differently. [They have to consider themselves as] collaborators in research and problem solving rather than simply providers of information and technical solutions.""[79]
",Technology
"Technology is properly defined as any application of science to accomplish a function. The science can be leading edge or well established and the function can have high visibility or be significantly more mundane, but it is all technology, and its exploitation is the foundation of all competitive advantage.
",Technology
"Technology-based planning is what was used to build the US industrial giants before WWII (e.g., Dow, DuPont, GM) and it is what was used to transform the US into a superpower. It was not economic-based planning.
",Technology
"The use of basic technology is also a feature of other animal species apart from humans. These include primates such as chimpanzees,[80] some dolphin communities,[81] and crows.[82][83] Considering a more generic perspective of technology as ethology of active environmental conditioning and control, we can also refer to animal examples such as beavers and their dams, or bees and their honeycombs.
",Technology
"The ability to make and use tools was once considered a defining characteristic of the genus Homo.[84] However, the discovery of tool construction among chimpanzees and related primates has discarded the notion of the use of technology as unique to humans. For example, researchers have observed wild chimpanzees utilising tools for foraging: some of the tools used include leaf sponges, termite fishing probes, pestles and levers.[85] West African chimpanzees also use stone hammers and anvils for cracking nuts,[86] as do capuchin monkeys of Boa Vista, Brazil.[87]
",Technology
"Theories of technology often attempt to predict the future of technology based on the high technology and science of the time. As with all predictions of the future, however, technology's is uncertain.
",Technology
"In 2005, futurist Ray Kurzweil predicted that the future of technology would mainly consist of an overlapping ""GNR Revolution"" of genetics, nanotechnology and robotics, with robotics being the most important of the three.[88]
",Technology
"
",Technology
"Information technology (IT) is the use of computers to store, retrieve, transmit, and manipulate data,[1] or information, often in the context of a business or other enterprise.[2] IT is considered to be a subset of information and communications technology (ICT).
",Technology
"Humans have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC,[3] but the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that ""the new technology does not yet have a single established name. We shall call it information technology (IT)."" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.[4]
",Technology
"The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce.[5][a]
",Technology
"Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present).[3] This article focuses on the most recent period (electronic), which began in about 1940.
",Technology
"Devices have been used to aid computation for thousands of years, probably initially in the form of a tally stick.[7] The Antikythera mechanism, dating from about the beginning of the first century BC, is generally considered to be the earliest known mechanical analog computer, and the earliest known geared mechanism.[8] Comparable geared devices did not emerge in Europe until the 16th century, and it was not until 1645 that the first mechanical calculator capable of performing the four basic arithmetical operations was developed.[9]
",Technology
"Electronic computers, using either relays or valves, began to appear in the early 1940s. The electromechanical Zuse Z3, completed in 1941, was the world's first programmable computer, and by modern standards one of the first machines that could be considered a complete computing machine. Colossus, developed during the Second World War to decrypt German messages, was the first electronic digital computer. Although it was programmable, it was not general-purpose, being designed to perform only a single task. It also lacked the ability to store its program in memory; programming was carried out using plugs and switches to alter the internal wiring.[10] The first recognisably modern electronic digital stored-program computer was the Manchester Baby, which ran its first program on 21 June 1948.[11]
",Technology
"The development of transistors in the late 1940s at Bell Laboratories allowed a new generation of computers to be designed with greatly reduced power consumption. The first commercially available stored-program computer, the Ferranti Mark I, contained 4050 valves and had a power consumption of 25 kilowatts. By comparison the first transistorised computer, developed at the University of Manchester and operational by November 1953, consumed only 150 watts in its final version.[12]
",Technology
"Early electronic computers such as Colossus made use of punched tape, a long strip of paper on which data was represented by a series of holes, a technology now obsolete.[13] Electronic data storage, which is used in modern computers, dates from World War II, when a form of delay line memory was developed to remove the clutter from radar signals, the first practical application of which was the mercury delay line.[14] The first random-access digital storage device was the Williams tube, based on a standard cathode ray tube,[15] but the information stored in it and delay line memory was volatile in that it had to be continuously refreshed, and thus was lost once power was removed. The earliest form of non-volatile computer storage was the magnetic drum, invented in 1932[16] and used in the Ferranti Mark 1, the world's first commercially available general-purpose electronic computer.[17]
",Technology
"IBM introduced the first hard disk drive in 1956, as a component of their 305 RAMAC computer system.[18]:6 Most digital data today is still stored magnetically on hard disks, or optically on media such as CD-ROMs.[19]:4–5 Until 2002 most information was stored on analog devices, but that year digital storage capacity exceeded analog for the first time. As of 2007 almost 94% of the data stored worldwide was held digitally:[20] 52% on hard disks, 28% on optical devices and 11% on digital magnetic tape. It has been estimated that the worldwide capacity to store information on electronic devices grew from less than 3 exabytes in 1986 to 295 exabytes in 2007,[21] doubling roughly every 3 years.[22]
",Technology
"Database management systems emerged in the 1960s to address the problem of storing and retrieving large amounts of data accurately and quickly. One of the earliest such systems was IBM's Information Management System (IMS),[23] which is still widely deployed more than 50 years later.[24] IMS stores data hierarchically,[23] but in the 1970s Ted Codd proposed an alternative relational storage model based on set theory and predicate logic and the familiar concepts of tables, rows and columns. The first commercially available relational database management system (RDBMS) was available from Oracle in 1981.[25]
",Technology
"All database management systems consist of a number of components that together allow the data they store to be accessed simultaneously by many users while maintaining its integrity.[26] A characteristic of all databases is that the structure of the data they contain is defined and stored separately from the data itself, in a database schema.[23]
",Technology
"The extensible markup language (XML) has become a popular format for data representation in recent years. Although XML data can be stored in normal file systems, it is commonly held in relational databases to take advantage of their ""robust implementation verified by years of both theoretical and practical effort"".[27] As an evolution of the Standard Generalized Markup Language (SGML), XML's text-based structure offers the advantage of being both machine and human-readable.[28]
",Technology
"The relational database model introduced a programming-language independent Structured Query Language (SQL), based on relational algebra.
",Technology
"The terms ""data"" and ""information"" are not synonymous. Anything stored is data, but it only becomes information when it is organized and presented meaningfully.[29]:1–9 Most of the world's digital data is unstructured, and stored in a variety of different physical formats[30][b] even within a single organization. Data warehouses began to be developed in the 1980s to integrate these disparate stores. They typically contain data extracted from various sources, including external sources such as the Internet, organized in such a way as to facilitate decision support systems (DSS).[31]:4–6
",Technology
"Data transmission has three aspects: transmission, propagation, and reception.[32] It can be broadly categorized as broadcasting, in which information is transmitted unidirectionally downstream, or telecommunications, with bidirectional upstream and downstream channels.[21]
",Technology
"XML has been increasingly employed as a means of data interchange since the early 2000s,[33] particularly for machine-oriented interactions such as those involved in web-oriented protocols such as SOAP,[28] describing ""data-in-transit rather than ... data-at-rest"".[33]  One of the challenges of such usage is converting data from relational databases into XML Document Object Model (DOM) structures.[34]:228–31
",Technology
"Hilbert and Lopez identify the exponential pace of technological change (a kind of Moore's law): machines' application-specific capacity to compute information per capita roughly doubled every 14 months between 1986 and 2007; the per capita capacity of the world's general-purpose computers doubled every 18 months during the same two decades; the global telecommunication capacity per capita doubled every 34 months; the world's storage capacity per capita required roughly 40 months to double (every 3 years); and per capita broadcast information has doubled every 12.3 years.[21]
",Technology
"Massive amounts of data are stored worldwide every day, but unless it can be analysed and presented effectively it essentially resides in what have been called data tombs: ""data archives that are seldom visited"".[35] To address that issue, the field of data mining – ""the process of discovering interesting patterns and knowledge from large amounts of data""[36] – emerged in the late 1980s.[37]
",Technology
"In an academic context, the Association for Computing Machinery defines IT as ""undergraduate degree programs that prepare students to meet the computer technology needs of business, government, healthcare, schools, and other kinds of organizations .... IT specialists assume responsibility for selecting hardware and software products appropriate for an organization, integrating those products with organizational needs and infrastructure, and installing, customizing, and maintaining those applications for the organization’s computer users.""[38]
",Technology
"Companies in the information technology field are often discussed as a group as the ""tech sector"" or the ""tech industry"".[39][40][41][42]
",Technology
"In a business context, the Information Technology Association of America has defined information technology as ""the study, design, development, application, implementation, support or management of computer-based information systems"".[43][page needed] The responsibilities of those working in the field include network administration, software development and installation, and the planning and management of an organization's technology life cycle, by which hardware and software are maintained, upgraded and replaced.
",Technology
"The business value of information technology lies in the automation of business processes, provision of information for decision making, connecting businesses with their customers, and the provision of productivity tools to increase efficiency.
",Technology
"Employment distribution of computer systems design and related services, 2011[44]
",Technology
"Employment in the computer systems and design related services industry, in thousands, 1990-2011[44]
",Technology
"Occupational growth and wages in computer systems design and related services, 2010-2020[44]
",Technology
"Projected percent change in employment in selected occupations in computer systems design and related services, 2010-2020[44]
",Technology
"Projected average annual percent change in output and employment in selected industries, 2010-2020[44]
",Technology
"The field of information ethics was established by mathematician Norbert Wiener in the 1940s.[45]:9 Some of the ethical issues associated with the use of information technology include:[46]:20–21
",Technology
"This article is about the important technologies that have historically increased productivity and is intended to serve as the History section of Productivity from which it was moved.
",Technology
"Productivity in general is a ratio of output to input in the production of goods and services.  Productivity is increased by lowering the amount of labor, capital, energy or materials that go into producing any given amount of economic goods and services. Increases in productivity are largely responsible for the increase in per capita living standards.
",Technology
"Productivity improving technologies date back to antiquity, with rather slow progress until the late Middle Ages.  Important examples of early to medieval European technology include the water wheel, the horse collar, the spinning wheel, the three-field system (after 1500 the four-field system—see Crop rotation) and the blast furnace.[1]  All of these technologies had been in use in China, some for centuries, before being introduced to Europe.[2]
",Technology
"Technological progress was aided by literacy and the diffusion of knowledge that accelerated after the spinning wheel spread to Western Europe in the 13th century.  The spinning wheel increased the supply of rags used for pulp in paper making, whose technology reached Sicily sometime in the 12th century.  Cheap paper was a factor in the development of the movable type printing press, which led to a large increase in the number of books and titles published.[3][4]  Books on science and technology eventually began to appear, such as the mining technical manual De Re Metallica, which was the most important technology book of the 16th century and was the standard chemistry text for the next 180 years.[5]
",Technology
"Francis Bacon (1561-1626) is known for the scientific method, which was a key factor in the scientific revolution.  Bacon stated that the technologies that distinguished Europe of his day from the Middle Ages were paper and printing, gunpowder and the magnetic compass, known as the four great inventions.  The four great inventions important to the development of Europe were of Chinese origin.[6]  Other Chinese inventions included the horse collar, cast iron, an improved plow and the seed drill.  See also List of Chinese inventions.
",Technology
"Mining and metal refining technologies played a key role in technological progress.  Much of our understanding of fundamental chemistry evolved from ore smelting and refining, with De Re Metallica being the leading chemistry text for 180 years.[5]  Railroads evolved from mine carts and the first steam engines were designed specifically for pumping water from mines.  The significance of the blast furnace goes far beyond its capacity for large scale production of cast iron.  The blast furnace was the first example of continuous production and is a countercurrent exchange process, various types of which are also used today in chemical and petroleum refining.  Hot blast, which recycled what would have otherwise been waste heat, was one of engineering's key technologies.  It had the immediate effect of dramatically reducing the energy required to produce pig iron, but reuse of heat was eventually applied to a variety of industries, particularly steam boilers, chemicals, petroleum refining and pulp and paper.
",Technology
"Before the 17th century scientific knowledge tended to stay within the intellectual community, but by this time it became accessible to the public in what is called ""open science"".[7]  Near the beginning of the Industrial Revolution came publication of the Encyclopédie, written by numerous contributors and edited by Denis Diderot and Jean le Rond d'Alembert (1751–72). It contained many articles on science and was the first general encyclopedia to provide in depth coverage on the mechanical arts, but is far more recognized for its presentation of thoughts of the Enlightenment.
",Technology
"Economic historians generally agree that, with certain exceptions such as the steam engine, there is no strong linkage between the 17th century scientific revolution (Descartes, Newton, etc.) and the Industrial Revolution.[7] However, an important mechanism for the transfer of technical knowledge was scientific societies, such as The Royal Society of London for Improving Natural Knowledge, better known as the Royal Society, and the Académie des Sciences.  There were also technical colleges, such as the École Polytechnique.  Scotland was the first place where science was taught (in the 18th century) and was where Joseph Black discovered heat capacity and latent heat and where his friend James Watt used knowledge of heat to conceive the separate condenser as a means to improve the efficiency of the steam engine.[8]
",Technology
"Probably the first period in history in which economic progress was observable after one generation was during the British Agricultural Revolution in the 18th century.[9]  However, technological and economic progress did not proceed at a significant rate until the English Industrial Revolution in the late 18th century, and even then productivity grew about 0.5% annually.  High productivity growth began during the late 19th century in what is sometimes call the Second Industrial Revolution.  Most major innovations of the Second Industrial Revolution were based on the modern scientific understanding of chemistry, electromagnetic theory and thermodynamics and other principles known to profession of engineering.
",Technology
"Before the industrial revolution the only sources of power were water, wind and muscle.  Most good water power sites (those not requiring massive modern dams) in Europe were developed during the medieval period.  In the 1750s John Smeaton, the ""father of civil engineering,"" significantly improved the efficiency of the water wheel by applying scientific principles, thereby adding badly needed power for the Industrial Revolution.[11]  However water wheels remained costly, relatively inefficient and not well suited to very large power dams.  Benoît Fourneyron's highly efficient turbine developed in the late 1820s eventually replaced waterwheels.  Fourneyron type turbines can operate at 95% efficiency and used in today's large hydro-power installations.  Hydro-power continued to be the leading source of industrial power in the United States until past the mid 19th century because of abundant sites, but steam power overtook water power in the UK decades earlier.[12]
",Technology
"In 1711 a Newcomen steam engine was installed for pumping water from a mine, a job that typically was done by large teams of horses, of which some mines used as many as 500. Animals convert feed to work at an efficiency of about 5%, but while this was much more than the less than 1% efficiency of the early Newcomen engine, in coal mines there was low quality coal with little market value available.  Fossil fuel energy first exceeded all animal and water power in 1870. The role energy and machines replacing physical work is discussed in Ayres-Warr (2004, 2009).[13][14]
",Technology
"While steamboats were used in some areas, as recently as the late 19th Century thousands of workers pulled barges.  Until the late 19th century most coal and other minerals were mined with picks and shovels and crops were harvested and grain threshed using animal power or by hand.  Heavy loads like 382 pound bales of cotton were handled on hand trucks until the early 20th century.
",Technology
"Excavation was done with shovels until the late 19th century when steam shovels came into use.  It was reported that a laborer on the western division of the Erie Canal was expected to dig 5 cubic yards per day in 1860; however, by 1890 only 3-1/2 yards per day were expected.[16]  Today's large electric shovels have buckets that can hold 168 cubic meters and consume the power of a city of 100,000.[17]
",Technology
"Dynamite, a safe to handle blend of nitroglycerin and diatomaceous earth was patented in 1867 by Alfred Nobel. Dynamite increased productivity of mining, tunneling, road building, construction and demolition and made projects such as the Panama Canal possible.
",Technology
"Steam power was applied to threshing machines in the late 19th century.  There were steam engines that moved around on wheels under their own power that were used for supplying temporary power to stationary farm equipment such as threshing machines.  These were called road engines, and Henry Ford seeing one as a boy was inspired to build an automobile.[18]  Steam tractors were used but never became popular.
",Technology
"With internal combustion came the first mass-produced tractors (Fordson c. 1917).   Tractors replaced horses and mules for pulling reapers and combine harvesters, but in the 1930s self powered combines were developed.  Output per man hour in growing wheat rose by a factor of about 10 from the end of World War II until about 1985, largely because of powered machinery, but also because of increased crop yields.[19]  Corn manpower showed a similar but higher productivity increase. 
See below:Mechanized agriculture
",Technology
"One of the greatest periods of productivity growth coincided with the electrification of factories which took place between 1900 and 1930 in the U.S.[13][20] See: Mass production: Factory electrification
",Technology
"In engineering and economic history the most important types of energy efficiency were in the conversion of heat to work, the reuse of heat and the reduction of friction.[21] There was also a dramatic reduction energy required to transmit electronic signals, both voice and data.
",Technology
"The early Newcomen steam engine was about 0.5% efficient and was improved to slightly over 1%  by John Smeaton before Watt's improvements, which increased thermal efficiency to 2%. In 1900 it took 7 lbs coal/ kw hr.
",Technology
"Electrical generation was the sector with the highest productivity growth in the U.S. in the early twentieth century.  After the turn of the century large central stations with high pressure boilers and efficient steam turbines replaced reciprocating steam engines and by 1960 it took 0.9 lb coal per kw-hr.  Counting the improvements in mining and transportation the total improvement was by a factor greater than 10.[22] Today's steam turbines have efficiencies in the 40% range.[14][23][24][25] Most electricity today is produced by thermal power stations using steam turbines.
",Technology
"The Newcomen and Watt engines operated near atmospheric pressure and used atmospheric pressure, in the form of a vacuum caused by condensing steam, to do work.  Higher pressure engines were light enough, and efficient enough to be used for powering ships and locomotives.  Multiple expansion (multi-stage) engines were developed in the 1870s and were efficient enough for the first time to allow ships to carry more freight than coal, leading to great increases in international trade.[26]
",Technology
"The first important diesel ship was the MS Selandia launched in 1912.  By 1950 one-third of merchant shipping was diesel powered.[27]  Today the most efficient prime mover is the two stroke marine diesel engine developed in the 1920s, now ranging in size to over 100,000 horsepower with a thermal efficiency of 50%.[28]
",Technology
"Steam locomotives that used up to 20% of the U.S. coal production were replaced by diesel locomotives after World War II, saving a great deal of energy and reducing manpower for handling coal, boiler water and mechanical maintenance.
",Technology
"Improvements in steam engine efficiency caused a large increase in the number of steam engines and the amount of coal used, as noted by William Stanley Jevons in The Coal Question.  This is called the Jevons paradox.
",Technology
"Electricity consumption and economic growth are strongly correlated.[29] Per capita electric consumption correlates almost perfectly with economic development.[30]
Electrification was the first technology to enable long distance transmission of power with minimal power losses.[31]  Electric motors did away with line shafts for distributing power and dramatically increased the productivity of factories. Very large central power stations created economies of scale and were much more efficient at producing power than reciprocating steam engines.[13][29][31][32][33]  Electric motors greatly reduced the capital cost of power compared to steam engines.[32]
",Technology
"The main forms of pre-electric power transmission were line shafts, hydraulic power networks and pneumatic and wire rope systems. Line shafts were the common form of power transmission in factories from the earliest industrial steam engines until factory electrification.  Line shafts limited factory arrangement and suffered from high power losses.[31] Hydraulic power came into use in the mid 19th century.  It was used extensively in the Bessemer process and for cranes at ports, especially in the UK.  London and a few other cities had hydraulic utilities that provided pressurized water for industrial over a wide area.[31]
",Technology
"Pneumatic power began being used industry and in mining and tunneling in the last quarter of the 19th century.  Common applications included rock drills and jack hammers.[31] Wire ropes supported by large grooved wheels were able to transmit power with low loss for a distance of a few miles or kilometers.  Wire rope systems appeared shortly before electrification.[31]
",Technology
"Recovery of heat for industrial processes was first widely used as hot blast in blast furnaces to make pig iron in 1828.  Later heat reuse included the Siemens-Martin process which was first used for making glass and later for steel with the open hearth furnace. (See: Iron and steel below).  Today heat is reused in many basic industries such as chemicals, oil refining and pulp and paper, using a variety of methods such as heat exchangers in many processes.[34] Multiple-effect evaporators use vapor from a high temperature effect to evaporate a lower temperature boiling fluid.  In the recovery of kraft pulping chemicals the spent black liquor can be evaporated five or six times by reusing the vapor from one effect to boil the liquor in the preceding effect. Cogeneration is a process that uses high pressure steam to generate electricity and then uses the resulting low pressure steam for process or building heat.
",Technology
"Industrial process have undergone numerous minor improvements which collectively made significant reductions in energy consumption per unit of production.
",Technology
"Reducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, U.K.  
",Technology
"“ A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton.  A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration.  Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together.  A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”[35]",Technology
"Better lubrication, such as from petroleum oils, reduced friction losses in mills and factories.[36]  Anti-friction bearings were developed using alloy steels and precision machining techniques available in the last quarter of the 19th century.  Anti-friction bearings were widely used on bicycles by the 1880s.  Bearings began being used on line shafts in the decades before factory electrification and it was the pre-bearing shafts that were largely responsible for their high power losses, which were commonly 25 to 30% and often as much as 50%.[31]
",Technology
"Electric lights were far more efficient than oil or gas lighting and did not generate smoke, fumes nor as much heat.  Electric light extended the work day, making factories, businesses and homes more productive.  Electric light was not a great fire hazard like oil and gas light.[37]
",Technology
"The efficiency of electric lights has continuously improved from the first incandescent lamps to tungsten filament lights.[38]  The fluorescent lamp, which became commercial in the late 1930s, is much more efficient than incandescent lighting.  Light-emitting diodes or LED's are highly efficient and long lasting.[39]
",Technology
"The relative energy required for transport of a tonne-km for various modes of transport are: pipelines=1(basis), water 2, rail 3, road 10, air 100.[40]
",Technology
"Unimproved roads were extremely slow, costly for transport and dangerous.[41]  In the 18th century layered gravel began being increasingly used, with the three layer Macadam coming into use in the early 19th century.  These roads were crowned to shed water and had drainage ditches along the sides.[41]  The top layer of stones eventually crushed to fines and smoothed the surface somewhat.  The lower layers were of small stones that allowed good drainage.[41] Importantly, they offered less resistance to wagon wheels and horses hooves and feet did not sink in the mud. Plank roads also came into use in the U.S. in the 1810s-1820s. Improved roads were costly, and although they cut the cost of land transportation in half or more, they were soon overtaken by railroads as the major transportation infrastructure.[41]
",Technology
"Sailing ships could transport goods for over a 3000 miles for the cost of 30 miles by wagon.[42]  A horse that could pull a one-ton wagon could pull a 30-ton barge. During the English or First Industrial Revolution, supplying coal to the furnaces at Manchester was difficult because there were few roads and because of the high cost of using wagons.  However, canal barges were known to be workable, and this was demonstrated by building the Bridgewater Canal, which opened in 1761, bringing coal from Worsley to Manchester.  The Bridgewater Canal’s success started a frenzy of canal building that lasted until the appearance of railroads in the 1830s.[40][41]
",Technology
"Railroads greatly reduced the cost of overland transportation.  It is estimated that by 1890 the cost of wagon freight was U.S. 24.5 cents/ton-mile versus 0.875 cents/ton-mile by railroad, for a decline of 96%.[43]
",Technology
"Electric street railways (trams, trolleys or streetcars) were in the final phase of railroad building from the late 1890s and first two decades of the 20th century. Street railways were soon displaced by motor buses and automobiles after 1920.[44]
",Technology
"Highways with internal combustion powered vehicles completed the mechanization of overland transportation.  When trucks appeared c. 1920 the price transporting farm goods to market or to rail stations was greatly reduced.  Motorized highway transport also reduced inventories.
",Technology
"The high productivity growth in the U.S. during the 1930s was in large part due to the highway building program of that decade.[45]
",Technology
"Pipelines are the most energy efficient means of transportation.[40] Iron and steel pipelines came into use during latter part of the 19th century, but only became a major infrastructure during the 20th century.[41][46] Centrifugal pumps and centrifugal compressors are efficient means of pumping liquids and natural gas.
",Technology
"The seed drill is a mechanical device for spacing and planting seed at the appropriate depth.  It originated in ancient China before the 1st century BC.  Saving seed was extremely important at a time when yields were measured in terms of seeds harvested per seed planted, which was typically between 3 and 5.  The seed drill also saved planting labor.  Most importantly, the seed drill meant crops were grown in rows, which reduced competition of plants and increase yields.  It was reinvented in 16th century Europe based on verbal descriptions and crude drawings brought back from China.[6] Jethro Tull patented a version in 1700; however, it was expensive and unreliable.  Reliable seed drills appeared in the mid 19th century.[47]
",Technology
"Since the beginning of agriculture threshing was done by hand with a flail, requiring a great deal of labor.  The threshing machine (ca. 1794) simplified the operation and allowed it to use animal power.  By the 1860s threshing machines were widely introduced and ultimately displaced as much as a quarter of agricultural labor.[48]
In Europe, many of the displaced workers were driven to the brink of starvation.
",Technology
"Before c. 1790 a worker could harvest 1/4 acre per day with a scythe.[26]  In the early 1800s the grain cradle was introduced, significantly increasing the productivity of hand labor.
It was estimated that each of Cyrus McCormick's horse pulled reapers (Ptd. 1834) freed up five men for military service in the U.S. Civil War.[49]  By 1890 two men and two horses could cut, rake and bind 20 acres of wheat per day.[26]  
In the 1880s the reaper and threshing machine were combined into the combine harvester.  These machines required large teams of horses or mules to pull.  Over the entire 19th century the output per man hour for producing wheat rose by about 500% and for corn about 250%.[19]
",Technology
"Farm machinery and higher crop yields reduced the labor to produce 100 bushels of corn from 35 to 40 hours in 1900 to 2 hours 45 minutes in 1999.[50] The conversion of agricultural mechanization to internal combustion power began after 1915.  The horse population began to decline in the 1920s after the conversion of agriculture and transportation to internal combustion.[51]  In addition to saving labor, this freed up much land previously used for supporting draft animals.
",Technology
"The peak years for tractor sales in the U.S. were the 1950s.[51]   There was a large surge in horsepower of farm machinery in the 1950s.
",Technology
"The most important mechanical devices before the Industrial Revolution were water and wind mills.  Water wheels date to Roman times and windmills somewhat later.  Water and wind power were first used for grinding grain into flour, but were later adapted to power trip hammers for pounding rags into pulp for making paper and for crushing ore.  Just before the Industrial revolution water power was applied to bellows for iron smelting in Europe.  (Water powered blast bellows were used in ancient China.)  Wind and water power were also used in sawmills.[40]  
The technology of building mills and mechanical clocks was important to the development of the machines of the Industrial Revolution.[52]
",Technology
"The spinning wheel was a medieval invention that increased thread making productivity by a factor greater than ten.  One of the early developments that preceded the Industrial Revolution was the stocking frame (loom) of c. 1589.  Later in the Industrial Revolution came the flying shuttle, a simple device that doubled the productivity of weaving.  Spinning thread had been a limiting factor in cloth making requiring 10 spinners using the spinning wheel to supply one weaver.  With the spinning jenny a spinner could spin eight threads at once.  The water frame (Ptd. 1768) adapted water power to spinning, but it could only spin one thread at a time. The water frame was easy to operate and many could be located in a single building.  The spinning mule (1779) allowed a large number of threads to be spun by a single machine using water power.  A change in consumer preference for cotton at the time of increased cloth production resulted in the invention of the cotton gin (Ptd. 1794).  Steam power eventually was used as a supplement to water during the Industrial Revolution, and both were used until electrification.  A graph of productivity of spinning technologies can be found in Ayres (1989), along with much other data related this article.[53]
",Technology
"With a cotton gin (1792) in one day a man could remove seed from as much upland cotton as would have previously taken a woman working two months to process at one pound per day using a roller gin.[54][55]
",Technology
"An early example of a large productivity increase by special purpose machines is the c. 1803 Portsmouth Block Mills.  With these machines 10 men could produce as many blocks as 110 skilled craftsmen.[40]
",Technology
"In the 1830s several technologies came together to allow an important shift in wooden building construction. The circular saw (1777), cut nail machines (1794), and steam engine allowed slender pieces of lumber such as 2""x4""s to be efficiently produced and then nailed together in what became known as balloon framing (1832). This was the beginning of the decline of the ancient method of timber frame construction with wooden joinery.[56]
",Technology
"Following mechanization in the textile industry was mechanization of the shoe industry.[57]
",Technology
"The sewing machine, invented and improved during the early 19th century and produced in large numbers by the 1870s, increased productivity by more than 500%.[58]  The sewing machine was an important productivity tool for mechanized shoe production.
",Technology
"With the widespread availability of machine tools, improved steam engines and inexpensive transportation provided by railroads, the machinery industry became the largest sector (by profit added) of the U. S. economy by the last quarter of the 19th century, leading to an industrial economy.[59]
",Technology
"The first commercially successful glass bottle blowing machine was introduced in 1905.[60]  The machine, operated by a two-man crew working 12-hour shifts, could produce 17,280 bottles in 24 hours, compared to 2,880 bottles made a crew of six men and boys working in a shop for a day.  The cost of making bottles by machine was 10 to 12 cents per gross compared to $1.80 per gross by the manual glassblowers and helpers.
",Technology
"Machine tools, which cut, grind and shape metal parts, were another important mechanical innovation of the Industrial Revolution.  Before machine tools it was prohibitively expensive to make precision parts, an essential requirement for many machines and interchangeable parts.    Historically important machine tools are the screw-cutting lathe, milling machine and metal planer (metalworking), which all came into use between 1800 and 1840.[54] However, around 1900, it was the combination of small electric motors, specialty steels and new cutting and grinding materials that allowed machine tools to mass-produce steel parts.[17]  Production of the Ford Model T required 32,000 machine tools.[49]
",Technology
"Modern manufacturing began around 1900 when machines, aided by electric, hydraulic and pneumatic power, began to replace hand methods in industry.[61]  An early example is the Owens automatic glass bottle blowing machine, which reduced labor in making bottles by over 80%.[62]  See also: Mass production#Factory electrification
",Technology
"Large mining machines, such as steam shovels, appeared in the mid-nineteenth century, but were restricted to rails until the widespread introduction of continuous track and pneumatic tires in the late 19th and early 20th centuries.  Until then much mining work was mostly done with pneumatic drills, jackhammers, picks and shovels.[63]
",Technology
"Coal seam undercutting machines appeared around 1890 and were used for 75% of coal production by 1934.  Coal loading was still being done manually with shovels around 1930, but mechanical pick up and loading machines were coming into use.[61]  The use of the coal boring machine improved productivity of sub-surface coal mining by a factor of three between 1949 and 1969.[64]
",Technology
"There is currently a transition going under way from more labor-intensive methods of mining to more mechanization and even automated mining.[65]
",Technology
"Dry bulk materials handling systems use a variety of stationary equipment such as conveyors, stackers, reclaimers and mobile equipment such as power shovels and loaders to handle high volumes of ores, coal, grains, sand, gravel, crushed stone, etc.   Bulk materials handling systems are used at mines, for loading and unloading ships and at factories that process bulk materials into finished goods, such as steel and paper mills.
",Technology
"Mechanical stokers for feeding coal to locomotives were in use in the 1920s.  A completely mechanized and automated coal handling and stoking system was first used to feed pulverized coal to an electric utility boiler in 1921.[61]
",Technology
"Liquids and gases are handled with centrifugal pumps and compressors, respectively.
",Technology
"Conversion to powered material handling increased during WW 1 as shortages of unskilled labor developed and unskilled wages rose relative to skilled labor.[61]
",Technology
"A noteworthy use of conveyors was Oliver Evans's automatic flour mill built in 1785.[49]
",Technology
"Around 1900 various types of conveyors (belt, slat, bucket, screw or auger), overhead cranes and industrial trucks began being used for handling materials and goods in various stages of production in factories.  See: Types of conveyor systems  See also: Mass production.
",Technology
"A well known application of conveyors is Ford. Motor Co.'s assembly line (c. 1913), although Ford used various industrial trucks, overhead cranes, slides and whatever devices necessary to minimize labor in handling parts in various parts of the factory.[49]
",Technology
"Cranes are an ancient technology but they became widespread following the Industrial Revolution.  Industrial cranes were used to handle heavy machinery at the Nasmyth, Gaskell and Company (Bridgewater foundry) in the late 1830s.[66] Hydraulic powered cranes became widely used in the late 19th century, especially at British ports.  Some cities, such as London, had public utility hydraulic service networks to power.  Steam cranes were also used in the late 19th century. Electric cranes, especially the overhead type, were introduce in factories at the end of the 19th century.[37]  Steam cranes were usually restricted to rails.[67] Continuous track (caterpillar tread) was developed in the late 19th century.
",Technology
"The important categories of cranes are:
",Technology
"Handling goods on pallets was a significant improvement over using hand trucks or carrying sacks or boxes by hand and greatly speeded up loading and unloading of trucks, rail cars and ships.  Pallets can be handled with pallet jacks or forklift trucks which began being used in industry in the 1930s and became widespread by the 1950s.[68] Loading docks built to architectural standards allow trucks or rail cars to load and unload at the same elevation as the warehouse floor.
",Technology
"Piggyback is the transporting of trailers or entire trucks on rail cars, which is a more fuel efficient means of shipping and saves loading, unloading and sorting labor.  Wagons had been carried on rail cars in the 19th century, with horses in separate cars.  Trailers began being carried on rail cars in the U.S. in 1956.[69]  Piggyback was 1% of freight in 1958, rising to 15% in 1986.[70]
",Technology
"Either loading or unloading break bulk cargo on and off ships typically took several days.  It was strenuous and somewhat dangerous work.  Losses from damage and theft were high. The work was erratic and most longshoreman had a lot of unpaid idle time.  Sorting and keeping track of break bulk cargo was also time consuming, and holding it in warehouses tied up capital.[68]
",Technology
"Old style ports with warehouses were congested and many lacked efficient transportation infrastructure, adding to costs and delays in port.[68]
",Technology
"By handling freight in standardized containers in compartmentalized ships, either loading or unloading could typically be accomplished in one day.  Containers can be more efficiently filled than break bulk because containers can be stacked several high, doubling the freight capacity for a given size ship.[68]
",Technology
"Loading and unloading labor for containers is a fraction of break bulk, and damage and theft are much lower.  Also, many items shipped in containers require less packaging.[68]
",Technology
"Containerization with small boxes was used in both world wars, particularly WW II, but became commercial in the late 1950s.[68]  Containerization left large numbers of warehouses at wharves in port cities vacant, freeing up land for other development.  See also: Intermodal freight transport
",Technology
"Before the factory system much production took place in the household, such as spinning and weaving, and was for household consumption.[71][72]   This was partly due to the lack of transportation infrastructures, especially in America.[73]
",Technology
"Division of labor was practiced in antiquity but became increasingly specialized during the Industrial Revolution, so that instead of a shoemaker cutting out leather as part of the operation of making a shoe, a worker would do nothing but cut out leather.[21][74]  In Adam Smith's famous example of a pin factory, workers each doing a single task were far more productive than a craftsmen making an entire pin.
",Technology
"Starting before and continuing into the industrial revolution, much work was subcontracted under the putting out system (also called the domestic system) whereby work was done at home.  Putting out work included spinning, weaving, leather cutting and, less commonly, specialty items such as firearms parts.  Merchant capitalists or master craftsmen typically provided the materials and collected the work pieces, which were made into finished product in a central workshop.[21][74][75]
",Technology
"During the industrial revolution much production took place in workshops, which were typically located in the rear or upper level of the same building where the finished goods were sold.   These workshops used tools and sometimes simple machinery, which was usually hand or animal powered.  The master craftsman, foreman or merchant capitalist supervised the work and maintained quality.   Workshops grew in size but were displaced by the factory system in the early 19th century.  Under the factory system capitalists hired workers and provided the buildings, machinery and supplies and handled the sale of the finished products.[76]
",Technology
"Changes to traditional work processes that were done after analyzing the work and making it more systematic greatly increased the productivity of labor and capital.  This was the changeover from the European system of craftsmanship, where a craftsman made a whole item,  to the American system of manufacturing which used special purpose machines and machine tools that made parts with precision to be interchangeable. The process took decades to perfect at great expense because interchangeable parts were more costly at first.  Interchangeable parts were achieved by using fixtures to hold and precisely align parts being machined, jigs to guide the machine tools and gauges to measure critical dimensions of finished parts.[49]
",Technology
"Other work processes involved minimizing the number of steps in doing individual tasks, such as bricklaying, by performing time and motion studies to determine the one best method, the system becoming known as Taylorism after Fredrick Winslow Taylor who is the best known developer of this method, which is also known as scientific management after his work The Principles of Scientific Management.[77]
",Technology
"Standardization and interchangeability are considered to be main reasons for U.S. exceptionality.[78]Standardization was part of the change to interchangeable parts, but was also facilitated by the railroad industry and mass-produced goods.[49][79]  Railroad track gauge standardization and standards for rail cars allowed inter-connection of railroads.  Railway time formalized time zones.  Industrial standards included screw sizes and threads and later electrical standards.  Shipping container standards were loosely adopted in the late 1960s and formally adopted ca. 1970.[68]  Today there are vast numbers of technical standards.  Commercial standards includes such things as bed sizes.  Architectural standards cover numerous dimensions including stairs, doors, counter heights and other designs to make buildings safe, functional and in some cases allow a degree of interchangeability.
",Technology
"Electrification allowed the placement of machinery such as machine tools in a systematic arrangement along the flow of the work.  Electrification was a practical way to motorize conveyors to transfer parts and assemblies to workers,  which was a key step leading to mass production and the assembly line.[20]
",Technology
"Business administration, which includes management practices and accounting systems is another important form of work practices.  As the size of businesses grew in the second half of the 19th century they began being  organized by departments and managed by professional managers as opposed to being run by sole proprietors or partners.[80]
",Technology
"Business administration as we know it was developed by railroads who had to keep up with trains, railcars, equipment, personnle and freight over large territories.[80]
",Technology
"Modern business enterprise (MBE) is the organization and management of businesses, particularly large ones.[81]  MBE's employ professionals who use knowledge based techniques such areas as engineering, research and development, information technology, business administration, finance and accounting.  MBE's typically benefit from economies of scale.
",Technology
"“Before railroad accounting we were moles burrowing in the dark.""[82]  Andrew Carnegie",Technology
"Continuous production is a method by which a process operates without interruption for long periods, perhaps even years.  Continuous production began with blast furnaces in ancient times and became popular with mechanized processes following the invention of the Fourdrinier paper machine during the Industrial Revolution, which was the inspiration for continuous rolling.[83]  It began being widely used in chemical and petroleum refining industries in the late nineteenth and early twentieth centuries.  It was later applied to direct strip casting of steel and other metals.
",Technology
"Early steam engines did not supply power at a constant enough load for many continuous applications ranging from cotton spinning to rolling mills, restricting their power source to water.  Advances in steam engines such as the Corliss steam engine and the development of control theory led to more constant engine speeds, which made steam power useful for sensitive tasks such as cotton spinning.  AC motors, which run at constant speed even with load variations, were well suited to such processes.
",Technology
"Losses of agricultural products to spoilage, insects and rats contributed greatly to productivity.  Much hay stored outdoors was lost to spoilage before indoor storage or some means of coverage became common.  Pasteurization of milk allowed it to be shipped by railroad.[26]
",Technology
"Keeping livestock indoors in winter reduces the amount of feed needed.  Also, feeding chopped hay and ground grains, particularly corn (maize), was found to improve digestibility.[26]  The amount of feed required to produce a kg of live weight chicken fell from 5 in 1930 to 2 by the late 1990s and the time required fell from three months to six weeks.[17]
",Technology
"The Green Revolution increased crop yields by a factor of 3 for soybeans and between 4 and 5 for corn (maize), wheat, rice and some other crops.  Using data for corn (maize) in the U.S., yields increased about 1.7 bushels per acre from the early 1940s until the first decade of the 21st century when concern was being expressed about reaching limits of photosynthesis.  Because of the constant nature of the yield increase, the annual percentage increase has declined from over 5% in the 1940s to 1% today, so while yields for a while outpaced population growth, yield growth now lags population growth.
",Technology
"High yields would not be possible without significant applications of fertilizer,[85][86] particularly nitrogen fertilizer which was made affordable by the Haber-Bosch ammonia process.[87]  Nitrogen fertilizer is applied in many parts of Asia in amounts subject to diminishing returns,[87] which however does still give a slight increase in yield.  Crops in Africa are in general starved for NPK and much of the world's soils are deficient in zinc, which leads to deficiencies in humans.
",Technology
"The greatest period of agricultural productivity growth in the U.S. occurred from World War 2 until the 1970s.[88]
",Technology
"Land is considered a form of capital, but otherwise has received little attention relative to its importance as a factor of productivity by modern economists, although it was important in classical economics.  However, higher crop yields effectively multiplied the amount of land.
",Technology
"The process of making cast iron was known before the 3rd century AD in China.[89]  Cast iron production reached Europe in the 14th century and Britain around 1500.  Cast iron was useful for casting into pots and other implements, but was too brittle for making most tools.  However, cast iron had a lower melting temperature than wrought iron and was much easier to make with primitive technology.[90] Wrought iron was the material used for making many hardware items, tools and other implements.  Before cast iron was made in Europe, wrought iron was made in small batches by the bloomery process, which was never used in China.[89]  Wrought iron could be made from cast iron more cheaply than it could be made with a bloomery.
",Technology
"The inexpensive process for making good quality wrought iron was puddling, which became widespread after 1800.[91]  Puddling involved stirring molten cast iron until small globs sufficiently decarburized to form globs of hot wrought iron that were then removed and hammered into shapes. Puddling was extremely labor-intensive. Puddling was used until the introduction of the Bessemer and open hearth processes in the mid and late 19th century, respectively.[21]
",Technology
"Blister steel was made from wrought iron by packing wrought iron in charcoal and heating for several days. See: Cementation process The blister steel could be heated and hammered with wrought iron to make shear steel, which was used for cutting edges like scissors, knives and axes.  Shear steel was of non uniform quality and a better process was needed for producing watch springs, a popular luxury item in the 18th century.  The successful process was crucible steel, which was made by melting wrought iron and blister steel in a crucible.[21][28]
",Technology
"Production of steel and other metals was hampered by the difficulty in producing sufficiently high temperatures for melting.  An understanding of thermodynamic principles such as recapturing heat from flue gas by preheating combustion air, known as hot blast, resulted in much higher energy efficiency and higher temperatures.  Preheated combustion air was used in iron production and in the open hearth furnace.  In 1780, before the introduction of hot blast in 1829, it required seven times as much coke as the weight of the product pig iron.[92]  The hundredweight of coke per short ton of pig iron was 35 in 1900, falling to 13 in 1950.  By 1970 the most efficient blast furnaces used 10 hundredweight of coke per short ton of pig iron.[27]
",Technology
"Steel has much higher strength than wrought iron and allowed long span bridges, high rise buildings, automobiles and other items.  Steel also made superior threaded fasteners (screws, nuts, bolts), nails, wire and other hardware items.  Steel rails lasted over 10 times longer than wrought iron rails.[93]
",Technology
"The Bessemer and open hearth processes were much more efficient than making steel by the puddling process because they used the carbon in the pig iron as a source of heat.  The Bessemer (patented in 1855) and the Siemens-Martin (c. 1865) processes greatly reduced the cost of steel. By the end of the 19th century, Gilchirst-Thomas “basic” process had reduced production costs by 90% compared to the puddling process of the mid-century.
",Technology
"Today a variety of alloy steels are available that have superior properties for special applications like automobiles, pipelines and drill bits.  High speed or tool steels, whose development began in the late 19th century, allowed machine tools to cut steel at much higher speeds.[94]  High speed steel and even harder materials were an essential component of mass production of automobiles.[95]
",Technology
"Some of the most important specialty materials are steam turbine and gas turbine blades, which have to withstand extreme mechanical stress and high temperatures.[28]
",Technology
"The size of blast furnaces grew greatly over the 20th century and innovations like additional heat recovery and pulverized coal, which displaced coke and increased energy efficiency.[96]
",Technology
"Bessemer steel became brittle with age because nitrogen was introduced when air was blown in.[97]  The Bessemer process was also restricted to certain ores (low phosphate hematite).  By the end of the 19th century the Bessemer process was displaced by the open hearth furnace (OHF).  After World War II the OHF was displaced by the basic oxygen furnace (BOF), which used oxygen instead of air and required about 35–40 minutes to produce a batch of steel compared to 8 to 9 hours for the OHF. The BOF also was more energy efficient.[96]
",Technology
"By 1913, 80% of steel was being made from molten pig iron directly from the blast furnace, eliminating the step of casting the ""pigs"" (ingots) and remelting.[61]
",Technology
"The continuous wide strip rolling mill, developed by ARMCO in 1928, was most important development in steel industry during the inter-war years.[98]  Continuous wide strip rolling started with a thick, coarse ingot.  It produced a smoother sheet with more uniform thickness, which was better for stamping and gave a nice painted surface.  It was good for automotive body steel and  appliances.  It used only a fraction of the labor of the discontinuous process, and was safer because it did not require continuous handling.  Continuous rolling was made possible by improved sectional speed control:  See: Automation, process control and servomechanisms
",Technology
"After 1950 continuous casting contributed to productivity of converting steel to structural shapes by eliminating the intermittent step of making slabs, billets (square cross-section) or blooms (rectangular) which then usually have to be reheated before rolling into shapes.[24]  Thin slab casting, introduced in 1989, reduced labor to less than one hour per ton.  Continuous thin slab casting and the BOF were the two most important productivity advancements in 20th-century steel making.[99]
",Technology
"As a result of these innovations, between 1920 and 2000 labor requirements in the steel industry decreased by a factor of 1,000, from more than 3 worker-hours per tonne to just 0.003.[24]
",Technology
"Sodium compounds: carbonate, bicarbonate and hydroxide are important industrial chemicals used in important products like making glass and soap.  Until the invention of the Leblanc process in 1791, sodium carbonate was made, at high cost, from the ashes of seaweed and the plant barilla.  The Leblanc process was replaced by the Solvay process beginning in the 1860s.  With the widespread availability of inexpensive electricity, much sodium is produced along with chlorine by electro-chemical processes.[21]
",Technology
"Cement is the binder for concrete, which is one of the most widely used construction materials today because of its low cost, versatility and durability.  Portland cement, which was invented 1824-5, is made by calcining limestone and other naturally occurring minerals in a kiln.[100] A great advance was the perfection of rotary cement kilns in the 1890s, the method still being used today.[101] Reinforced concrete, which is suitable for structures, began being used in the early 20th century.[102]
",Technology
"Paper was made one sheet at a time by hand until development of the Fourdrinier paper machine (c. 1801) which made a continuous sheet.  Paper making was severely limited by the supply of cotton and linen rags from the time of the invention of the printing press until the development of wood pulp (c. 1850s)in response to a shortage of rags.[4]  The sulfite process for making wood pulp started operation in Sweden in 1874. Paper made from sulfite pulp had superior strength properties than the previously used ground wood pulp (c. 1840).[103] The kraft (Swedish for strong) pulping process was commercialized in the 1930s.  Pulping chemicals are recovered and internally recycled in the kraft process, also saving energy and reducing pollution.[103][104]  Kraft paperboard is the material that the outer layers of corrugated boxes are made of.  Until Kraft corrugated boxes were available, packaging consisted of poor quality paper and paperboard boxes along with wood boxes and crates. Corrugated boxes require much less labor to manufacture than wooden boxes and offer good protection to their contents.[103]  Shipping containers reduce the need for packaging.[68]
",Technology
"Vulcanized rubber made the pneumatic tire possible, which in turn enabled the development of on and off-road vehicles as we know them.  Synthetic rubber became important during the Second World War when supplies of natural rubber were cut off.
",Technology
"Rubber inspired a class of chemicals known as elastomers, some of which are used by themselves or in blends with rubber and other compounds for seals and gaskets, shock absorbing bumpers and a variety of other applications.
",Technology
"Plastics can be inexpensively made into everyday items and have significantly lowered the cost of a variety of goods including packaging, containers, parts and household piping.
",Technology
"Optical fiber began to replace copper wire in the telephone network during the 1980s.  Optical fibers are very small diameter, allowing many to be bundled in a cable or conduit.  Optical fiber is also an energy efficient means of transmitting signals.
",Technology
"Seismic exploration, beginning in the 1920s, uses reflected sound waves to map subsurface geology to help locate potential oil reservoirs.  This was a great improvement over previous methods, which involved mostly luck and good knowledge of geology, although luck continued to be important in several major discoveries.  Rotary drilling was a faster and more efficient way of drilling oil and water wells.  It became popular after being used for the initial discovery of the East Texas field in 1930.
",Technology
"Numerous new hard materials were developed for cutting edges such as in machining.  Mushet steel, which was developed in 1868, was a forerunner of High speed steel, which was developed by a team led by Fredrick Winslow Taylor at Bethlehem Steel Company around 1900.[77]  High speed steel held its hardness even when it became red hot.  It was followed by a number of  modern alloys.
",Technology
"From 1935 to 1955 machining cutting speeds increased from 120–200 ft/min to 1000 ft/min due to harder cutting edges, causing machining costs to fall by 75%.[105]
",Technology
"One of the most important new hard materials for cutting is tungsten carbide.
",Technology
"Dematerialization is the reduction of use of materials in manufacturing, construction, packaging or other uses.  In the U.S. the quantity of raw materials per unit of output decreased approx 60% since 1900.  In Japan the reduction has been 40% since 1973.[106]
",Technology
"Dematerialization is made possible by substitution with better materials and by engineering to reduce weight while maintaining function.  Modern examples are plastic beverage containers replacing glass and paperboard, plastic shrink wrap used in shipping and light weight plastic packing materials.  Dematerialization has been occurring in the U. S. steel industry where the peak in consumption occurred in 1973 on both an absolute and per capita basis.[96] At the same time, per capita steel consumption grew globally through outsourcing.[107] Cumulative global GDP or wealth has grown in direct proportion to energy consumption since 1970, while Jevons paradox posits that efficiency improvement leads to increased energy consumption.[108][109] Access to energy globally constrains dematerialization.[110]
",Technology
"The telegraph appeared around the beginning of the railroad era and railroads typically installed telegraph lines along their routes for communicating with the trains.[111]
",Technology
"Teleprinters appeared in 1910[112] and had replaced between 80 and 90% of Morse code operators by 1929.  It is estimated that one teletypist replaced 15 Morse code operators.[61]
",Technology
"The early use of telephones was primarily for business.  Monthly service cost about one third of the average worker's earnings.[24] The telephone along with trucks and the new road networks allowed businesses to reduce inventory sharply during the 1920s.[53]
",Technology
"Telephone calls were handled by operators using switchboards until the automatic switchboard was introduced in 1892.   By 1929, 31.9% of the Bell system was automatic.[61]
",Technology
"Automatic telephone switching originally used electro-mechanical switches controlled by vacuum tube devices, which consumed a large amount of electricity.  Call volume eventually grew so fast that it was feared the telephone system would consume all electricity production, prompting Bell Labs to begin research on the transistor.[113]
",Technology
"After WWII microwave transmission began being used for long distance telephony and transmitting television programming to local stations for rebroadcast.
",Technology
"The diffusion of telephony to households was mature by the arrival of fiber optic communications in the late 1970s.  Fiber optics greatly increased the transmission capacity of information over previous copper wires and further lowered the cost of long distance communication.[114]
",Technology
"Communications satellites came into use in the 1960s and today carry a variety of information including credit card transaction data, radio, television and telephone calls.[111]  The Global Positioning System (GPS) operates on signals from satellites.
",Technology
"Fax (short for facsimile) machines of various types had been in existence since the early 1900s but became widespread beginning in the mid-1970s.
",Technology
"Before public water was supplied to households it was necessary for someone annually to haul up to 10,000 gallons of water to the average household.[115]
",Technology
"Natural gas began being supplied to households in the late 19th century.
",Technology
"Household appliances followed household electrification in the 1920s, with consumers buying electric ranges, toasters, refrigerators and washing machines.  As a result of appliances and convenience foods, time spent on meal preparation and clean up, laundry and cleaning decreased from 58 hours/week in 1900 to 18 hours/week by 1975.  Less time spent on housework allowed more women to enter the labor force.[116]
",Technology
"Automation means automatic control, meaning a process is run with minimum operator intervention.  Some of the various levels of automation are: mechanical methods, electrical relay, feedback control with a controller and computer control.  Common applications of automation are for controlling temperature, flow and pressure. Automatic speed control is important in many industrial applications, especially in sectional drives, such as found in metal rolling and paper drying.[117]
",Technology
"The earliest applications of process control were mechanisms that adjusted the gap between mill stones for grinding grain and for keeping windmills facing into the wind.  The centrifugal governor used for adjusting the mill stones was copied by James Watt for controlling speed of steam engines in response to changes in heat load to the boiler; however, if the load on the engine changed the governor only held the speed steady at the new rate.  It took much development work to achieve the degree of steadiness necessary to operate textile machinery.[118]  A mathematical analysis of control theory was first developed by James Clerk Maxwell.  Control theory was developed to its ""classical"" form by the 1950s.[119] See: Control theory#History
",Technology
"Factory electrification brought simple electrical controls such as ladder logic, whereby push buttons could be used to activate relays to engage motor starters.  Other controls such as interlocks, timers and limit switches could be added to the circuit.
",Technology
"Today automation usually refers to feedback control.  An example is cruise control on a car, which applies continuous correction when a sensor on the controlled variable (Speed in this example) deviates from a set-point and can respond in a corrective manner to hold the setting.  Process control is the usual form of automation that allows industrial operations like oil refineries, steam plants generating electricity or paper mills to be run with a minimum of manpower, usually from a number of control rooms.
",Technology
"The need for instrumentation grew with the rapidly growing central electric power stations after the First World War.  Instrumentation was also important for heat treating ovens, chemical plants and refineries.  Common instrumentation was for measuring temperature, pressure or flow.  Readings were typically recorded on circle charts or strip charts.  Until the 1930s control was typically ""open loop"", meaning that it did not use feedback.  Operators made various adjustments by such means as turning handles on valves.[120]  If done from a control room a message could be sent to an operator in the plant by color coded light, letting him know whether to increase or decrease whatever was being controlled.  The signal lights were operated by a switchboard, which soon became automated.[121]  Automatic control became possible with the feedback controller, which sensed the measured variable, measured the deviation from the setpoint and perhaps the rate of change and time weighted amount of deviation, compared that with the setpoint and automatically applied a calculated adjustment.  A stand-alone controller may use a combination of mechanical, pneumatic, hydraulic or electronic analogs to manipulate the controlled device.  The tendency was to use electronic controls after these were developed, but today the tendency is to use a computer to replace individual controllers.
",Technology
"By the late 1930s feedback control was gaining widespread use.[119]   Feedback control was an important technology for continuous production.
",Technology
"Automation of the telephone system allowed dialing local numbers instead of having calls placed through an operator.  Further automation allowed callers to place long distance calls by direct dial. Eventually almost all operators were replaced with automation.
",Technology
"Machine tools were automated with Numerical control (NC) in the 1950s.  This soon evolved into computerized numerical control (CNC).
",Technology
"Servomechanisms are commonly position or speed control devices that use feedback.  Understanding of these devices is covered in control theory.  Control theory was successfully applied to steering ships in the 1890s, but after meeting with personnel resistance it was not widely implemented for that application until after the First World War.  Servomechanisms are extremely important in providing automatic stability control for airplanes and in a wide variety of industrial applications.
",Technology
"Industrial robots were used on a limited scale from the 1960s but began their rapid growth phase in the mid-1980s after the widespread availability of microprocessors used for their control.  By 2000 there were over 700,000 robots worldwide.[17]
",Technology
"Early electric data processing was done by running punched cards through tabulating machines, the holes in the cards allowing electrical contact to increment electronic counters. Tabulating machines were in a category called unit record equipment, through which the flow of punched cards was arranged in a program-like sequence to allow sophisticated data processing.  Unit record equipment was widely used before the introduction of computers.
",Technology
"The usefulness of tabulating machines was demonstrated by compiling the 1890 U.S. census, allowing the census to be processed in less than a year and with great labor savings compared to the estimated 13 years by the previous manual method.[122]
",Technology
"The first digital computers were more productive than tabulating machines, but not by a great amount.  Early computers used thousands of vacuum tubes (thermionic valves) which used a lot of electricity and constantly needed replacing.  By the 1950s the vacuum tubes were replaced by transistors which were much more reliable and used relatively little electricity.  By the 1960s thousands of transistors and other electronic components could be manufactured on a silicon semiconductor wafer as integrated circuits, which are universally used in today's computers.
",Technology
"Computers used paper tape and punched cards for data and programming input until the 1980s when it was still common to receive monthly utility bills printed on a punched card that was returned with the customer’s payment.
",Technology
"In 1973 IBM introduced point of sale (POS) terminals in which electronic cash registers were networked to the store mainframe computer. By the 1980s bar code readers were added. These technologies automated inventory management. Wal-Mart was an early adopter of POS.  The Bureau of Labor Statistics estimated  that bar code scanners at checkout increased ringing speed by 30% and reduced labor requirements of cashiers and baggers by 10-15%.[123]
",Technology
"Data storage became better organized after the development of relational database software that allowed data to be stored in different tables.  For example, a theoretical airline may have numerous tables such as: airplanes, employees, maintenance contractors, caterers, flights, airports, payments, tickets, etc. each containing a narrower set of more specific information than would a flat file, such as a spreadsheet.  These tables are related by common data fields called keys. (See: Relational model) Data can be retrieved in various specific configurations by posing a query without having to pull up a whole table.  This, for example, makes it easy to find a passenger's seat assignment by a variety of means such as ticket number or name, and provide only the queried information.  See: SQL
",Technology
"Since the mid-1990s, interactive web pages have allowed users to access various servers over Internet to engage in e-commerce such as online shopping, paying bills, trading stocks, managing bank accounts and renewing auto registrations.  This is the ultimate form of back office automation because the transaction information is transferred directly to the database.
",Technology
"Computers also greatly increased productivity of the communications sector, especially in areas like the elimination of telephone operators. In engineering, computers replaced manual drafting with CAD, with a 500% average increase in a draftsman's output.[17]  Software was developed for calculations used in designing electronic circuits, stress analysis, heat and material balances.  Process simulation software has been developed for both steady state and dynamic simulation, the latter able to give the user a very similar experience to operating a real process like a refinery or paper mill, allowing the user to optimize the process or experiment with process modifications.
",Technology
"Automated teller machines (ATM's) became popular in recent decades and self checkout at retailers appeared in the 1990s.
",Technology
"The Airline Reservations System and banking are areas where computers are practically essential. Modern military systems also rely on computers.
",Technology
"In 1959 Texaco’s Port Arthur refinery became the first chemical plant to use digital process control.[123]
",Technology
"Computers did not revolutionize manufacturing because automation, in the form of control systems, had already been in existence for decades, although computers did allow more sophisticated control, which led to improved product quality and process optimization. See: Productivity paradox
",Technology
"""The years 1929-1941 were, in the aggregate, the most technologically progressive of any comparable period in U.S. economic history."" Alexander J. Field[124]",Technology
"""As industrialization has proceeded, its effects, relatively speaking, have become less, not more, revolutionary""....""There has, in effect, been a general progression in industrial commodities from a deficiency to a surplus of capital relative to internal investments"".[125]  Alan Sweezy, 1943",Technology
"U.S. productivity growth has been in long term decline since the early 1970s, with the exception of a 1996–2004 spike caused by an acceleration of Moore's law semiconductor innovation.[126][127][128][129][130] Part of the early decline was attributed to increased governmental regulation since the 1960s, including stricter environmental regulations.[131] Part of the decline in productivity growth is due to exhaustion of opportunities, especially as the traditionally high productivity sectors decline in size.[132][133] Robert J. Gordon considered productivity to be ""one big wave"" that crested and is now receding to a lower level, while M. King Hubbert called the phenomenon of the great productivity gains preceding the Great Depression a ""one time event.""[134][135]
",Technology
"Because of reduced population growth in the U.S. and a peaking of productivity growth, sustained U.S. GDP growth has never returned to the 4% plus rates of the pre-World War I decades.[136][137][138]
",Technology
"The computer and computer-like semiconductor devices used in automation are the most significant productivity improving technologies developed in the final decades of the twentieth century; however, their contribution to overall productivity growth was disappointing. Most of the productivity growth occurred in the new industry computer and related industries.[124]  Economist Robert J. Gordon is among those who questioned whether computers lived up to the great innovations of the past, such as electrification.[134] This issue is known as the productivity paradox. Gordon's (2013) analysis of productivity in the U.S. gives two possible surges in growth, one during 1891–1972 and the second in 1996–2004 due to the acceleration in Moore's law-related technological innovation.[139]
",Technology
"Improvements in productivity affected the relative sizes of various economic sectors by reducing prices and employment. Agricultural productivity released labor at a time when manufacturing was growing. Manufacturing productivity growth peaked with factory electrification and automation, but still remains significant. However, as the relative size of the manufacturing sector shrank the government and service sectors, which have low productivity growth, grew.[132]
",Technology
"Chronic hunger and malnutrition were the norm for the majority of the population of the world including England and France, until the latter part of the 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years, and only slightly higher in England.  The U.S. population of the time was adequately fed, were much taller and had life expectancies of 45–50 years.[140][141]  (See also: British Agricultural Revolution)
",Technology
"The gains in standards of living have been accomplished largely through increases in productivity.  In the U.S. the amount of personal consumption that could be bought with one hour of work was about $3.00 in 1900 and increased to about $22 by 1990, measured in 2010 dollars.[116]  For comparison, a U. S. worker today earns more (in terms of buying power) working for ten minutes than subsistence workers, such as the English mill workers that Fredrick Engels wrote about in 1844, earned in a 12-hour day.
",Technology
"As a result of productivity increases, the work week declined considerably over the 19th century.[142][143]  By the 1920s the average work week in the U.S. was 49 hours, but the work week was reduced to 40 hours (after which overtime premium was applied) as part of the National Industrial Recovery Act of 1933.
",Technology
"The Second Industrial Revolution, also known as the Technological Revolution,[1] was a phase of rapid industrialization in the final third of the 19th century and the beginning of the 20th. The First Industrial Revolution, which ended in the early to mid 1800s, was punctuated by a slowdown in macroinventions[clarification needed] before the Second Industrial Revolution in 1870. Though a number of its characteristic events can be traced to earlier innovations in manufacturing, such as the establishment of a machine tool industry, the development of methods for manufacturing interchangeable parts and the invention of the Bessemer Process to produce steel, the Second Industrial Revolution is generally dated between 1870 and 1914 (the start of World War I).[2]
",Technology
"Advancements in manufacturing and production technology enabled the widespread adoption of preexisting technological systems such as telegraph and railroad networks, gas and water supply, and sewage systems, which had earlier been concentrated to a few select cities. The enormous expansion of rail and telegraph lines after 1870 allowed unprecedented movement of people and ideas, which culminated in a new wave of globalization. In the same time period, new technological systems were introduced, most significantly electrical power and telephones. The Second Industrial Revolution continued into the 20th century with early factory electrification and the production line, and ended at the start of World War I.
",Technology
"The Second Industrial Revolution was a period of rapid industrial development, primarily in Britain, Germany and the United States, but also in France, the Low Countries, Italy and Japan. It followed on from the First Industrial Revolution that began in Britain in the late 18th century that then spread throughout Western Europe and later North America. It was characterized by the build out of railroads, large-scale iron and steel production, widespread use of machinery in manufacturing, greatly increased use of steam power, widespread use of the telegraph, use of petroleum and the beginning of electrification.  It also was the period during which modern organizational methods for operating large scale businesses over vast areas came into use.
",Technology
"The concept was introduced by Patrick Geddes, Cities in Evolution (1910), but David Landes' use of the term in a 1966 essay and in The Unbound Prometheus (1972) standardized scholarly definitions of the term, which was most intensely promoted by Alfred Chandler (1918–2007). However, some continue to express reservations about its use.[3]
",Technology
"Landes (2003) stresses the importance of new technologies, especially, the internal combustion engine and petroleum, new materials and substances, including alloys and chemicals, electricity and communication technologies (such as the telegraph, telephone and radio).
",Technology
"Vaclav Smil called the period 1867–1914 ""The Age of Synergy"" during which most of the great innovations were developed since the inventions and innovations were engineering and science-based.[4]
",Technology
"A synergy between iron and steel, railroads and coal developed at the beginning of the Second Industrial Revolution.  Railroads allowed cheap transportation of materials and products, which in turn led to cheap rails to build more roads.  Railroads also benefited from cheap coal for their steam locomotives.  This synergy led to the laying of 75,000 miles of track in the U.S. in the 1880s, the largest amount anywhere in world history.[5]
",Technology
"The hot blast technique, in which the hot flue gas from a blast furnace is used to preheat combustion air blown into a blast furnace, was invented and patented by James Beaumont Neilson in 1828 at Wilsontown Ironworks in Scotland. Hot blast was the single most important advance in fuel efficiency of the blast furnace as it greatly reduced the fuel consumption for making pig iron, and was one of the most important technologies developed during the Industrial Revolution.[6] Falling costs for producing wrought iron coincided with the emergence of the railway in the 1830s.
",Technology
"The early technique of hot blast used iron for the regenerative heating medium.  Iron caused problems with expansion and contraction, which stressed the iron and caused failure. Edward Alfred Cowper developed the Cowper stove in 1857.[7]  This stove used firebrick as a storage medium, solving the expansion and cracking problem.  The Cowper stove was also capable of producing high heat, which resulted in very high throughput of blast furnaces.  The Cowper stove is still used in today's blast furnaces.
",Technology
"With the greatly reduced cost of producing pig iron with coke using hot blast, demand grew dramatically and so did the size of blast furnaces.[8][9]
",Technology
"The Bessemer process, invented by Sir Henry Bessemer, allowed the mass-production of steel, increasing the scale and speed of production of this vital material, and decreasing the labor requirements. The key principle was the removal of excess carbon and other impurities from pig iron by oxidation with air blown through the molten iron. The oxidation also raises the temperature of the iron mass and keeps it molten.
",Technology
"The ""acid"" Bessemer process had a serious limitation in that it required relatively scarce hematite ore[10] which is low in phosphorus. Sidney Gilchrist Thomas developed a more sophisticated process to eliminate the phosphorus from iron. Collaborating with his cousin, Percy Gilchrist a chemist at the Blaenavon Ironworks, Wales, he patented his process in 1878;[11] Bolckow Vaughan & Co. in Yorkshire was the first company to use his patented process.[12] His process was especially valuable on the continent of Europe, where the proportion of phosphoric iron was much greater than in England, and both in Belgium and in Germany the name of the inventor became more widely known than in his own country. In America, although non-phosphoric iron largely predominated, an immense interest was taken in the invention.[12]
",Technology
"The next great advance in steel making was the Siemens-Martin process. Sir Charles William Siemens developed his regenerative furnace in the 1850s, for which he claimed in 1857 to able to recover enough heat to save 70–80% of the fuel. The furnace operated at a high temperature by using regenerative preheating of fuel and air for combustion. Through this method, an open-hearth furnace can reach temperatures high enough to melt steel, but Siemens did not initially use it in that manner.
",Technology
"French engineer Pierre-Émile Martin was the first to take out a license for the Siemens furnace and apply it to the production of steel in 1865. The Siemens-Martin process complemented rather than replaced the Bessemer process.  Its main advantages were that it did not expose the steel to excessive nitrogen (which would cause the steel to become brittle), it was easier to control, and that it permitted the melting and refining of large amounts of scrap steel, lowering steel production costs and recycling an otherwise troublesome waste material. It became the leading steel making process by the early 20th century.
",Technology
"The availability of cheap steel allowed building larger bridges, railroads, skyscrapers, and ships.[13] Other important steel products—also made using the open hearth process—were steel cable, steel rod and sheet steel which enabled large, high-pressure boilers and high-tensile strength steel for machinery which enabled much more powerful engines, gears and axles than were previously possible. With large amounts of steel it became possible to build much more powerful guns and carriages, tanks, armored fighting vehicles and naval ships.
",Technology
"The increase in steel production from the 1860s meant that railroads could finally be made from steel at a competitive cost. Being a much more durable material, steel steadily replaced iron as the standard for railway rail, and due to its greater strength, longer lengths of rails could now be rolled. Wrought iron was soft and contained flaws caused by included dross.  Iron rails could also not support heavy locomotives and was damaged by hammer blow. The first to make durable rails of steel rather than wrought iron was Robert Forester Mushet at the Darkhill Ironworks, Gloucestershire in 1857.
",Technology
"The first of his steel rails was sent to Derby Midland railway station.  They were laid at part of the station approach where the iron rails had to be renewed at least every six months, and occasionally every three. Six years later, in 1863, the rail seemed as perfect as ever, although some 700 trains had passed over it daily.[14] This provided the basis for the accelerated construction of rail transportation throughout the world in the late nineteenth century.  Steel rails lasted over ten times longer than did iron,[15] and with the falling cost of steel, heavier weight rails were used.  This allowed the use of more powerful locomotives, which could pull longer trains, and longer rail cars, all of which greatly increased the productivity of railroads.[16] Rail became the dominant form of transport infrastructure throughout the industrialized world,[17] producing a steady decrease in the cost of shipping seen for the rest of the century.[18]
",Technology
"The theoretical and practical basis for the harnessing of electric power was laid by the scientist and experimentalist Michael Faraday. Through his research on the magnetic field around a conductor carrying a direct current, Faraday established the basis for the concept of the electromagnetic field in physics.[19][20] His inventions of electromagnetic rotary devices were the foundation of the practical use of electricity in technology.
",Technology
"In 1881, Sir Joseph Swan, inventor of the first feasible incandescent light bulb, supplied about 1,200 Swan incandescent lamps to the Savoy Theatre in the City of Westminster, London, which was the first theatre, and the first public building in the world, to be lit entirely by electricity.[21][22] Swan's lightbulb had already been used in 1879 to light Mosley Street, in Newcastle upon Tyne, the first electrical street lighting installation in the world.[23][24] This set the stage for the electrification of industry and the home. The first large scale central distribution supply plant was opened at Holborn Viaduct in London in 1882[25] and later at Pearl Street Station in New York City.[26]
",Technology
"The first modern power station in the world was built by the English electrical engineer Sebastian de Ferranti at Deptford. Built on an unprecedented scale and pioneering the use of high voltage (10,000V) alternating current, it generated 800 kilowatts and supplied central London. On its completion in 1891 it supplied high-voltage AC power that was then ""stepped down"" with transformers for consumer use on each street. Electrification allowed the final major developments in manufacturing methods of the Second Industrial Revolution, namely the assembly line and mass production.[27]
",Technology
"Electrification was called ""the most important engineering achievement of the 20th century"" by the National Academy of Engineering.[28] Electric lighting in factories greatly improved working conditions, eliminating the heat and pollution caused by gas lighting, and reducing the fire hazard to the extent that the cost of electricity for lighting was often offset by the reduction in fire insurance premiums. Frank J. Sprague developed the first successful DC motor in 1886. By 1889 110 electric street railways were either using his equipment or in planning.  The electric street railway became a major infrastructure before 1920.  The AC (Induction motor) was developed in the 1890s and soon began to be used in the electrification of industry.[29] Household electrification did not become common until the 1920s, and then only in cities. Fluorescent lighting was commercially introduced at the 1939 World's Fair.
",Technology
"Electrification also allowed the inexpensive production of electro-chemicals, such as aluminium, chlorine, sodium hydroxide, and magnesium.[30]
",Technology
"The use of machine tools began with the onset of the First Industrial Revolution. The increase in mechanization required more metal parts, which were usually made of cast iron or wrought iron—and hand working lacked precision and was a slow and expensive process. One of the first machine tools was John Wilkinson's boring machine, that bored a precise hole in James Watt's first steam engine in 1774. Advances in the accuracy of machine tools can be traced to Henry Maudslay and refined by Joseph Whitworth. Standardization of screw threads began with Henry Maudslay around 1800, when the modern screw-cutting lathe made interchangeable V-thread machine screws a practical commodity.
",Technology
"In 1841, Joseph Whitworth created a design that, through its adoption by many British railroad companies, became the world's first national machine tool standard called British Standard Whitworth.[31] During the 1840s through 1860s, this standard was often used in the United States and Canada as well, in addition to myriad intra- and inter-company standards.
",Technology
"The importance of machine tools to mass production is shown by the fact that production of the Ford Model T used 32,000 machine tools, most of which were powered by electricity.[32] Henry Ford is quoted as saying that mass production would not have been possible without electricity because it allowed placement of machine tools and other equipment in the order of the work flow.[33]
",Technology
"The first paper making machine was the Fourdrinier machine, built by Sealy and Henry Fourdrinier, stationers in London. In 1800, Matthias Koops, working in London, investigated the idea of using wood to make paper, and began his printing business a year later. However, his enterprise was unsuccessful due to the prohibitive cost at the time.[34][35][36]
",Technology
"It was in the 1840s,  that Charles Fenerty in Nova Scotia and Friedrich Gottlob Keller in Saxony both invented a successful machine which extracted the fibres from wood (as with rags) and from it, made paper. This started a new era for paper making,[37] and, together with the invention of the fountain pen and the mass-produced pencil of the same period, and in conjunction with the advent of the steam driven rotary printing press, wood based paper caused a major transformation of the 19th century economy and society in industrialized countries. With the introduction of cheaper paper, schoolbooks, fiction, non-fiction, and newspapers became gradually available by 1900. Cheap wood based paper also allowed keeping personal diaries or writing letters and so, by 1850, the clerk, or writer, ceased to be a high-status job. By the 1880s chemical processes for paper manufacture were in use, becoming dominant by 1900.
",Technology
"The petroleum industry, both production and refining, began in 1848 with the first oil works in Scotland. The chemist James Young set up a small business refining the crude oil in 1848. Young found that by slow distillation he could obtain a number of useful liquids from it, one of which he named ""paraffine oil"" because at low temperatures it congealed into a substance resembling paraffin wax.[38] In 1850 Young built the first truly commercial oil-works and oil refinery in the world at Bathgate, using oil extracted from locally mined torbanite, shale, and bituminous coal to manufacture naphtha and lubricating oils; paraffin for fuel use and solid paraffin were not sold till 1856.
",Technology
"Cable tool drilling was developed in ancient China and was used for drilling brine wells.  The salt domes also held natural gas, which some wells produced and which was used for evaporation of the brine.  Chinese well drilling technology was introduced to Europe in 1828.[39]
",Technology
"Although there were many efforts in the mid-19th century to drill for oil Edwin Drake's 1859 well near Titusville, Pennsylvania, is considered the first ""modern oil well"".[40] Drake's well touched off a major boom in oil production in the United States.[41]  Drake learned of cable tool drilling from Chinese laborers in the U. S.[42]  The first primary product was kerosene for lamps and heaters.[30][43] Similar developments around Baku fed the European market.
",Technology
"Kerosene lighting was much more efficient and less expensive than vegetable oils, tallow and whale oil.  Although town gas lighting was available in some cities, kerosene produced a brighter light until the invention of the gas mantle. Both were replaced by electricity for street lighting following the 1890s and for households during the 1920s. Gasoline was an unwanted byproduct of oil refining until automobiles were mass-produced after 1914, and gasoline shortages appeared during World War I. The invention of the Burton process for thermal cracking doubled the yield of gasoline, which helped alleviate the shortages.[43]
",Technology
"Synthetic dye was discovered by English chemist William Henry Perkin in 1856. At the time, chemistry was still in a quite primitive state; it was still a difficult proposition to determine the arrangement of the elements in compounds and chemical industry was still in its infancy. Perkin's accidental discovery was that aniline could be partly transformed into a crude mixture which when extracted with alcohol produced a substance with an intense purple colour. He scaled up production of the new ""mauveine"", and commercialized it as the world's first synthetic dye.[44]
",Technology
"After the discovery of mauveine, many new aniline dyes appeared (some discovered by Perkin himself), and factories producing them were constructed across Europe.
Towards the end of the century, Perkin and other British companies found their research and development efforts increasingly eclipsed by the German chemical industry which became world dominant by 1914.
",Technology
"This era saw the birth of the modern ship as disparate technological advances came together.
",Technology
"The screw propeller was introduced in 1835 by Francis Pettit Smith who discovered a new way of building propellers by accident. Up to that time, propellers were literally screws, of considerable length. But during the testing of a boat propelled by one, the screw snapped off, leaving a fragment shaped much like a modern boat propeller. The boat moved faster with the broken propeller.[45] The superiority of screw against paddles was taken up by navies. Trials with Smith's SS Archimedes, the first steam driven screw, led to the famous tug-of-war competition in 1845 between the screw-driven HMS Rattler and the paddle steamer HMS Alecto; the former pulling the latter backward at 2.5 knots (4.6 km/h).
",Technology
"The first seagoing iron steamboat was built by Horseley Ironworks and named the Aaron Manby. It also used an innovative oscillating engine for power. The boat was built at Tipton using temporary bolts, disassembled for transportation to London, and reassembled on the Thames in 1822, this time using permanent rivets.
",Technology
"Other technological developments followed, including the invention of the surface condenser, which allowed boilers to run on purified water rather than salt water, eliminating the need to stop to clean them on long sea journeys. The Great Western[46]
,[47][48] built by engineer Isambard Kingdom Brunel, was the longest ship in the world at 236 ft (72 m) with a 250-foot (76 m) keel and was the first to prove that transatlantic steamship services were viable. The ship was constructed mainly from wood, but Brunel added bolts and iron diagonal reinforcements to maintain the keel's strength. In addition to its steam-powered paddle wheels, the ship carried four masts for sails.
",Technology
"Brunel followed this up with the Great Britain, launched in 1843 and considered the first modern ship built of metal rather than wood, powered by an engine rather than wind or oars, and driven by propeller rather than paddle wheel.[49] Brunel's vision and engineering innovations made the building of large-scale, propeller-driven, all-metal steamships a practical reality, but the prevailing economic and industrial conditions meant that it would be several decades before transoceanic steamship travel emerged as a viable industry.
",Technology
"Highly efficient multiple expansion steam engines began being used on ships, allowing them to carry less coal than freight.[50] The oscillating engine was first built by Aaron Manby and Joseph Maudslay in the 1820s as a type of direct-acting engine that was designed to achieve further reductions in engine size and weight. Oscillating engines had the piston rods connected directly to the crankshaft, dispensing with the need for connecting rods. In order to achieve this aim, the engine cylinders were not immobile as in most engines, but secured in the middle by trunnions which allowed the cylinders themselves to pivot back and forth as the crankshaft rotated, hence the term oscillating.
",Technology
"It was John Penn, engineer for the Royal Navy who perfected the oscillating engine. One of his earliest engines was the grasshopper beam engine. In 1844 he replaced the engines of the Admiralty yacht, HMS Black Eagle with oscillating engines of double the power, without increasing either the weight or space occupied, an achievement which broke the naval supply dominance of Boulton & Watt and Maudslay, Son & Field. Penn also introduced the trunk engine for driving screw propellers in vessels of war. HMS Encounter (1846) and HMS Arrogant (1848) were the first ships to be fitted with such engines and such was their efficacy that by the time of Penn's death in 1878, the engines had been fitted in 230 ships and were the first mass-produced, high-pressure and high-revolution marine engines.[51]
",Technology
"The revolution in naval design led to the first modern battleships in the 1870s, evolved from the ironclad design of the 1860s. The Devastation-class turret ships were built for the British Royal Navy as the first class of ocean-going capital ship that did not carry sails, and the first whose entire main armament was mounted on top of the hull rather than inside it.
",Technology
"The vulcanization of rubber, by American Charles Goodyear and Englishman Thomas Hancock in the 1840s paved the way for a growing rubber industry, especially the manufacture of rubber tyres[52]
",Technology
"John Boyd Dunlop developed the first practical pneumatic tyre in 1887 in South Belfast. Willie Hume demonstrated the supremacy of Dunlop's newly invented pneumatic tyres in 1889, winning the tyre's first ever races in Ireland and then England.[53]
[54] Dunlop's development of the pneumatic tyre arrived at a crucial time in the development of road transport and commercial production began in late 1890.
",Technology
"The modern bicycle was designed by the English engineer Harry John Lawson in 1876, although it was John Kemp Starley who produced the first commercially successful safety bicycle a few years later.[55] Its popularity soon grew, causing the bike boom of the 1890s.
",Technology
"Road networks improved greatly in the period, using the Macadam method pioneered by Scottish engineer John Loudon McAdam, and hard surfaced roads were built around the time of the bicycle craze of the 1890s. Modern tarmac was patented by British civil engineer Edgar Purnell Hooley in 1901.[56]
",Technology
"German inventor Karl Benz patented the world's first automobile in 1886. It featured wire wheels (unlike carriages' wooden ones)[57] with a four-stroke engine of his own design between the rear wheels, with a very advanced coil ignition [58] and evaporative cooling rather than a radiator.[58] Power was transmitted by means of two roller chains to the rear axle. It was the first automobile entirely designed as such to generate its own power, not simply a motorized-stage coach or horse carriage.
",Technology
"Benz began to sell the vehicle (advertising it as the Benz Patent Motorwagen) in the late summer of 1888, making it the first commercially available automobile in history.
",Technology
"Henry Ford built his first car in 1896 and worked as a pioneer in the industry, with others who would eventually form their own companies, until the founding of Ford Motor Company in 1903.[27] Ford and others at the company struggled with ways to scale up production in keeping with Henry Ford's vision of a car designed and manufactured on a scale so as to be affordable by the average worker.[27] The solution that Ford Motor developed was a completely redesigned factory with machine tools and special purpose machines that were systematically positioned in the work sequence. All unnecessary human motions were eliminated by placing all work and tools within easy reach, and where practical on conveyors, forming the assembly line, the complete process being called mass production. This was the first time in history when a large, complex product consisting of 5000 parts had been produced on a scale of hundreds of thousands per year.[27][32] The savings from mass production methods allowed the price of the Model T to decline from $780 in 1910 to $360 in 1916. In 1924 2 million T-Fords were produced and retailed $290 each.[59]
",Technology
"Applied science opened many opportunities. By the middle of the 19th century there was a scientific understanding of chemistry and a fundamental understanding of thermodynamics and by the last quarter of the century both of these sciences were near their present-day basic form. Thermodynamic principles were used in the development of physical chemistry. Understanding chemistry greatly aided the development of basic inorganic chemical manufacturing and the aniline dye industries.
",Technology
"The science of metallurgy was advanced through the work of Henry Clifton Sorby and others. Sorby pioneered the study of iron and steel under microscope, which paved the way for a scientific understanding of metal and the mass-production of steel. In 1863 he used etching with acid to study the microscopic structure of metals and was the first to understand that a small but precise quantity of carbon gave steel its strength.[60] This paved the way for Henry Bessemer and Robert Forester Mushet to develop the method for mass-producing steel.
",Technology
"Other processes were developed for purifying various elements such as chromium, molybdenum, titanium, vanadium and nickel which could be used for making alloys with special properties, especially with steel. Vanadium steel, for example, is strong and fatigue resistant, and was used in half the automotive steel.[61]  Alloy steels were used for ball bearings which were used in large scale bicycle production in the 1880s.  Ball and roller bearings also began being used in machinery.  Other important alloys are used in high temperatures, such as steam turbine blades, and stainless steels for corrosion resistance.
",Technology
"The work of Justus von Liebig and August Wilhelm von Hofmann laid the groundwork for modern industrial chemistry. Liebig is considered the ""father of the fertilizer industry"" for his discovery of nitrogen as an essential plant nutrient and went on to establish Liebig's Extract of Meat Company which produced the Oxo meat extract. Hofmann headed a school of practical chemistry in London, under the style of the Royal College of Chemistry, introduced modern conventions for molecular modeling and taught Perkin who discovered the first synthetic dye.
",Technology
"The science of thermodynamics was developed into its modern form by Sadi Carnot, William Rankine, Rudolf Clausius, William Thomson, James Clerk Maxwell, Ludwig Boltzmann and J. Willard Gibbs. These scientific principles were applied to a variety of industrial concerns, including improving the efficiency of boilers and steam turbines. The work of Michael Faraday and others was pivotal in laying the foundations of the modern scientific understanding of electricity.
",Technology
"Scottish scientist James Clerk Maxwell was particularly influential—his discoveries ushered in the era of modern physics.[62] His most prominent achievement was to formulate a set of equations that described electricity, magnetism, and optics as manifestations of the same phenomenon, namely the electromagnetic field.[63] The unification of light and electrical phenomena led to the prediction of the existence of radio waves and was the basis for the future development of radio technology by Hughes, Marconi and others.[64]
",Technology
"Maxwell himself developed the first durable colour photograph in 1861 and published the first scientific treatment of control theory.[65][66] Control theory is the basis for process control, which is widely used in automation, particularly for process industries, and for controlling ships and airplanes.[67] Control theory was developed to analyze the functioning of centrifugal governors on steam engines.  These governors came into use in the late 18th century on wind and water mills to correctly position the gap between mill stones, and were adapted to steam engines by James Watt.  Improved versions were used to stabilize automatic tracking mechanisms of telescopes and to control speed of ship propellers and rudders.  However, those governors were sluggish and oscillated about the set point. James Clerk Maxwell wrote a paper mathematically analyzing the actions of governors, which marked the beginning of the formal development of control theory.  The science was continually improved and evolved into an engineering discipline.
",Technology
"Justus von Liebig was the first to understand the importance of ammonia as fertilizer, and promoted the importance of inorganic minerals to plant nutrition. In England, he attempted to implement his theories commercially through a fertilizer created by treating phosphate of lime in bone meal with sulfuric acid. Another pioneer was John Bennet Lawes who began to experiment on the effects of various manures on plants growing in pots in 1837, leading to a manure formed by treating phosphates with sulphuric acid; this was to be the first product of the nascent artificial manure industry.[68]
",Technology
"The discovery of coprolites in commercial quantities in East Anglia, led Fisons and Edward Packard to develop one of the first large-scale commercial fertilizer plants at Bramford, and Snape in the 1850s. By the 1870s superphosphates produced in those factories, were being shipped around the world from the port at Ipswich.[69][70]
",Technology
"The Birkeland–Eyde process was developed by Norwegian industrialist and scientist Kristian Birkeland along with his business partner Sam Eyde in 1903,[71] but was soon replaced by the much more efficient Haber process,[72]
developed by the Nobel prize-winning chemists Carl Bosch of IG Farben and Fritz Haber in Germany.[73] The process utilized molecular nitrogen (N2) and methane (CH4) gas in an economically sustainable synthesis of ammonia (NH3). The ammonia produced in the Haber process is the main raw material for production of nitric acid.
",Technology
"The steam turbine was developed by Sir Charles Parsons in 1884. His first model was connected to a dynamo that generated 7.5 kW (10 hp) of electricity.[74] The invention of Parson's steam turbine made cheap and plentiful electricity possible and revolutionized marine transport and naval warfare.[75] By the time of Parson's death, his turbine had been adopted for all major world power stations.[76] Unlike earlier steam engines, the turbine produced rotary power rather than reciprocating power which required a crank and heavy flywheel. The large number of stages of the turbine allowed for high efficiency and reduced size by 90%. The turbine's first application was in shipping followed by electric generation in 1903.
",Technology
"The first widely used internal combustion engine was the Otto type of 1876. From the 1880s until electrification it was successful in small shops because small steam engines were inefficient and required too much operator attention.[4] The Otto engine soon began being used to power automobiles, and remains as today's common gasoline engine.
",Technology
"The diesel engine was independently designed by Rudolf Diesel and Herbert Akroyd Stuart in the 1890s using thermodynamic principles with the specific intention of being highly efficient. It took several years to perfect and become popular, but found application in shipping before powering locomotives. It remains the world's most efficient prime mover.[4]
",Technology
"The first commercial telegraph system was installed by Sir William Fothergill Cooke and Charles Wheatstone in May 1837 between Euston railway station and Camden Town in London.[77]
",Technology
"The rapid expansion of telegraph networks took place throughout the century, with the first undersea cable being built by John Watkins Brett between France and England.
The Atlantic Telegraph Company was formed in London in 1856 to undertake to construct a commercial telegraph cable across the Atlantic Ocean. This was successfully completed on 18 July 1866 by the ship SS Great Eastern, captained by Sir James Anderson after many mishaps along the away.[78] From the 1850s until 1911, British submarine cable systems dominated the world system. This was set out as a formal strategic goal, which became known as the All Red Line.[79]
",Technology
"The telephone was patented in 1876 by Alexander Graham Bell, and like the early telegraph, it was used mainly to speed business transactions.[80]
",Technology
"As mentioned above, one of the most important scientific advancements in all of history was the unification of light, electricity and magnetism through Maxwell's electromagnetic theory. A scientific understanding of electricity was necessary for the development of efficient electric generators, motors and transformers. David Edward Hughes and Heinrich Hertz both demonstrated and confirmed the phenomenon of electromagnetic waves that had been predicted by Maxwell.[4]
",Technology
"It was Italian inventor Guglielmo Marconi who successfully commercialized radio at the turn of the century.[81] He founded The Wireless Telegraph & Signal Company in Britain in 1897[82][83] and in the same year transmitted Morse code across Salisbury Plain, sent the first ever wireless communication over open sea[84] and made the first transatlantic transmission in 1901 from Poldhu, Cornwall to Signal Hill, Newfoundland. Marconi built high-powered stations on both sides of the Atlantic and began a commercial service to transmit nightly news summaries to subscribing ships in 1904.[85]
",Technology
"The key development of the vacuum tube by Sir John Ambrose Fleming in 1904 underpinned the development of modern electronics and radio broadcasting. Lee De Forest's subsequent invention of the triode allowed the amplification of electronic signals, which paved the way for radio broadcasting in the 1920s.
",Technology
"Railroads are credited with creating the modern business enterprise by scholars such as Alfred Chandler.  Previously, the management of most businesses had consisted of individual owners or groups of partners, some of whom often had little daily hands-on operations involvement.  Centralized expertise in the home office was not enough.  A railroad required expertise available across the whole length of its trackage, to deal with daily crises, breakdowns and bad weather.    A collision in Massachusetts in 1841 led to a call for safety reform.  This led to the reorganization of railroads into different departments with clear lines of management authority.  When the telegraph became available, companies built telegraph lines  along the railroads to keep track of trains.[86]
",Technology
"Railroads involved complex operations and employed extremely large amounts of capital and ran a more complicated business compared to anything previous.  Consequently, they needed better ways to track costs.  For example, to calculate rates they needed to know the cost of a ton-mile of freight.  They also needed to keep track of cars, which could go missing for months at a time.  This led to what was called ""railroad accounting"", which was later adopted by steel and other industries, and eventually became modern accounting.[87]
",Technology
"Later in the Second Industrial Revolution, Frederick Winslow Taylor and others in America developed the concept of scientific management or Taylorism. Scientific management initially concentrated on reducing the steps taken in performing work (such as bricklaying or shoveling) by using analysis such as time-and-motion studies, but the concepts evolved into fields such as industrial engineering, manufacturing engineering, and business management that helped to completely restructure[citation needed] the operations of factories, and later entire segments of the economy.
",Technology
"Taylor's core principles included:[citation needed]
",Technology
"The period from 1870 to 1890 saw the greatest increase in economic growth in such a short period as ever in previous history. Living standards improved significantly in the newly industrialized countries as the prices of goods fell dramatically due to the increases in productivity.  This caused unemployment and great upheavals in commerce and industry, with many laborers being displaced by machines and many factories, ships and other forms of fixed capital becoming obsolete in a very short time span.[50]
",Technology
"""The economic changes that have occurred during the last quarter of a century -or during the present generation of living men- have unquestionably been more important and more varied than during any period of the world's history"".[50]
",Technology
"Crop failures no longer resulted in starvation in areas connected to large markets through transport infrastructure.[50]
",Technology
"Massive improvements in public health and sanitation resulted from public health initiatives, such as the construction of the London sewerage system in the 1860s and the passage of laws that regulated filtered water supplies—(the Metropolis Water Act introduced regulation of the water supply companies in London, including minimum standards of water quality for the first time in 1852). This greatly reduced the infection and death rates from many diseases.
",Technology
"By 1870 the work done by steam engines exceeded that done by animal and human power.  Horses and mules remained important in agriculture until the development of the internal combustion tractor near the end of the Second Industrial Revolution.[88]
",Technology
"Improvements in steam efficiency, like triple-expansion steam engines, allowed ships to carry much more freight than coal, resulting in greatly increased volumes of international trade.  Higher steam engine efficiency caused the number of steam engines to increase several fold, leading to an increase in coal usage, the phenomenon being called the Jevons paradox.[89]
",Technology
"By 1890 there was an international telegraph network allowing orders to be placed by merchants in England or the US to suppliers in India and China for goods to be transported in efficient new steamships.  This, plus the opening of the Suez Canal, led to the decline of the great warehousing districts in London and elsewhere, and the elimination of many middlemen.[50]
",Technology
"The tremendous growth in productivity, transportation networks, industrial production and agricultural output lowered the prices of almost all goods.  This led to many business failures and periods that were called depressions that occurred as the world economy actually grew.[50]  See also: Long depression
",Technology
"The factory system centralized production in separate buildings funded and directed by specialists (as opposed to work at home). The division of labor made both unskilled and skilled labor more productive, and led to a rapid growth of population in industrial centers. The shift away from agriculture toward industry had occurred in Britain by the 1730s, when the percentage of the working population engaged in agriculture fell below 50%, a development that would only happen elsewhere (the Low Countries) in the 1830s and '40s. By 1890, the figure had fallen to under 10% percent and the vast majority of the British population was urbanized. This milestone was reached by the Low Countries and the US in the 1950s.[90]
",Technology
"Like the first industrial revolution, the second supported population growth and saw most governments protect their national economies with tariffs. Britain retained its belief in free trade throughout this period.  The wide-ranging social impact of both revolutions included the remaking of the working class as new technologies appeared. The changes resulted in the creation of a larger, increasingly professional, middle class, the decline of child labor and the dramatic growth of a consumer-based, material culture.[91]
",Technology
"By 1900, the leaders in industrial production was Britain with 24% of the world total, followed by the US (19%), Germany (13%), Russia (9%) and France (7%). Europe together accounted for 62%.[92]
",Technology
"The great inventions and innovations of the Second Industrial Revolution are part of our modern life.  They continued to be drivers of the economy until after WWII.  Only a few major innovations occurred in the post-war era, some of which are: computers, semiconductors, the fiber optic network and the Internet, cellular telephones, combustion turbines (jet engines) and the Green Revolution.[93] Although commercial aviation existed before WWII, it became a major industry after the war.
",Technology
"New products and services were introduced which greatly increased international trade. Improvements in steam engine design and the wide availability of cheap steel meant that slow, sailing ships were replaced with faster steamship, which could handle more trade with smaller crews. The chemical industries also moved to the forefront. Britain invested less in technological research than the U.S. and Germany, which caught up.
",Technology
"The development of more intricate and efficient machines along with mass production techniques (after 1910) greatly expanded output and lowered production costs. As a result, production often exceeded domestic demand. Among the new conditions, more markedly evident in Britain, the forerunner of Europe's industrial states, were the long-term effects of the severe Long Depression of 1873–1896, which had followed fifteen years of great economic instability. Businesses in practically every industry suffered from lengthy periods of low — and falling — profit rates and price deflation after 1873.
",Technology
"The U.S. had its highest economic growth rate in the last two decades of the Second Industrial Revolution;[95] however, population growth slowed while productivity growth peaked around the mid 20th century.  The Gilded Age in America was based on heavy industry such as factories, railroads and coal mining.  The iconic event was the opening of the First Transcontinental Railroad in 1869, providing six-day service between the East Coast and San Francisco.[96]
",Technology
"During the Gilded Age, American railroad mileage tripled between 1860 and 1880, and tripled again by 1920, opening new areas to commercial farming, creating a truly national marketplace and inspiring a boom in coal mining and steel production. The voracious appetite for capital of the great trunk railroads facilitated the consolidation of the nation's financial market in Wall Street. By 1900, the process of economic concentration had extended into most branches of industry—a few large corporations, some organized as ""trusts"" (e.g. Standard Oil), dominated in steel, oil, sugar, meatpacking, and the manufacture of agriculture machinery. Other major components of this infrastructure were the new methods for manufacturing steel, especially the Bessemer process. The first billion-dollar corporation was United States Steel, formed by financier J. P. Morgan in 1901, who purchased and consolidated steel firms built by Andrew Carnegie and others.[97]
",Technology
"Increased mechanization of industry and improvements to worker efficiency, increased the productivity of factories while undercutting the need for skilled labor. Mechanical innovations such as batch and continuous processing began to become much more prominent in factories. This mechanization made some factories an assemblage of unskilled laborers performing simple and repetitive tasks under the direction of skilled foremen and engineers. In some cases, the advancement of such mechanization substituted for low-skilled workers altogether. Both the number of unskilled and skilled workers increased, as their wage rates grew[98] Engineering colleges were established to feed the enormous demand for expertise. Together with rapid growth of small business, a new middle class was rapidly growing, especially in northern cities.[99]
",Technology
"In the early 1900s there was a disparity between the levels of employment seen in the northern and southern United States. On average, states in the North had both a higher population, and a higher rate of employment than states in the South. The higher rate of employment is easily seen by considering the 1909 rates of employment compared to the populations of each state in the 1910 census. This difference was most notable in the states with the largest populations, such as New York and Pennsylvania. Each of these states had roughly 5 percent more of the total US workforce than would be expected given their populations. Conversely, the states in the South with the best actual rates of employment, North Carolina and Georgia, had roughly 2 percent less of the workforce than one would expect from their population. When the averages of all southern states and all northern states are taken, the trend holds with the North over-performing by about 2 percent, and the South under-performing by about 1 percent.[100]
",Technology
"The German Empire came to rival Britain as Europe's primary industrial nation during this period. Since Germany industrialized later, it was able to model its factories after those of Britain, thus making more efficient use of its capital and avoiding legacy methods in its leap to the envelope of technology. Germany invested more heavily than the British in research, especially in chemistry, motors and electricity. The German concern system (known as Konzerne), being significantly concentrated, was able to make more efficient use of capital.  Germany was not weighted down with an expensive worldwide empire that needed defense. Following Germany's annexation of Alsace-Lorraine in 1871, it absorbed parts of what had been France's industrial base.[101]
",Technology
"By 1900 the German chemical industry dominated the world market for synthetic dyes. The three major firms BASF, Bayer and Hoechst produced several hundred different dyes, along with the five smaller firms. In 1913 these eight firms produced almost 90 percent of the world supply of dyestuffs, and sold about 80 percent of their production abroad.  The three major firms had also integrated upstream into the production of essential raw materials and they began to expand into other areas of chemistry such as pharmaceuticals, photographic film, agricultural chemicals and electrochemical. Top-level decision-making was in the hands of professional salaried managers, leading Chandler to call the German dye companies ""the world's first truly managerial industrial enterprises"".[102]  There were many spin offs from research—such as the pharmaceutical industry, which emerged from chemical research.[103]
",Technology
"Belgium during the Belle Époque showed the value of the railways for speeding the Second Industrial Revolution. After 1830, when it broke away from the Netherlands and became a new nation, it decided to stimulate industry. It planned and funded a simple cruciform system that connected major cities, ports and mining areas, and linked to neighboring countries. Belgium thus became the railway center of the region. The system was soundly built along British lines, so that profits were low but the infrastructure necessary for rapid industrial growth was put in place.[104]
",Technology
"There have been other times that have been called ""second industrial revolution"". Industrial revolutions may be renumbered by taking earlier developments, such as the rise of medieval technology in the 12th century, or of ancient Chinese technology during the Tang Dynasty, or of ancient Roman technology, as first. ""Second industrial revolution"" has been used in the popular press and by technologists or industrialists to refer to the changes following the spread of new technology after World War I.
",Technology
"Excitement and debate over the dangers and benefits of the Atomic Age were more intense and lasting than those over the Space age but both were predicted to lead to another industrial revolution. At the start of the 21st century[105] the term ""second industrial revolution"" has been used to describe the anticipated effects of hypothetical molecular nanotechnology systems upon society. In this more recent scenario, they would render the majority of today's modern manufacturing processes obsolete, transforming all facets of the modern economy.
",Technology
"
",Technology
"The Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power, the development of machine tools and the rise of the factory system. The Industrial Revolution also led to an unprecedented rise in the rate of population growth.
",Technology
"Textiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested. The textile industry was also the first to use modern production methods.[1]:40
",Technology
"The Industrial Revolution began in Great Britain, and many of the technological innovations were of British origin.[2][3]  By the mid-18th century Britain was the world's leading commercial nation,[4] controlling a global trading empire with colonies in North America and the Caribbean, and with some political influence on the Indian subcontinent, through the activities of the East India Company.[5] The development of trade and the rise of business were major causes of the Industrial Revolution.[1]:15
",Technology
"The Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth.  Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.[6][7][8]
",Technology
"GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy,[9] while the Industrial Revolution began an era of per-capita economic growth in capitalist economies.[10] Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants.[11]
",Technology
"Although the structural change from agriculture to industry is widely associated with Industrial Revolution, in United Kingdom it was already almost complete by 1760.[12]
",Technology
"The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes.[13][14][15][16] Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s,[13] while T. S. Ashton held that it occurred roughly between 1760 and 1830.[14] Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s,[17] with high rates of growth in steam power and iron production occurring after 1800.  Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.[1]
",Technology
"An economic recession occurred from the late 1830s to the early 1840s when the adoption of the original innovations of the Industrial Revolution, such as mechanized spinning and weaving, slowed and their markets matured. Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies, such as the electrical telegraph, widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth.  Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution.  These new innovations included new steel making processes, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories.[1][18][19][20]
",Technology
"The earliest recorded use of the term ""Industrial Revolution"" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise.[21] In his 1976 book Keywords: A Vocabulary of Culture and Society, Raymond Williams states in the entry for ""Industry"": ""The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century."" The term Industrial Revolution applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of la révolution industrielle.[22] Friedrich Engels in The Condition of the Working Class in England in 1844 spoke of ""an industrial revolution, a revolution which at the same time changed the whole of civil society"".  However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.[23]
",Technology
"Some historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term revolution is a misnomer. This is still a subject of debate among some historians.
",Technology
"The commencement of the Industrial Revolution is closely linked to a small number of innovations,[24] beginning in the second half of the 18th century.  By the 1830s the following gains had been made in important technologies:
",Technology
"In 1750 Britain imported 2.5 million pounds of raw cotton, most of which was spun and woven by cottage industry in Lancashire. The work was done by hand in workers' homes or occasionally in shops of master weavers. In 1787 raw cotton consumption was 22 million pounds, most of which was cleaned, carded and spun on machines.[1]:41–42 The British textile industry used 52 million pounds of cotton in 1800, which increased to 588 million pounds in 1850.[32]
",Technology
"The share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801 and 22.4% in 1831.  Value added by the British woollen industry was 14.1% in 1801.  Cotton factories in Britain numbered approximately 900 in 1797.  In 1760 approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800.  In 1781 cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800.  In 1800 less than 0.1% of world cotton cloth was produced on machinery invented in Britain.  In 1788 there were 50,000 spindles in Britain, rising to 7 million over the next 30 years.[33]
",Technology
"Wages in Lancashire, a core region for cottage industry and later factory spinning and weaving, were about six times those in India in 1770, when overall productivity in Britain was about three times higher than in India.[33]
",Technology
"Parts of India, China, Central America, South America and the Middle-East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD. In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption.  In the 15th century China began to require households to pay part of their taxes in cotton cloth. By the 17th century almost all Chinese wore cotton clothing.  Almost everywhere cotton cloth could be used as a medium of exchange.  In India a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers.  Some merchants also owned small weaving workshops. India produced a variety of cotton cloth, some of exceptionally fine quality.[33]
",Technology
"Cotton was a difficult raw material for Europe to obtain before it was grown on colonial plantations in the Americas.[33]  The early Spanish explorers found Native Americans growing unknown species of excellent quality cotton: sea island cotton (Gossypium barbadense) and upland green seeded cotton Gossypium hirsutum.  Sea island cotton grew in tropical areas and on barrier islands of Georgia and South Carolina, but did poorly inland.  Sea island cotton began being exported from Barbados in the 1650s. Upland green seeded cotton grew well on inland areas of the southern U.S., but was not economical because of the difficulty of removing seed, a problem solved by the cotton gin.[19]:157 A strain of cotton seed brought from Mexico to Natchez, Mississippi, USA in 1806 became the parent genetic material for over 90% of world cotton production today; it produced bolls that were three to four times faster to pick.[33]
",Technology
"The Age of Discovery was followed by a period of colonialism beginning around the 16th century. Following the discovery of a trade route to India around southern Africa by the Portuguese, the Dutch established the Verenigde Oostindische Compagnie (abbr. VOC) or Dutch East India Company and the British founded the East India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region and between the Indian Ocean region and North Atlantic Europe.  One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago, where spices were purchased for sale to Southeast Asia and Europe.  By the mid-1760s cloth was over three-quarters of the East India Company's exports.  Indian textiles were in demand in North Atlantic region of Europe where previously only wool and linen were available; however, the amount of cotton goods consumed in Western Europe was minor until the early 19th century.[33]
",Technology
"By 1600 Flemish refugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established; however, they were left alone by the guilds who did not consider cotton a threat.  Earlier European attempts at cotton spinning and weaving were in 12th-century Italy and 15th-century southern Germany, but these industries eventually ended when the supply of cotton was cut off.  The Moors in Spain grew, spun and wove cotton beginning around the 10th century.[33]
",Technology
"British cloth could not compete with Indian cloth because India's labor cost was approximately one-fifth to one-sixth that of Britain's.[17] In 1700 and 1721 the British government passed Calico Acts in order to protect the domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India.[1][34]
",Technology
"The demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft.  Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.[34]
",Technology
"On the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system.  Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials.  In the off season the women, typically farmers' wives, did the spinning and the men did the weaving.  Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.[1][34][35]:823
",Technology
"The flying shuttle, patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving.  It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box, which facilitated changing thread colors.[35]:821–22
",Technology
"Lewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham.  Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.
",Technology
"In 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles.[36]  The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting.[37] It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792,[38] and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.[35]:825–27
",Technology
"The spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright.[35]:827–30  For each spindle the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle.  The roller spacing was slightly longer than the fibre length.  Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread.  The top rollers were leather-covered and loading on the rollers was applied by a weight.  The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length.  The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain.  A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.
",Technology
"Samuel Crompton's Spinning Mule was introduced in 1779.  Mule implies a hybrid because it was a combination of the spinning jenny and the water frame, in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating.[35]:832  Crompton's mule was able to produce finer thread than hand spinning and at a lower cost.  Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive yarn in large quantities.[35]:832
",Technology
"Realising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785.  In 1776 he patented a two-man operated loom which was more conventional.[35]:834  Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813.  Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.[39]
",Technology
"The demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin.  A man using a cotton gin could remove seed from as much upland cotton in one day as would previously, working at the rate of one pound of cotton per day, have taken a woman two months to process.[19][40]
",Technology
"These advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power – first horse power and then water power—which made cotton manufacture a mechanised industry. Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.[41]
",Technology
"Although mechanization dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth, in part due to the fineness of thread made possible by the type of cotton used in India, which allowed high thread counts.  However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, eventually destroying the industry.[33]
",Technology
"The earliest European attempts at mechanized spinning were with wool; however, wool spinning proved more difficult to mechanize than cotton.  Productivity improvement in wool spinning during the Industrial Revolution was significant but was far less than that of cotton.[1][5]
",Technology
"Arguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets closely, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition.  In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.[42][43]
",Technology
"Bar iron was the commodity form of iron used as the raw material for making hardware goods such as nails, wire, hinges, horse shoes, wagon tires, chains, etc. and for structural shapes. A small amount of bar iron was converted into steel.  Cast iron was used for pots, stoves and other items where its brittleness was tolerable.  Most cast iron was refined and converted to bar iron, with substantial losses.  Bar iron was also made by the bloomery process, which was the predominant iron smelting process until the late 18th century.
",Technology
"In  the UK in 1720 there were 20,500 tons of cast iron produced with charcoal and 400 tons with coke. In 1750 charcoal iron production was 24,500 and coke iron was 2,500 tons.  In 1788 the production of charcoal cast iron was 14,000 tons while coke iron production was 54,000 tons.  In 1806 charcoal cast iron production was 7,800 tons and coke cast iron was 250,000 tons.[29]:125
",Technology
"In 1750 the UK imported 31,200 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron using charcoal and 100 tons using  coke.  In 1796 the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.[29]:125
",Technology
"A major change in the iron industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal,[45] and coal was much more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century.[1][29]:122  By 1750 coke had generally replaced charcoal in smelting of copper and lead and was in widespread use in making glass.  In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content.  Low sulfur coals were known, but they still contained harmful amounts.  Conversion of coal to coke only slightly reduces the sulfur content.[29]:122–125 A minority of coals are coking.
",Technology
"Another factor limiting the iron industry before the Industrial Revolution was the scarcity of water power to power blast bellows.  This limitation was overcome by the steam engine.[29]
",Technology
"Use of coal in iron smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (The foundry cupola is a different, and later, innovation.)
",Technology
"By 1709 Abraham Darby made progress using coke to fuel his blast furnaces at Coalbrookdale.[46] However, the coke pig iron he made was not suitable for making wrought iron and was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.
",Technology
"Coke pig iron was hardly used to produce wrought iron until 1755-56, when Darby's son Abraham Darby II built furnaces at Horsehay and Ketley where low sulfur coal was available (and not far from Coalbrookdale).  These new furnaces were equipped with water-powered bellows, the water being pumped by Newcomen steam engines.  The Newcomen engines were not attached directly to the blowing cylinders because the engines alone could not produce a steady air blast.  Abraham Darby III installed similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768.  The Dale Company used several Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.[29]:123–125
",Technology
"Steam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, iron master John Wilkinson patented a hydraulic powered blowing engine for blast furnaces.[47] The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768 that was designed by John Smeaton.[29]:124, 135  Cast iron cylinders for use with a piston were difficult to manufacture; the cylinders had to be free of holes and had to be machined smooth and straight to remove any warping.  James Watt had great difficulty trying to have a cylinder made for his first steam engine.  In 1774 John Wilkinson, who built a cast iron blowing cylinder for his iron works, invented a precision boring machine for boring cylinders. After Wilkinson bored the first successful cylinder for a Boulton and Watt steam engine in 1776, he was given an exclusive contract for providing cylinders.[19][48]  After Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.[29]:124
",Technology
"The solutions to the sulfur problem were the addition of sufficient limestone to the furnace to force sulfur into the slag and the use of low sulfur coal.  Use of lime or limestone required higher furnace temperatures to form a free-flowing slag. The increased furnace temperature made possible by improved blowing also increased the capacity of blast furnaces and allowed for increased furnace height.[29]:123–125  In addition to lower cost and greater availability, coke had other important advantages over charcoal in that it was harder and made the column of materials (iron ore, fuel, slag) flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.[49][50]
",Technology
"As cast iron became cheaper and widely available, it began being a structural material for bridges and buildings.  A famous early example was the Iron Bridge built in 1778 with cast iron produced by Abraham Darby III.[44]  However, most cast iron was converted to wrought iron.
",Technology
"Europe relied on the bloomery for most of its wrought iron until the large scale production of cast iron.  Conversion of cast iron was done in a finery forge, as it long had been.  An improved refining process known as potting and stamping was developed, but this was superseded by Henry Cort's puddling process. Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784.[1]:91  Puddling produced a structural grade iron at a relatively low cost.
",Technology
"Puddling was a means of decarburizing molten pig iron by slow oxidation in a reverberatory furnace by manually stirring it with a long rod.  The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler.  When the glob was large enough, the puddler would remove it.  Puddling was backbreaking and extremely hot work.  Few puddlers lived to be 40.[51]  Because puddling was done in a reverberatory furnace, coal or coke could be used as fuel.  The puddling process continued to be used until the late 19th century when iron was being displaced by steel.  Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Rolling was an important part of the puddling process because the grooved rollers expelled most of the molten slag and consolidated the mass of hot wrought iron. Rolling was 15 times faster at this than a trip hammer.  A different use of rolling, which was done at lower temperatures than that for expelling slag, was in the production of iron sheets, and later structural shapes such as beams, angles and rails.
",Technology
"The puddling process was improved in 1818 by Baldwyn Rogers, who replaced some of the sand lining on the reverberatory furnace bottom with iron oxide.[52]  In 1838 John Hall patented the use of roasted tap cinder (iron silicate) for the furnace bottom, greatly reducing the loss of iron through increased slag caused by a sand lined bottom. The tap cinder also tied up some phosphorus, but this was not understood at the time.[29]:166  Hall's process also used iron scale or rust, which reacted with carbon in the molten iron. Hall's process, called wet puddling, reduced losses of iron with the slag from almost 50% to around 8%.[1]:93
",Technology
"Puddling became widely used after 1800. Up to that time British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies.  Because of the increased British production, imports began to decline in 1785 and by the 1790s Britain eliminated imports and became a net exporter of bar iron.
",Technology
"Hot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron.  By using preheated combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal;[53] however, the efficiency gains continued as the technology improved.[54]  Hot blast also raised the operating temperature of furnaces, increasing their capacity.  Using less coal or coke meant introducing fewer impurities into the pig iron.  This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive;[55] however, by the end of the 19th century transportation costs fell considerably.
",Technology
"Shortly before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.
",Technology
"The supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.
",Technology
"The development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind.  In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam.  By 1815 steam power had grown to 210,000 hp.[56]
",Technology
"The first commercially successful industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its ""brand name"", The Miner's Friend). Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes.  Savery pumps continued to be produced until the late 18th century.
",Technology
"The first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were installed in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital to build, and produced upwards of 5 hp (3.7 kW). They were also used to power municipal water supply pumps.  They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.[57]
",Technology
"A fundamental change in working principles was brought about by Scotsman James Watt. With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber.  The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam.  Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency.  These improvements increased engine efficiency so that Boulton & Watts engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.
",Technology
"By 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from 5 to 10 hp (3.7 to 7.5 kW).
",Technology
"Until about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats.
",Technology
"The development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.
",Technology
"Small industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century. These included crank-powered, treadle-powered and horse-powered workshop and light industrial machinery.[58]
",Technology
"Pre-industrial machinery was built by various craftsmen – millwrights built water and windmills, carpenters made wooden framing, and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time.  As the Industrial Revolution progressed, machines with metal parts and frames became more common.  Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts and nuts. There was also the need for precision in making parts.  Precision would allow better working machinery, interchangeability of parts and standardization of threaded fasteners.
",Technology
"The demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.
",Technology
"Before the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal machine parts was kept to a minimum.  Hand methods of production were very laborious and costly and precision was difficult to achieve.[31][19]
",Technology
"The first large precision machine tool was the cylinder boring machine invented by John Wilkinson in 1774. It used for boring the large-diameter cylinders on early steam engines. Wilkinson's boring machine differed from earlier cantilevered machines used for boring cannon in that the cutting tool was mounted on a beam that ran through the cylinder being bored and was supported outside on both ends.[19]
",Technology
"The planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.[31][19]
",Technology
"Henry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He worked as an apprentice in the Royal Gun Foundry of Jan Verbruggen. In 1774 Jan Verbruggen had installed a horizontal boring machine in Woolwich which was the first industrial size Lathe in the UK. Maudslay was hired away by Joseph Bramah for the production of high-security metal locks that required precision craftsmanship.  Bramah patented a lathe that had similarities to the slide rest lathe.  Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw.  Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template.[19][35]:392–95  The slide rest lathe was called one of history's most important inventions.  Although it was not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest and change gears.[19]:31, 36
",Technology
"Maudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.[19]
",Technology
"James Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.
",Technology
"The impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century.[31]
",Technology
"In the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the U.S. economy, by value added.[59]
",Technology
"The large-scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around 100 pounds (50 kg) in each of the chambers, at least a tenfold increase.
",Technology
"The production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulphuric acid with sodium chloride to give sodium sulphate and hydrochloric acid. The sodium sulphate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulphide. Adding water separated the soluble sodium carbonate from the calcium sulphide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulphide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash,[60] and also to potash (potassium carbonate) produced from hardwood ashes.
",Technology
"These two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulphuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.
",Technology
"The development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.
",Technology
"After 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry.[61] Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.[62]
",Technology
"In 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about 1,400 °C (2,552 °F), then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel.[63] Cement was used on a large scale in the construction of the London sewerage system a generation later.
",Technology
"Another major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.
",Technology
"A new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure.
",Technology
"A machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.
",Technology
"The method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.[64]
",Technology
"The British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy.[65]  However, per-capita food supply in Europe was stagnant or declining and did not improve in some parts of Europe until the late 18th century.[66]
",Technology
"Industrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.
",Technology
"Jethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth.  This was important because the yield of seeds harvested to seeds planted at that time was around four or five.  Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact. Good quality seed drills were not produced until the mid 18th century.[67]
",Technology
"Joseph Foljambe's Rotherham plough of 1730 was the first commercially successful iron plough.[68][69][70][71]  The threshing machine,  invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour.[72]:286  It took several decades to diffuse[73] and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.
",Technology
"Machine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.[31]
",Technology
"Coal mining in Britain, particularly in South Wales, started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable.  The Cornish engine, developed in the 1810s, was much more efficient than the Watt steam engine.
",Technology
"Coal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.
",Technology
"At the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagonways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea.  The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century.  Improving sailing technologies boosted average sailing speed 50% between 1750 and 1830.[74]
",Technology
"The Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.
",Technology
"Before and during the Industrial Revolution navigation on several British rivers was improved by removing obstructions, straightening curves, widening and deepening and building navigation locks.  Britain had over 1000 miles of navigable rivers and streams by 1750.[1]:46
",Technology
"Canals and waterways allowed bulk materials to be economically transported long distances inland.  This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.[35][75]
",Technology
"Building of canals dates to ancient times.  The Grand Canal in China is ""the world's largest artificial waterway and oldest canal still in existence,"" parts of which were started between the 6th and 4th centuries BC, is 1,121 miles (1,804 km) long and links Hangzhou with Beijing.[76]
",Technology
"In the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country.  Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater.  From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£22,589,130 as of  2013[update]),[77][78] but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half.[79] This success helped inspire a period of intense canal building, known as Canal Mania.[80] New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.
",Technology
"By the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world,[81] and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.
",Technology
"Britain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.
",Technology
"France was known for having an excellent system of roads at the time of the Industrial Revolution; however, most of the roads on the European Continent and in the U.K. were in bad condition and dangerously rutted.[75][83]
",Technology
"Much of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816.[84] The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stagecoaches carried the rich, and the less wealthy could pay to ride on carriers carts.
",Technology
"Reducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England.
",Technology
"“A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton.  A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration.  Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together.  A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”[85]",Technology
"Railways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine also around 1800.
",Technology
"Wagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs. See: Metallurgy
",Technology
"Steam locomotives began being built after the introduction of high-pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water.  They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines.  A few of these early locomotives were used in mines.  Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.
",Technology
"The rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of Hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity the blast furnace.
",Technology
"On 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington.[86] The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.
",Technology
"Construction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.
",Technology
"Other developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton[87] the beginnings of a machine industry[19][88] and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years.[89]
",Technology
"Prior to the Industrial Revolution, most of the workforce was employed in agriculture, either as self-employed farmers as landowners or tenants, or as landless agricultural labourers. It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing.  Households also spun and wove for market production.  At the beginning of the Industrial Revolution India, China and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods.
",Technology
"In Britain by the 16th century the putting-out system, by which farmers and townspeople produced goods for market in their homes, often described as cottage industry, was being practiced.  Typical putting out system goods included spinning and weaving.  Merchant capitalist typically provided the raw materials, paid workers by the piece, and were responsible for the sale of the goods.  Embezzlement of supplies by workers and poor quality were common problems.  The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system.[90]
",Technology
"Some early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792,  was affordable for cottagers.[91]  Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories.
",Technology
"The majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans.  They typically worked for 12 to 14 hours per day with only Sundays off.  It was common for women take factory jobs seasonally during slack periods of farm work.  Lack of adequate transportation, long hours and poor pay made it difficult to recruit and maintain workers.[33]	Many workers, such as displaced farmers and agricultural workers, who had nothing but their labour to sell, became factory workers out of necessity.  (See: British Agricultural Revolution, Threshing machine)
",Technology
"The change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx, however, he recognized the increase in productivity made possible by technology.[92]
",Technology
"Some economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that ""for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility.""[6] Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s.[7][8] Similarly, the average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing. Real wages were not keeping up with the price of food.[93][94]
",Technology
"During the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.[95]
",Technology
"The effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s.[96] A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards.[96] During 1813–1913, there was a significant increase in worker wages.[97][98][99]
",Technology
"Chronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years and about 40 years in Britain. The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years although U.S. life expectancy declined by a few years by the mid 19th century. Food consumption per capita also declined during an episode known as the Antebellum Puzzle.[100]
",Technology
"Food supply in Great Britain was adversely affected by the Corn Laws (1815-1846).  The Corn Laws, which imposed tariffs on imported grain, were enacted to keep prices high in order to benefit domestic producers.  The Corn Laws were repealed in the early years of the Great Irish Famine.
",Technology
"The initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices.[66] In Britain and the Netherlands, food supply increased before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus.[1][72][101][102]  This condition is called the Malthusian trap, and it finally started to overcome by transportation improvements, such as canals, improved roads and steamships.[103]  Railroads and steamships were introduced near the end of the Industrial Revolution.[72]
",Technology
"The very rapid growth in population in the 19th century in the cities included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London.[104] The critical factor was financing, which was handled by building societies that dealt directly with  large contracting firms.[105][106]   Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants.[107]  People moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such as devastated Ireland in the 1840s.[108][109][110]
",Technology
"A large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the Socialist movement, The Condition of the Working Class in England in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors.  These shanty towns had narrow walkways between irregularly shaped lots and dwellings.  There were no sanitary facilities. Population density was extremely high.[111]  Not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions.
",Technology
"Conditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.
",Technology
"In The Condition of the Working Class in England in 1844 Friedrich Engels described how untreated sewage created awful odors and turned the rivers green in industrial cities.
",Technology
"In 1854 John Snow traced a cholera outbreak in Soho to fecal contamination of a public water well by a home cesspit.  Snow's findings that cholera could be spread by contaminated water took some years to be accepted, but his work led to fundamental changes in the design of public water and waste systems.
",Technology
"Pre-industrial water supply relied on gravity systems and pumping of water was done by water wheels.  Pipes were typically made of wood.  Steam powered pumps and iron pipes allowed the widespread piping of water to horse watering troughs and households.[83]
",Technology
"The invention of the paper machine and the application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which contributed to rising literacy and demands for mass political participation.
",Technology
"Consumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating.  Coffee, tea, sugar, tobacco and chocolate became affordable to many in Europe.  Watches and household clocks became popular consumer items.
",Technology
"Meeting the demands of the consumer revolution and growth in wealth of the middle classes in Britain, potter and entrepreneur Josiah Wedgwood, founder of Wedgwood fine china and porcelain, created goods such as tableware, which was starting to become a common feature on dining tables.[112]
",Technology
"The Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.[113]
",Technology
"According to Robert Hughes in The Fatal Shore, the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million.[114]  Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s.[115][116] Europe's population increased from about 100 million in 1700 to 400 million by 1900.[117]
",Technology
"The growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas.  In 1800, only 3% of the world's population lived in cities,[118] compared to nearly 50% today (the beginning of the 21st century).[119] Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.[120]
",Technology
"Women's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women.[121][122] Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th-century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.[123]
",Technology
"In a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation.[124]  Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the ""family wage economy"" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the ""family consumer economy,"" in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.[125]
",Technology
"Ideas of thrift and hard work characterized middle-class families as the Industrial Revolution swept Europe. These values were displayed in Samuel Smiles' book Self-Help, in which he states that the misery of the poorer classes was ""voluntary and self-imposed - the results of idleness, thriftlessness, intemperance, and misconduct.""[126]
",Technology
"In terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life;[127] however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children.[33]  For workers of the laboring classes, industrial life ""was a stony desert, which they had to make habitable by their own efforts.""[128] Also, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.[129]
",Technology
"Industrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed ""Cottonopolis"", and the world's first industrial city.[130] Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.[131]
",Technology
"In addition, between 1815 and 1939, 20 percent of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labor overseas, the ready availability of land, and cheap transportation. Still, many did not find a satisfactory life in their new homes, leading 7 million of them to return to Europe.[132] This mass migration had large demographic impacts: in 1800, less than one percent of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11 percent.[133] The Americas felt the brunt of this huge emigration, largely concentrated in the United States.
",Technology
"For much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.
",Technology
"In other industries, the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.
",Technology
"By 1746 an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.
",Technology
"The Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although infant mortality rates were reduced markedly.[95][135] There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.[136]
",Technology
"Child labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders,[137] 10–20% of an adult male's wage.[138] Children as young as four were employed.[138] Beatings and long hours were common, with some child coal miners and hurriers working from 4 am until 5 pm.[138] Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions.[138] Many children developed lung cancer and other diseases and died before the age of 25.[138] Workhouses would sell orphans and abandoned children as ""pauper apprentices"", working without wages for board and lodging.[138] Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape.[138] Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week. Some lost hands or limbs, others were crushed under the machines, and some were decapitated.[138] Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw.[138] Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust.[138]
",Technology
"Reports were written detailing some of the abuses, particularly in the coal mines[139] and textile factories,[140] and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.
",Technology
"Politicians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult.[138] About ten years later, the employment of children and women in mining was forbidden. Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century.[141]
",Technology
"The Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of combinations or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.
",Technology
"The main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted. One British newspaper in 1834 described unions as ""the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country...""[142]
",Technology
"In 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its Charter of reforms received over three million signatures but was rejected by Parliament without consideration.
",Technology
"Working people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.
",Technology
"Unions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.[143]
",Technology
"Eventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party.
",Technology
"The rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers, and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.
",Technology
"Unrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.
",Technology
"The traditional centers of hand textile production such as India, parts of the Middle East and later China could not withstand the competition from machine-made textiles, which over a period of decades destroyed the hand made textile industries and left millions of people without work, many of whom starved.[33]
",Technology
"The Industrial Revolution also generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.
",Technology
"Cheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. Some cotton had been grown in the West Indies, particularly in Hispaniola, but Haitian cotton production was halted by the Haitian Revolution in 1791. The invention of the cotton gin in 1792 allowed Georgia green seeded cotton to be profitable, leading to the widespread growth of cotton plantations in the United States and Brazil. In 1791 world cotton production was estimated to be 490,000,000 pounds with U.S. production accounting to 2,000,000 pounds.  By 1800 U.S. production was 35,000,000 pounds, of which 17,790,000 were exported.  In 1945 the U.S. produced seven-eights of the 1,169,600,000 pounds of world production.[19]:150
",Technology
"The Americas, particularly the U.S., had labor shortages and high priced labor, which made slavery attractive.  America's cotton plantations were highly efficient and profitable, and able to keep up with demand.[145]  The U.S. Civil war created a ""cotton famine"" that lead to increased production in other areas of the world, including new colonies in Africa.
",Technology
"The origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste.[146] The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.
",Technology
"The manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity.[147]  The industry reached the US around 1850 causing pollution and lawsuits.[148]
",Technology
"In industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms.[149] Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash, and gritty particles and to empower local authorities to impose their own regulations.[150]
",Technology
"The Industrial Revolution on Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Russian and Belgian governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.
",Technology
"Belgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.[151]
",Technology
"Wallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word ""houille"" was coined in Wallonia),[152] the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its Sillon industriel, 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, [...] there was a huge industrial development based on coal-mining and iron-making...'.[153] Philippe Raxhon wrote about the period after 1830: ""It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain.""[154] ""The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent.""[155] Michel De Coster, Professor at the Université de Liège wrote also: ""The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory [...] But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated."" [156]
",Technology
"Wallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the Sillon industriel, which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:
",Technology
"The industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres [...] at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent [...] Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.[157]",Technology
"The industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear take-off.[158] Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:
",Technology
"Based on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.[159]
",Technology
"Germany's political disunity – with three dozen states – and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France[160]
",Technology
"During the period 1790–1815 Sweden experienced two parallel economic movements: an agricultural revolution with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a protoindustrialisation, with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a consumption revolution starting in the 1820s.
",Technology
"During 1815–1850 the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.
",Technology
"During 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873.
",Technology
"During 1890–1930, Sweden experienced the second industrial revolution. New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.
",Technology
"The industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).
",Technology
"In 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882,[161] used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.
",Technology
"Modern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.[162]
",Technology
"During the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy.[163]  The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.[164][165]
",Technology
"Important American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US.  The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century.
",Technology
"Oliver Evans invented an automated flour mill in the mid-1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon.  This is considered to be the first modern materials handling system an important advance in the progress toward mass production.[31]
",Technology
"The United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the  Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.
",Technology
"Thomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era,[166] and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.[167]
",Technology
"In 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills.[168] Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of ""America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than 45 miles (72 km) from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.
",Technology
"Merchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using 5.6 miles (9.0 km) of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.
",Technology
"A major U.S. contribution to industrialization was the development of techniques to make interchangeable parts from metal.  Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms.  The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory.  Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy.  The milling machine, a fundamental machine tool, is believed to have been invented by Eli Whitney, who was a government contractor who built firearms as part of this program.  Another important invention was the Blanchard lathe, invented by Thomas Blanchard.  The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks.  The use of machinery and the techniques for producing standardized and interchangeable parts became known as the American system of manufacturing.[31]
",Technology
"Precision manufacturing techniques made it possible to build machines that mechanized the shoe industry.[169] and the watch industry. The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.
",Technology
"Steel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a ""Second Industrial Revolution"", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality.[35][170]  Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.
",Technology
"This Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.
",Technology
"The increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.
",Technology
"A new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.
",Technology
"By the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.
",Technology
"The causes of the Industrial Revolution were complicated and remain a topic for debate.  Geographic factors include Britain's vast mineral resources.  In addition to metal ores, Britain had the highest quality coal reserves known at the time.  Britain also had abundant water power and highly productive agriculture.  Britain also had numerous seaports and navigable waterways.[171]
",Technology
"Some historians believe the Industrial Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century, although feudalism began to break down after the Black Death of the mid 14th century, followed by other epidemics, until the population reached a low in the 14th century. This created labor shortages and led to falling food prices and a peak in real wages around 1500, after which population growth began reducing wages. Inflation caused by coinage debasement after 1540 followed by precious metals supply increasing from the Americas caused land rents (often long term leases that transferred to heirs on death) to fall in real terms.[172]
",Technology
"The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories.[173] The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century.[174]   A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.[175]
",Technology
"Until the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine.[176] However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.[177]
",Technology
"Lewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates.[178] He explains that the model for standardised mass production was the printing press and that ""the archetypal model for the industrial era was the clock"". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.
",Technology
"The presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them.[179] Internal tariffs were abolished by Henry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.
",Technology
"Governments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors.[180] Watt's monopoly prevented other inventors, such as Richard Trevithick, William Murdoch,  or Jonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread of steam power.[181][182]
",Technology
"One question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East (which pioneered in shipbuilding, textile production, water mills, and much more in the period between 750 and 1100[183]), or at other times like in Classical Antiquity[184] or the Middle Ages.[185] A recent account argued that Europeans have been characterized for thousands of years by a freedom-loving culture originating from the aristocratic societies of early Indo-European invaders.[186] Many historians, however, have challenged this explanation as being not only Eurocentric, but also ignoring historical context. In fact, before the Industrial Revolution, ""there existed something of a global economic parity between the most advanced regions in the world economy.""[187] These historians have suggested a number of other factors, including education, technological changes[188] (see Scientific Revolution in Europe), ""modern"" government, ""modern"" work attitudes, ecology, and culture.[189]
",Technology
"China was the world's most technologically advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before the Age of Discovery, by which time China banned imports and denied entry to foreigners. China was also a totalitarian society.  China also heavily taxed transported goods.[190][191]  
Modern estimates of per capita income in Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars[192]) whereas China, by comparison, had only 450 dollars.  India was essentially feudal, politically fragmented and not as economically advanced as Western Europe.[193]
",Technology
"Historians such as David Landes and sociologist Max Weber and Rodney Stark credit the different belief systems in Asia and Europe with dictating where the revolution occurred.[194][195]  The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews.[196] Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.[197]
",Technology
"Regarding India, the Marxist historian Rajani Palme Dutt said: ""The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain.""[198] In contrast to China, India was split up into many competing kingdoms after the decline of the Mughal Empire, with the major ones in its aftermath including the Marathas, Sikhs, Bengal Subah, and Kingdom of Mysore. In addition, the economy was highly dependent on two sectors – agriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over.[citation needed]
",Technology
"Economic historian Joel Mokyr argued that political fragmentation (the presence of a large number of European states) made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China and India by providing ""an insurance against economic and technological stagnation"".[199] China had both a printing press and movable type, and India had similar levels scientific and technological achievement as Europe in 1700, yet the Industrial Revolution would occur in Europe, not China or India. In Europe, political fragmentation was coupled with an ""integrated market for ideas"" where Europe's intellectuals used the lingua franca of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters.[200]
",Technology
"In addition, Europe's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Small groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region ""at the hub of the largest and most varied network of exchange in history,""[201] Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading historian Peter Stearns to conclude that ""Europe's Industrial Revolution stemmed in great part from Europe's ability to draw disproportionately on world resources.""[202]
",Technology
"Modern capitalism originated in the Italian city-states around the end of the first millennium.  The city-states were prosperous cities that were independent from feudal lords. They were largely republics whose governments were typically composed of merchants, manufacturers, members of guilds, bankers and financiers.  The Italian city-states built a network of branch banks in leading western European cities and introduced double entry bookkeeping.  Italian commerce was supported by schools that taught numeracy in financial calculations through abacus schools.[195]
",Technology
"Great Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution.[203]  Key factors fostering this environment were: 
",Technology
"– British historian Jeremy Black on the BBC's Why the Industrial Revolution Happened Here.[112]
",Technology
"There were two main values that really drove the Industrial Revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution.[112] These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.[204]
",Technology
"The debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution.[205] Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.[206]
",Technology
"Instead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy[207]). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position – an island separated from the rest of mainland Europe.
",Technology
"Another theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.
",Technology
"The stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism.[209] (This point is also made in Hilaire Belloc's The Servile State.)
",Technology
"The French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, Letters on the English (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. ""Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace.""[210]
",Technology
"Britain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs 3 to 5 acres (1.21 to 2.02 ha) for fodder while even early steam engines produced four times more mechanical energy.
",Technology
"In 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s.[211]  Based on science and experimentation from the continent, the steam engine was developed specifically for pumping water out of mines, many of which in Britain had been mined to below the water table. Although extremely inefficient they were economical because they used unsaleable coal.[212] Iron rails were developed to transport coal, which was a major economic sector in Britain.
",Technology
"Economic historian Robert Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur.[213] These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies.[213] However, two 2018 studies in The Economic History Review showed that wages were not particularly high in the British spinning sector or the construction sector, casting doubt on Allen's explanation.[214][215]
",Technology
"Knowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.
",Technology
"Another means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' (i.e. science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, ""They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution"".[216] Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual Transactions.
",Technology
"There were publications describing technology. Encyclopaedias such as Harris's Lexicon Technicum (1704) and Abraham Rees's Cyclopaedia (1802–1819) contain much of value. Cyclopaedia contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the Descriptions des Arts et Métiers and Diderot's Encyclopédie explained foreign methods with fine engraved plates.
",Technology
"Periodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the Annales des Mines, published accounts of travels made by French engineers who observed British methods on study tours.
",Technology
"Another theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work.[217] The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.
",Technology
"Dissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.
",Technology
"Historians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.
",Technology
"During the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement.  Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes.[218]  Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of ""nature"" in art and language, in contrast to ""monstrous"" machines and factories; the ""Dark satanic mills"" of Blake's poem ""And did those feet in ancient time"". Mary Shelley's novel Frankenstein reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.[219]
",Technology
"Renaissance technology is the set of European artifacts and inventions which span the Renaissance period, roughly the 14th century through the 16th century. The era is marked by profound technical advancements such as the printing press, linear perspective in drawing, patent law, double shell domes and Bastion fortresses. Sketchbooks from artisans of the period (Taccola and Leonardo da Vinci,  example) give a deep insight into the mechanical technology then known and applied.
",Technology
"Renaissance science spawned the Scientific Revolution; science and technology began a cycle of mutual advancement.
",Technology
"Some important Renaissance technologies, including both innovations and improvements on existing techniques:
",Technology
"Some of the technologies were the arquebus and the musket.
",Technology
"The technologies that developed in Europe during the second half of the 15th century were commonly associated by authorities of the time with a key theme in Renaissance thought: the rivalry of the Moderns and the Ancients. Three inventions in particular — the printing press, firearms, and the nautical compass — were indeed seen as evidence that the Moderns could not only compete with the Ancients, but had surpassed them, for these three inventions allowed modern people to communicate, exercise power, and finally travel at distances unimaginable in earlier times.[1]
",Technology
"Crank and connecting rod
",Technology
"The crank and connecting rod mechanism which converts circular into reciprocal motion is of utmost importance for the mechanization of work processes; it is first attested for Roman water-powered sawmills.[2] During the Renaissance, its use is greatly diversified and mechanically refined; now connecting-rods are also applied to double compound cranks, while the flywheel is employed to get these cranks over the 'dead-spot'.[3] Early evidence of such machines appears, among other things, in the works of the 15th-century engineers Anonymous of the Hussite Wars and Taccola.[4] From then on, cranks and connecting rods become an integral part of machine design and are applied in ever more elaborate ways: Agostino Ramelli's The Diverse and Artifactitious Machines of 1588 depicts eighteen different applications, a number which rises in the 17th-century Theatrum Machinarum Novum by Georg Andreas Böckler to forty-five.[5]
",Technology
"Printing press
",Technology
"The invention of the printing press by the German goldsmith Johannes Gutenberg (1398–1468) is widely regarded as the single most important event of the second millennium,[7] and is one of the defining moments of the Renaissance. The Printing Revolution which it sparks throughout Europe works as a modern ""agent of change"" in the transformation of medieval society.
",Technology
"The mechanical device consists of a screw press modified for printing purposes which can produce 3,600 pages per workday,[6] allowing the mass production of printed books on a proto-industrial scale. By the start of the 16th century, printing presses are operating in over 200 cities in a dozen European countries, producing more than twenty million volumes.[8] By 1600, their output had risen tenfold to an estimated 150 to 200 million copies, while Gutenberg book printing spread from Europe further afield.[8]
",Technology
"The relatively free flow of information transcends borders and induced a sharp rise in Renaissance literacy, learning and education; the circulation of (revolutionary) ideas among the rising middle classes, but also the peasants, threatens the traditional power monopoly of the ruling nobility and is a key factor in the rapid spread of the Protestant Reformation. The dawn of the Gutenberg Galaxy, the era of mass communication, is instrumental in fostering the gradual democratization of knowledge which sees for the first time modern media phenomena such as the press or bestsellers emerging.[9]  The prized incunables, which are testimony to the aesthetic taste and high proficient competence of Renaissance book printers, are one lasting legacy of the 15th century.
",Technology
"Parachute
",Technology
"The earliest known parachute design appears in an anonymous manuscript from 1470s Renaissance Italy;[10] it depicts a free-hanging man clutching a crossbar frame attached to a conical canopy.[11] As a safety measure, four straps run from the ends of the rods to a waist belt. Around 1485, a more advanced parachute was sketched by the polymath Leonardo da Vinci in his Codex Atlanticus (fol. 381v), which he scales in a more favorable proportion to the weight of the jumper.[11] Leonardo's canopy was held open by a square wooden frame, altering the shape of the parachute from conical to pyramidal.[12] The Venetian inventor Fausto Veranzio (1551–1617) modifies da Vinci's parachute sketch by keeping the square frame, but replacing the canopy with a bulging sail-like piece of cloth. This he realized decelerates the fall more effectively.[12] Claims[13] that Veranzio successfully tested his parachute design in 1617 by jumping from a tower in Venice cannot be substantiated; since he was around 65 years old at the time, it seems unlikely.
",Technology
"Mariner's astrolabe
",Technology
"The earliest recorded uses of the astrolabe for navigational purposes are by the Portuguese explorers Diogo de Azambuja (1481), Bartholomew Diaz (1487/88) and Vasco da Gama (1497/98) during their sea voyages around Africa.[14]
",Technology
"Dry dock
",Technology
"While dry docks were already known in Hellenistic shipbuilding,[15] these facilities were reintroduced in 1495/96, when Henry VII of England ordered one to be built at the Portsmouth navy base.[16]
",Technology
"Floating dock
",Technology
"The earliest known description of a floating dock comes from a small Italian book printed in Venice in 1560, titled Descrittione dell'artifitiosa machina. In the booklet, an unknown author asks for the privilege of using a new method for the salvaging of a grounded ship and then proceeds to describe and illustrate his approach. The included woodcut shows a ship flanked by two large floating trestles, forming a roof above the vessel. The ship is pulled in an upright position by a number of ropes attached to the superstructure.[17]
",Technology
"Lifting tower
",Technology
"A lifting tower was used to great effect by Domenico Fontana to relocate the monolithic Vatican obelisk in Rome.[18] Its weight of 361 t was far greater than any of the blocks the Romans are known to have lifted by cranes.[18][A 1]
",Technology
"Mining, machinery and chemistry
A standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise De re metallica (1556), which also contains sections on geology, mining and chemistry.  De re metallica was the standard chemistry reference for the next 180 years.[19]
",Technology
"Newspaper
",Technology
"The newspaper is an offspring of the printing press from which the press derives its name.[21] The 16th century sees a rising demand for up-to-date information which can not be covered effectively by the circulating hand-written newssheets. For ""gaining time"" from the slow copying process, Johann Carolus of Strassburg is the first to publish his German-language Relation by using a printing press (1605).[22] In rapid succession, further German newspapers are established in Wolfenbüttel (Avisa Relation oder Zeitung), Basel, Frankfurt and Berlin.[22] From 1618 onwards, enterprising Dutch printers take up the practice and begin to provide the English and French market with translated news.[22] By the mid-17th century it is estimated that political newspapers which enjoyed the widest popularity reach up to 250,000 readers in the Holy Roman Empire, around one quarter of the literate population.[23]
",Technology
"Air-gun
",Technology
"In 1607 Bartolomeo Crescentio described an air-gun equipped with a powerful spiral spring, a device so complex that it must have had predecessors.[original research?] In 1610 Mersenne spoke in detail of ""sclopeti pneumatici constructio"", and four years later Wilkins wrote enthusiastically of ""that late ingenious invention the wind-gun"" as being ""almost equall to our powder-guns"". In the 1650s Otto von Guericke, famed for his experiments with vacua and pressures, built the Madeburger Windbuchse, one of the technical wonders of its time.[citation needed]
",Technology
"Cranked Archimedes' screw
",Technology
"The German engineer Konrad Kyeser equips in his Bellifortis (1405) the Archimedes' screw with a crank mechanism which soon replaces the ancient practice of working the pipe by treading.[24]
",Technology
"Cranked reel
",Technology
"In the textile industry, cranked reels for winding skeins of yarn were introduced in the early 15th century.[25]
",Technology
"Brace
",Technology
"The earliest carpenter's braces equipped with a U-shaped grip, that is with a compound crank, appears between 1420 and 1430 in Flanders.[3]
",Technology
"Cranked well-hoist
",Technology
"The earliest evidence for the fitting of a well-hoist with cranks is found in a miniature of c. 1425 in the German Hausbuch of the Mendel Foundation.[26]
",Technology
"Paddle wheel boat powered by crank and connecting rod mechanism
",Technology
"While paddle wheel boats powered by manually turned crankshafts were already conceived of by earlier writers such as Guido da Vigevano and the Anonymous Author of the Hussite Wars,[27] the Italian Roberto Valturio much improves on the design in 1463 by devising a boat with five sets of parallel cranks which are all joined to a single power source by one connecting rod; the idea is also taken up by his compatriot Francesco di Giorgio.[28]
",Technology
"Rotary grindstone with treadle
",Technology
"Evidence for rotary grindstones operated by a crank handle goes back to the Carolingian Utrecht Psalter.[29] Around 1480, the crank mechanism is further mechanized by adding a treadle.[30]
",Technology
"Geared hand-mill
",Technology
"The geared hand-mill, operated either with one or two cranks, appears in the 15th century.[25]
",Technology
"Grenade musket
",Technology
"Two 16th-century German grenade muskets working with a wheellock mechanism are on display in the Bayerisches Nationalmuseum, Munich.[31]
",Technology
"The revived scientific spirit of the age can perhaps be best exemplified by the voluminous corpus of technical drawings which the artist-engineers left behind, reflecting the wide variety of interests the Renaissance Homo universalis pursued. The establishment of the laws of linear perspective by Brunelleschi gave his successors, such as Taccola, Francesco di Giorgio Martini and Leonardo da Vinci, a powerful instrument to depict mechanical devices for the first time in a realistic manner. The extant sketch books give modern historians of science invaluable insights into the standards of technology of the time. Renaissance engineers showed a strong proclivity to experimental study, drawing a variety of technical devices, many of which appeared for the first time in history on paper.
",Technology
"However, these designs were not always intended to be put into practice, and often practical limitations impeded the application of the revolutionary designs. For example, da Vinci's ideas on the conical parachute or the winged flying machine were only applied much later. While earlier scholars showed a tendency to attribute inventions based on their first pictorial appearance to individual Renaissance engineers, modern scholarship is more prone to view the devices as products of a technical evolution which often went back to the Middle Ages.
",Technology
"Medieval technology is the technology used in medieval Europe under Christian rule. After the Renaissance of the 12th century, medieval Europe saw a radical change in the rate of new inventions, innovations in the ways of managing traditional means of production, and economic growth.[2] The period saw major technological advances, including the adoption of gunpowder, the invention of vertical windmills, spectacles, mechanical clocks, and greatly improved water mills, building techniques (Gothic architecture, medieval castles), and agriculture in general (three-field crop rotation).
",Technology
"The development of water mills from their ancient origins was impressive, and extended from agriculture to sawmills both for timber and stone. By the time of the Domesday Book, most large villages had turnable mills, around 6,500 in England alone.[3] Water-power was also widely used in mining for raising ore from shafts, crushing ore, and even powering bellows.
",Technology
"European technical advancements from the 12th to 14th centuries were either built on long-established techniques in medieval Europe, originating from Roman and Byzantine antecedents, or adapted from cross-cultural exchanges through trading networks with the Islamic world, China, and India. Often, the revolutionary aspect lay not in the act of invention itself, but in its technological refinement and application to political and economic power. Though gunpowder along with other weapons had been started by Chinese, it was the Europeans who developed and perfected its military potential, precipitating European expansion and eventual imperialism in the Modern Era.
",Technology
"Also significant in this respect were advances in maritime technology. Advances in shipbuilding included the multi-masted ships with lateen sails, the sternpost-mounted rudder and the skeleton-first hull construction. Along with new navigational techniques such as the dry compass, the Jacob's staff and the astrolabe, these allowed economic and military control of the seas adjacent to Europe and enabled the global navigational achievements of the dawning Age of Exploration.
",Technology
"At the turn to the Renaissance, Gutenberg’s invention of mechanical printing made possible a dissemination of knowledge to a wider population, that would not only lead to a gradually more egalitarian society, but one more able to dominate other cultures, drawing from a vast reserve of knowledge and experience. The technical drawings of late-medieval artist-engineers Guido da Vigevano and Villard de Honnecourt can be viewed as forerunners of later Renaissance works such as Taccola or da Vinci.
",Technology
"The following is a list of some important medieval technologies. The approximate date or first mention of a technology in medieval Europe is given. Technologies were often a matter of cultural exchange and date and place of first inventions are not listed here (see main links for a more complete history of each).
",Technology
"Carruca (6th to 9th centuries)
",Technology
"A type of heavy wheeled plough commonly found in Northern Europe.[4] The device consisted of four major parts. The first part was a coulter at the bottom of the plough.[5] This knife was used to vertically cut into the top sod to allow for the plowshare to work.[5] The plowshare was the second pair of knives which cut the sod horizontally, detaching it from the ground below.[5] The third part was the moldboard, which curled the sod outward.[5] The fourth part of the device was the team of eight oxen guided by the farmer.[6] This type of plough eliminated the need for cross-plowing by turning over the furrow instead of merely pushing it outward.[6] This type of wheeled plough made seed placement more consistent throughout the farm as the blade could be locked in at a certain level relative to the wheels.[7] A disadvantage to this type of plough was its maneuverability.[7] Since this equipment was large and lead by a small herd of oxen, turning the plough was difficult and time-consuming.[7] This caused many farmers to turn away from traditional square fields and adopt a longer, more rectangular field to ensure maximum efficiency.[7]
",Technology
"Ard (plough) (5th century)
",Technology
"An early medieval plough that consisted of a sharpened wooden post pulled by either animals or humans.[8] This lightweight and primitive plough was used primarily before the invention of the carruca.[8] The ard was inefficient in more firm northern soil but did decent work in southern areas where the soil was much softer.[8] Although the ard required the user to apply constant pressure to the plough in order to make sure the edge could break the ground, the soil was merely pushed to the sides instead of being properly turned over.[8]
",Technology
"Horse collar (6th to 9th centuries)
",Technology
"Once oxen started to be replaced by horses on farms and in fields, the yoke became obsolete due to its shape not working well with a horses' posture.[9] The first design for a horse collar was a throat-and-girth-harness.[9] These types of harnesses were unreliable though due to them not being sufficiently set in place.[9] The loose straps were prone to slipping and shifting positions as the horse was working and often caused asphyxiation.[9] Around the eighth century, the introduction of the rigid collar eliminated the problem of choking.[9] The rigid collar was ""placed over the horses head and rested on its shoulders.[9] This permitted unobstructed breathing and placed the weight of the plow or wagon where the horse could best support it.""[9]
",Technology
"Horseshoes (9th century)
",Technology
"Allowed horses to carry larger loads and move around with greater traction on hard to walk surfaces.[10] The practice of shoeing horses was initially practiced in the Roman Empire but lost popularity throughout the Middle Ages until around the 11th century.[10] Although horses in the southern lands could easily work while on the softer soil, the rocky soil of the north proved to be damaging to the horses' hooves.[11] Since the north was the problematic area, this is where shoeing horses first became popular.[11] The introduction of gravel roadways was also cause for the popularity of horseshoeing.[11] The loads a shoed horse could take on these roads were significantly higher than one that was barefoot.[11] By the 14th century, not only did horses have shoes, but many farmers were shoeing oxen and donkeys in order to help prolong the life of their hooves.[11] The size and weight of the horseshoe changed significantly over the course of the middle ages.[11] In the 10th century, horseshoes were secured by six nails and weighed around one-quarter of a pound, but throughout the years, the shoes grew larger and by the 14th century, the shoes were being secured with eight nails and weighed nearly half a pound.[11]
",Technology
"Crop rotation (8th century)
",Technology
"Also called the Two-field system. This system included the farmers' field being divided into two separate crops.[12] One field would grow a crop while the other was allowed to lie fallow and was used to feed livestock and regain lost nutrients.[12] Every year, the two fields would switch in order to ensure fields did not become nutrient deficient.[12] In the 11th century, this system was introduced into Sweden and spread to become the most popular form of farming.[12]
",Technology
"Three-field system (8th century)
",Technology
"The ideal three-field system is one that separates a section of land into three equal parts.[13] Each one of the three parts holds a different crop.[13] One part holds a spring crop, such as barley or oats, another part holds a winter crop, such as wheat or rye, and the third part is an off-field that is left alone to grow and is used to help feed livestock.[13] By rotating the three crops to a new part of the land after each year, the off-field regains some of the nutrients lost during the growing of the two crops.[13] This system increases agricultural productivity over the Two-field system by only having one-third of the field not being used instead of one half.[13] Another advantage of crop rotation is that many scholars believe it helped increase yields by up to 50%.[13]
",Technology
"Wine press (12th century)
",Technology
"This device was the first practical means of Pressing (wine) on a plane surface.[14] The wine press was an expensive piece of machinery that only the wealthy could afford.[14] The method of Grape stomping was often used as a less expensive alternative.[14] While white wines required the use of a wine press in order to preserve the color of the wine by removing the juices quickly from the skin, red wine did not need to be pressed until the end of the juice removal process since the color did not matter.[14] Many red wine winemakers used their feet to smash the grapes then used a press to remove any juice that remained in the grape skins.[14]
",Technology
"Qanat (5th century)
",Technology
"An underground passage used to water fields, crops, and provide drinking water.[15] These tunnels had a gradual slope which used gravity to pull the water from either an aquifer or water well.[15] This system was originally found in middle eastern areas and is still used today in places where surface water is hard to find.
",Technology
"Pendentive architecture (6th century)
",Technology
"A specific spherical form in the upper corners to support a dome. Although the first experimentation was made in the 200s, it was in the 6th century in the Byzantine Empire that its potential was fully achieved.
",Technology
"Artesian well (1126)
",Technology
"A thin rod with a hard iron cutting edge is placed in the bore hole and repeatedly struck with a hammer, underground water pressure forces the water up the hole without pumping. Artesian wells are named after the town of Artois in France, where the first one was drilled by Carthusian monks in 1126.
",Technology
"Central heating through underfloor channels (9th century)
",Technology
"In the early medieval Alpine upland, a simpler central heating system where heat travelled through underfloor channels from the furnace room replaced the Roman hypocaust at some places. In Reichenau Abbey a network of interconnected underfloor channels heated the 300 m2 large assembly room of the monks during the winter months. The degree of efficiency of the system has been calculated at 90%.[16]
",Technology
"Rib vault (12th century)
",Technology
"An essential element for the rise of Gothic architecture, rib vaults allowed vaults to be built for the first time over rectangles of unequal lengths. It also greatly facilitated scaffolding and largely replaced the older groin vault.
",Technology
"Chimney (12th century)
",Technology
"The earliest true chimneys appeared in Northern Europe during the 12th century, and with them came the first true fireplaces.
",Technology
"Segmental arch bridge (1345)
",Technology
"The Ponte Vecchio in Florence is considered medieval Europe's first stone segmental arch bridge.
",Technology
"Treadwheel crane (1220s)
",Technology
"The earliest reference to a treadwheel in archival literature is in France about 1225,[17] followed by an illuminated depiction in a manuscript of probably also French origin dating to 1240.[18] Apart from tread-drums, windlasses and occasionally cranks were employed for powering cranes.[19]
",Technology
"Stationary harbour crane (1244)
",Technology
"Stationary harbour cranes are considered a new development of the Middle Ages; its earliest use being documented for Utrecht in 1244.[20] The typical harbour crane was a pivoting structure equipped with double treadwheels. There were two types: wooden gantry cranes pivoting on a central vertical axle and stone tower cranes which housed the windlass and treadwheels with only the jib arm and roof rotating.[1] These cranes were placed on docksides for the loading and unloading of cargo where they replaced or complemented older lifting methods like see-saws, winches and yards.[20] Slewing cranes which allowed a rotation of the load and were thus particularly suited for dockside work appeared as early as 1340.[21]
",Technology
"Floating crane
",Technology
"Beside the stationary cranes, floating cranes which could be flexibly deployed in the whole port basin came into use by the 14th century.[1]
",Technology
"Mast crane
",Technology
"Some harbour cranes were specialised at mounting masts to newly built sailing ships, such as in Gdańsk, Cologne and Bremen.[1]
",Technology
"Wheelbarrow (1170s)
",Technology
"The wheelbarrow proved useful in building construction, mining operations, and agriculture. Literary evidence for the use of wheelbarrows appeared between 1170 and 1250 in north-western Europe. The first depiction is in a drawing by Matthew Paris in the mid-13th century.
",Technology
"Oil paint (by 1125)
",Technology
"As early as the 13th century, oil was used to add details to tempera paintings and paint wooden statues. Flemish painter Jan van Eyck developed the use of a stable oil mixture for panel painting around 1410.[22]
",Technology
"Hourglass (1338)
",Technology
"Reasonably dependable, affordable and accurate measure of time. Unlike water in a clepsydra, the rate of flow of sand is independent of the depth in the upper reservoir, and the instrument is not liable to freeze. Hourglasses are a medieval innovation (first documented in Siena, Italy).
",Technology
"Mechanical clocks (13th to 14th centuries)
",Technology
"A European innovation, these weight-driven clocks were used primarily in clock towers.
",Technology
"Compound crank
",Technology
"The Italian physician Guido da Vigevano combines in his 1335 Texaurus, a collection of war machines intended for the recapture of the Holy Land, two simple cranks to form a compound crank for manually powering war carriages and paddle wheel boats. The devices were fitted directly to the vehicle's axle respectively to the shafts turning the paddle wheels.[23]
",Technology
"Blast furnace (1150–1350)
",Technology
"Cast iron had been made in China before the 4th century BC.[24]  European cast iron first appears in Middle Europe (for instance Lapphyttan in Sweden, Dürstel in Switzerland and the Märkische Sauerland in Germany) around 1150,[25] in some places according to recent research even before 1100.[26] The technique is considered to be an independent European development.[27]
",Technology
"Ship mill (6th century)
",Technology
"The ship mill is a Byzantine invention, designed to mill grains using hydraulic power. The technology eventually spread to the rest of Europe and was in use until ca. 1800.
Paper mill (13th century)
",Technology
"The first certain use of a water-powered paper mill, evidence for which is elusive in both Chinese[28][29] and Muslim paper making,[30] dates to 1282.[31]
",Technology
"Rolling mill (15th century)
",Technology
"Used to produce metal sheet of an even thickness. First used on soft, malleable metals, such as lead, gold and tin. Leonardo da Vinci described a rolling mill for wrought iron.
",Technology
"Tidal Mills (6th century)
",Technology
"The earliest tidal mills were excavated on the Irish coast where watermillers knew and employed the two main waterwheel types: a 6th-century tide mill at Killoteran near Waterford was powered by a vertical waterwheel,[32] while the tide changes at Little Island were exploited by a twin-flume horizontal-wheeled mill (c. 630) and a vertical undershot waterwheel alongside it.[33][34] Another early example is the Nendrum Monastery mill from 787 which is estimated to have developed seven to eight horsepower at its peak.[35][36]
",Technology
"Vertical windmills (1180s)
",Technology
"Invented in Europe as the pivotable post mill, the first surviving mention of one comes from Yorkshire in England in 1185. They were efficient at grinding grain or draining water. Stationary tower mills were also developed in the 13th century.
",Technology
"Water hammer (12th century at the latest)
",Technology
"Used in metallurgy to forge the metal blooms from bloomeries and Catalan forges, they replaced manual hammerwork. The water hammer was eventually superseded by steam hammers in the 19th century.
",Technology
"Dry compass (12th century)
",Technology
"The first European mention of the directional compass is in Alexander Neckam's On the Natures of Things, written in Paris around 1190.[37] It was either transmitted from China or the Arabs or an independent European innovation. Dry compass were invented in the Mediterranean around 1300.[38]
",Technology
"Astronomical compass (1269)
",Technology
"The French scholar Pierre de Maricourt describes in his experimental study Epistola de magnete (1269) three different compass designs he has devised for the purpose of astronomical observation.[39]
",Technology
"Stern-mounted rudders (1180s)
",Technology
"The first depiction of a pintle-and-gudgeon rudder on church carvings dates to around 1180. They first appeared with cogs in the North and Baltic Seas and quickly spread to Mediterranean. The iron hinge system was the first stern rudder permanently attached to the ship hull and made a vital contribution to the navigation achievements of the age of discovery and thereafter.[40]
",Technology
"Movable type printing press (1440s)
",Technology
"Johannes Gutenberg's great innovation was not the printing itself, but instead of using carved plates as in woodblock printing, he used separate letters (types) from which the printing plates for pages were made up. This meant the types were recyclable and a page cast could be made up far faster.
",Technology
"Paper (13th century)
",Technology
"Paper was invented in China and transmitted through Islamic Spain in the 13th century. In Europe, the paper-making processes was mechanized by water-powered mills and paper presses (see paper mill).
",Technology
"Rotating bookmark (13th century)
",Technology
"A rotating disc and string device used to mark the page, column, and precise level in the text where a person left off reading in a text.  Materials used were often leather, velum, or paper.
",Technology
"Spectacles (1280s)
",Technology
"The first spectacles, invented in Florence, used convex lenses which were of help only to the far-sighted. Concave lenses were not developed prior to the 15th century.
",Technology
"Watermark (1282)
",Technology
"This medieval innovation was used to mark paper products and to discourage counterfeiting. It was first introduced in Bologna, Italy.
",Technology
"Theory of impetus (6th century)
",Technology
"A scientific theory that was introduced by John Philoponus who made criticism of Aristotelian principles of physics, and it served as an inspiration to medieval scholars as well as to Galileo Galilei  who ten centuries later, during the Scientific Revolution, extensively cited Philoponus in his works while making the case as to why Aristotelian physics was flawed. It is the intellectual precursor to the concepts of inertia, momentum and acceleration in classical mechanics.
",Technology
"Arabic numerals (13th century)
",Technology
"The first recorded mention in Europe was in 976, and they were first widely published in 1202 by Fibonacci with his Liber Abaci.
",Technology
"University
",Technology
"The first medieval universities were founded between the 11th and 13th centuries leading to a rise in literacy and learning. By 1500, the institution had spread throughout most of Europe and played a key role in the Scientific Revolution. Today, the educational concept and institution has been globally adopted.[41]
",Technology
"Functional button (13th century)
",Technology
"German buttons appeared in 13th-century Germany as an indigenous innovation.[42] They soon became widespread with the rise of snug-fitting clothing.
",Technology
"Horizontal loom (11th century)
",Technology
"Horizontal looms operated by foot-treadles were faster and more efficient.
",Technology
"Silk (6th century)
",Technology
"Manufacture of silk began in Eastern Europe in the 6th century and in Western Europe in the 11th or 12th century. Silk had been imported over the Silk Road since antiquity. The technology of ""silk throwing"" was mastered in Tuscany in the 13th century. The silk works used waterpower and some regard these as the first mechanized textile mills.
",Technology
"Spinning wheel (13th century)
",Technology
"Brought to Europe probably from India.
",Technology
"Chess (1450)
",Technology
"The earliest predecessors of the game originated in 6th-century AD India and spread via Persia and the Muslim world to Europe. Here the game evolved into its current form in the 15th century.
",Technology
"Forest glass (c. 1000)
",Technology
"This type of glass uses wood ash and sand as the main raw materials and is characterised by a variety of greenish-yellow colours.
",Technology
"Grindstones (834)
",Technology
"Grindstones are a rough stone, usually sandstone, used to sharpen iron. The first rotary grindstone (turned with a leveraged handle) occurs in the Utrecht Psalter, illustrated between 816 and 834.[43] According to Hägermann, the pen drawing is a copy of a late-antique manuscript.[44] A second crank which was mounted on the other end of the axle is depicted in the Luttrell Psalter from around 1340.[45]
",Technology
"Liquor (12th century)
",Technology
"Primitive forms of distillation were known to the Babylonians,[46] as well as Indians in the first centuries AD.[47] Early evidence of distillation also comes from alchemists working in Alexandria, Roman Egypt, in the 1st century.[48] The medieval Arabs adopted the distillation process,[49] which later spread to Europe. Texts on the distillation of waters, wine, and other spirits were written in Salerno and Cologne in the twelfth and thirteenth centuries.[49]
",Technology
"Liquor consumption rose dramatically in Europe in and after the mid-14th century, when distilled liquors were commonly used as remedies for the Black Death. These spirits would have had a much lower alcohol content (about 40% ABV) than the alchemists' pure distillations, and they were likely first thought of as medicinal elixirs. Around 1400, methods to distill spirits from wheat, barley, and rye were discovered. Thus began the ""national"" drinks of Europe, including  gin (England) and grappa (Italy). In 1437, ""burned water"" (brandy) was mentioned in the records of the County of Katzenelnbogen in Germany.[50]
",Technology
"Magnets (12th century)
",Technology
"Magnets were first referenced in the Roman d'Enéas, composed between 1155 and 1160.
",Technology
"Mirrors (1180)
",Technology
"The first mention of a ""glass"" mirror is in 1180 by Alexander Neckham who said ""Take away the lead which is behind the glass and there will be no image of the one looking in.""
",Technology
"Illustrated surgical atlas (1345)
",Technology
"Guido da Vigevano (c. 1280 − 1349) was the first author to add illustrations to his anatomical descriptions. His Anathomia provides pictures of neuroanatomical structures and techniques such as the dissection of the head by means of trephination, and depictions of the meninges, cerebrum, and spinal cord.[51]
",Technology
"Quarantine (1377)
",Technology
"Initially a 40-day-period, the quarantine was introduced by the Republic of Ragusa as a measure of disease prevention related to the Black Death. It was later adopted by Venice from where the practice spread all around in Europe.
",Technology
"Rat traps (1170s)
",Technology
"The first mention of a rat trap is in the medieval romance Yvain, the Knight of the Lion by Chrétien de Troyes.
",Technology
"Soap (9th century)
",Technology
"Soap came into widespread European use in the 9th century in semi-liquid form, with hard soap perfected by the Arabs in the 12th century.
",Technology
"Quilted Armour (pre 5th - 14th Century)
",Technology
"There was a vast amount of armour technology available through the 5th to 16th centuries. 
Most soldiers during this time wore padded or quilted armor. This was the cheapest and most available armor for the majority of soldiers. Quilted armour was usually just a jacket made of thick linen and wool meant to pad or soften the impact of blunt weapons and light blows. Although, this technology predated the 5th century, it was still extremely prevalent because of the low cost and the weapon technology at the time made the bronze armor of the Greeks and Romans obsolete. Quilted armour was also used in conjunction with other types of armour. Usually worn over or under leather, mail, and later plate armour.[52]
",Technology
"Cuir Bouilli (5th-10th Century)
",Technology
"Hardened leather armour also called Cuir Bouilli was a step up from quilted armour. Made by boiling leather in either water, wax or oil to soften it so it can be shaped, it would then be allowed to dry and become very hard.[53] Large pieces of armour could be made such as breast plates, helmets, and leg guards, but many times smaller pieces would be sewn into the quilting of quilted armour or strips would be sewn together on the outside of a linen jacket. This was not as affordable as the quilted armour but offered much better protection against edged slashing weapons.
",Technology
"Chain Mail (11th-16th Century)
",Technology
"The most common type during the 11th through the 16th centuries was the Hauberk, also known earlier than the 11th century as the Carolingian byrnie.[54] Made of interlinked rings of metal, it sometimes consisted of a coif that covered the head and a tunic that covered the torso, arms, and legs down to the knees. Chain mail was very effective at protecting against light slashing blows but ineffective against stabbing or thrusting blows. The great advantage was that it allowed a great freedom of movement and was relatively light with significant protection over quilted or hardened leather armour. It was far more expensive than the hardened leather or quilted armour because of the massive amount of labor it required to create. This made it unattainable for most soldiers and only the more wealthy soldiers could afford it. Later, toward the end of the 13th century banded mail became popular.[55] Constructed of washer shaped rings of iron overlapped and woven together by straps of leather as opposed to the interlinked metal rings of chain mail, banded mail was much more affordable to manufacture. The washers were so tightly woven together that it was very difficult penetrate and offered greater protection from arrow and bolt attacks.[56]
",Technology
"Jazerant (11th century)
",Technology
"The Jazerant or Jazeraint was an adaptation of chain mail in which the chain mail would be sewn in between layers of linen or quilted armour.[57] Exceptional protection against light slashing weapons and slightly improved protection against small thrusting weapons, but little protection against large blunt weapons such as maces and axes. This gave birth to reinforced chain mail and became more prevalent in the 12th and 13th century. Reinforced armour was made up of chain mail with metal plates or hardened leather plates sewn in. This greatly improved protection from stabbing and thrusting blows. 
",Technology
"Scale Armour (12th century)
",Technology
"A type of Lamellar armour,[58] was made up entirely of small, overlapping plates. Either sewn together, usually with leather straps, or attached to a backing such as linen, or a quilted armor. Scale armour does not require the labor to produce that chain mail does and therefore is more affordable. It also affords much better protection against thrusting blows and pointed weapons. Though, it is much heavier, more restrictive and impedes free movement. 
",Technology
"Plate Armour (14th century)
",Technology
"Plate armour covered the entire body. Although parts of the body were already covered in plate armour as early as 1250, such as the Poleyns for covering the knees and Couters - plates that protected the elbows,[59] the first complete full suit without any textiles was seen around 1410-1430. [60] Components of medieval armour that made up a full suit consisted of a cuirass, a gorget, vambraces, gauntlets, cuisses, greaves, and sabatons held together by internal leather straps. Improved weaponry such as crossbows and the long bow had greatly increased range and power. This made penetration of the chain mail hauberk much easier and more common.[61] By the mid 1400's most plate was worn alone and without the need of a hauberk.[62] Advances in metal working such as the blast furnace and new techniques for carburizing made plate armour nearly impenetrable and the best armour protection available at the time. Although plate armour was fairly heavy, because each suit was custom tailored to the wearer, it was very easy to move around in. A full suit of plate armour was extremely expensive and mostly unattainable for the majority of soldiers. Only very wealthy land owners and nobility could afford it. The quality of plate armour increases as more armour makers became more proficient in metal working. A suit of plate armour became a symbol of social status and the best made were personalized with embellishments and engravings. Plate armour saw continued use in battle until the 17th century.
",Technology
"Arched saddle (11th century)
",Technology
"The arched saddle enabled mounted knights to wield lances underarm and prevent the charge from turning into an unintentional pole-vault. This innovation gave birth to true shock cavalry, enabling fighters to charge on full gallop.
",Technology
"Spurs (11th century)
",Technology
"Spurs were invented by the Normans and appeared at the same time as the cantled saddle. They enabled the horseman to control his horse with his feet, replacing the whip and leaving his arms free. Rowel spurs familiar from cowboy films were already known in the 13th century. Gilded spurs were the ultimate symbol of the knighthood - even today someone is said to ""earn his spurs"" by proving his or her worthiness.
",Technology
"Stirrup (6th century)
",Technology
"Stirrups were invented by steppe nomads in what is today Mongolia and northern China in the 4th century. They were introduced in Byzantium in the 6th century and in the Carolingian Empire in the 8th. They allowed a mounted knight to wield a sword and strike from a distance leading to a great advantage for mounted cavalry.
",Technology
"Cannon (1324)
",Technology
"Cannons are first recorded in Europe at the siege of Metz in 1324. In 1350 Petrarch wrote ""these instruments which discharge balls of metal with most tremendous noise and flashes of fire...were a few years ago very rare and were viewed with greatest astonishment and admiration, but now they are become as common and familiar as any other kinds of arms.""[1]
",Technology
"Volley gun
",Technology
"See Ribauldequin.
",Technology
"Corned gunpowder (late 14th century)
",Technology
"First practiced in Western Europe, corning the black powder allowed for more powerful and faster ignition of cannons. It also facilitated the storage and transportation of black powder. Corning constituted a crucial step in the evolution of gunpowder warfare.
",Technology
"Supergun (late 14th century)
",Technology
"Extant examples include the wrought-iron Pumhart von Steyr, Dulle Griet and Mons Meg as well as the cast-bronze Faule Mette and Faule Grete (all from the 15th century).
",Technology
"Counterweight trebuchet (12th century)
",Technology
"Powered solely by the force of gravity, these catapults revolutionized medieval siege warfare and construction of fortifications by hurling huge stones unprecedented distances. Originating somewhere in the eastern Mediterranean basin, counterweight trebuchets were introduced in the Byzantine Empire around 1100 CE, and was later adopted by the Crusader states and as well by the other armies of Europe and Asia.[63]
",Technology
"Greek fire (7th century)
",Technology
"An incendiary weapon which could even burn on water is also attributed to the Byzantines where they installed it on their ships. It played a crucial role in the Byzantine Empire's victory over the Umayyad Caliphate during the Siege of Constantinople (717–718)
",Technology
"Grenade (8th century)
",Technology
"Rudimentary incendiary grenades appeared in the Byzantine Empire, as the Byzantine soldiers learned that Greek fire, a Byzantine invention of the previous century, could not only be thrown by flamethrowers at the enemy, but also in stone and ceramic jars. 
",Technology
"Longbow with massed, disciplined archery (13th century)
",Technology
"Having a high rate of fire and penetration power, the longbow contributed to the eventual demise of the medieval knight class[dubious  – discuss]. Used particularly by the English to great effect against the French cavalry during the Hundred Years' War (1337–1453).
",Technology
"Steel crossbow (late 14th century)
",Technology
"European innovation. Came with several different cocking aids to enhance draw power, making the weapons also the first hand-held mechanical crossbows.
",Technology
"Combined arms tactics (14th century)
",Technology
"The battle of Halidon Hill 1333 was the first battle where intentional and disciplined combined arms infantry tactics were employed.[dubious  – discuss] The English men-at-arms dismounted aside the archers, combining thus the staying power of super-heavy infantry and striking power of their two-handed weapons with the missiles and mobility of the archers using longbows and shortbows
. Combining dismounted knights and men-at-arms with archers was the archetypal Western Medieval battle tactics until the battle of Flodden 1513 and final emergence of firearms.
",Technology
"Longbowmen (c. 1493)
",Technology
"Cranked rack-and-pinion device for cocking a crossbow (c. 1493)
",Technology
"Organ gun in the Bellifortis (c. 1405)
",Technology
"
",Health
"Health, as defined by the World Health Organization (WHO), is ""a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity.""[1][2] This definition has been subject to controversy, as it may have limited value for implementation.[3][4][5] Health may be defined as the ability to adapt and manage physical, mental and social challenges throughout life.[6]
",Health
"The meaning of health has evolved over time. In keeping with the biomedical perspective, early definitions of health focused on the theme of the body's ability to function; health was seen as a state of normal function that could be disrupted from time to time by disease. An example of such a definition of health is: ""a state characterized by anatomic, physiologic, and psychological integrity; ability to perform personally valued family, work, and community roles; ability to deal with physical, biological, psychological, and social stress"".[7] Then in 1948, in a radical departure from previous definitions, the World Health Organization (WHO) proposed a definition that aimed higher: linking health to well-being, in terms of ""physical, mental, and social well-being, and not merely the absence of disease and infirmity"".[8] Although this definition was welcomed by some as being innovative, it was also criticized as being vague, excessively broad and was not construed as measurable. For a long time, it was set aside as an impractical ideal and most discussions of health returned to the practicality of the biomedical model.[9]
",Health
"Just as there was a shift from viewing disease as a state to thinking of it as a process, the same shift happened in definitions of health. Again, the WHO played a leading role when it fostered the development of the health promotion movement in the 1980s. This brought in a new conception of health, not as a state, but in dynamic terms of resiliency, in other words, as ""a resource for living"". 1984 WHO revised the definition of health defined it as ""the extent to which an individual or group is able to realize aspirations and satisfy needs and to change or cope with the environment. Health is a resource for everyday life, not the objective of living; it is a positive concept, emphasizing social and personal resources, as well as physical capacities"".[10] Thus, health referred to the ability to maintain homeostasis and recover from insults. Mental, intellectual, emotional and social health referred to a person's ability to handle stress, to acquire skills, to maintain relationships, all of which form resources for resiliency and independent living.[9] This opens up many possibilities for health to be taught, strengthened and learned.
",Health
"Since the late 1970s, the federal Healthy People Initiative has been a visible component of the United States’ approach to improving population health.[11][12] In each decade, a new version of Healthy People is issued,[13] featuring updated goals and identifying topic areas and quantifiable objectives for health improvement during the succeeding ten years, with assessment at that point of progress or lack thereof. Progress has been limited to many objectives, leading to concerns about the effectiveness of Healthy People in shaping outcomes in the context of a decentralized and uncoordinated US health system. Healthy People 2020 gives more prominence to health promotion and preventive approaches and adds a substantive focus on the importance of addressing social determinants of health. A new expanded digital interface facilitates use and dissemination rather than bulky printed books as produced in the past. The impact of these changes to Healthy People will be determined in the coming years.[14]
",Health
"Systematic activities to prevent or cure health problems and promote good health in humans are undertaken by health care providers. Applications with regard to animal health are covered by the veterinary sciences. The term ""healthy"" is also widely used in the context of many types of non-living organizations and their impacts for the benefit of humans, such as in the sense of healthy communities, healthy cities or healthy environments. In addition to health care interventions and a person's surroundings, a number of other factors are known to influence the health status of individuals, including their background, lifestyle, and economic, social conditions and spirituality; these are referred to as ""determinants of health."" Studies have shown that high levels of stress can affect human health.[15]
",Health
"In the first decade of the 21st century, the conceptualization of health as an ability opened the door for self-assessments to become the main indicators to judge the performance of efforts aimed at improving human health.[16] It also created the opportunity for every person to feel healthy, even in the presence of multiple chronic diseases, or a terminal condition, and for the re-examination of determinants of health, away from the traditional approach that focuses on the reduction of the prevalence of diseases.[17]
",Health
"Generally, the context in which an individual lives is of great importance for both his health status and quality of their life It is increasingly recognized that health is maintained and improved not only through the advancement and application of health science, but also through the efforts and intelligent lifestyle choices of the individual and society. According to the World Health Organization, the main determinants of health include the social and economic environment, the physical environment and the person's individual characteristics and behaviors.[18]
",Health
"More specifically, key factors that have been found to influence whether people are healthy or unhealthy include the following:[18][19][20]
",Health
"
",Health
"
",Health
"An increasing number of studies and reports from different organizations and contexts examine the linkages between health and different factors, including lifestyles, environments, health care organization and health policy, one specific health policy brought into many countries in recent years was the introduction of the sugar tax. Beverage taxes came into light with increasing concerns about obesity, particularly among youth. Sugar-sweetened beverages have become a target of anti-obesity initiatives with increasing evidence of their link to obesity.[21]– such as the 1974 Lalonde report from Canada;[20] the Alameda County Study in California;[22] and the series of World Health Reports of the World Health Organization, which focuses on global health issues including access to health care and improving public health outcomes, especially in developing countries.[23]
",Health
"The concept of the ""health field,"" as distinct from medical care, emerged from the Lalonde report from Canada. The report identified three interdependent fields as key determinants of an individual's health. These are:[20]
",Health
"The maintenance and promotion of health is achieved through different combination of physical, mental, and social well-being, together sometimes referred to as the ""health triangle.""[24][25] The WHO's 1986 Ottawa Charter for Health Promotion further stated that health is not just a state, but also ""a resource for everyday life, not the objective of living. Health is a positive concept emphasizing social and personal resources, as well as physical capacities.""[26]
",Health
"Focusing more on lifestyle issues and their relationships with functional health, data from the Alameda County Study suggested that people can improve their health via exercise, enough sleep, maintaining a healthy body weight, limiting alcohol use, and avoiding smoking.[27] Health and illness can co-exist, as even people with multiple chronic diseases or terminal illnesses can consider themselves healthy.[28]
",Health
"The environment is often cited as an important factor influencing the health status of individuals. This includes characteristics of the natural environment, the built environment and the social environment. Factors such as clean water and air, adequate housing, and safe communities and roads all have been found to contribute to good health, especially to the health of infants and children.[18][29] Some studies have shown that a lack of neighborhood recreational spaces including natural environment leads to lower levels of personal satisfaction and higher levels of obesity, linked to lower overall health and well being.[30] This suggests that the positive health benefits of natural space in urban neighborhoods should be taken into account in public policy and land use.
",Health
"Genetics, or inherited traits from parents, also play a role in determining the health status of individuals and populations. This can encompass both the predisposition to certain diseases and health conditions, as well as the habits and behaviors individuals develop through the lifestyle of their families. For example, genetics may play a role in the manner in which people cope with stress, either mental, emotional or physical. For example, obesity is a significant problem in the United States that contributes to bad mental health and causes stress in the lives of great numbers of people.[31] (One difficulty is the issue raised by the debate over the relative strengths of genetics and other factors; interactions between genetics and environment may be of particular importance.)
",Health
"A number of types of health issues are common around the globe. Disease is one of the most common. According to GlobalIssues.org, approximately 36 million people die each year from non-communicable (not contagious) disease including cardiovascular disease, cancer, diabetes and chronic lung disease (Shah, 2014).
",Health
"Among communicable diseases, both viral and bacterial, AIDS/HIV, tuberculosis, and malaria are the most common, causing millions of deaths every year (Shah, 2014).
",Health
"Another health issue that causes death or contributes to other health problems is malnutrition, especially among children. One of the groups malnutrition affects most is young children. Approximately 7.5 million children under the age of 5 die from malnutrition, usually brought on by not having the money to find or make food (Shah, 2014).
",Health
"Bodily injuries are also a common health issue worldwide. These injuries, including broken bones, fractures, and burns can reduce a person's quality of life or can cause fatalities including infections that resulted from the injury or the severity injury in general (Moffett, 2013).[32]
",Health
"Lifestyle choices are contributing factors to poor health in many cases. These include smoking cigarettes, and can also include a poor diet, whether it is overeating or an overly constrictive diet. Inactivity can also contribute to health issues and also a lack of sleep, excessive alcohol consumption, and neglect of oral hygiene (Moffett2013).There are also genetic disorders that are inherited by the person and can vary in how much they affect the person and when they surface (Moffett, 2013).
",Health
"Though the majority of these health issues are preventable, a major contributor to global ill health is the fact that approximately 1 billion people lack access to health care systems (Shah, 2014). Arguably, the most common and harmful health issue is that a great many people do not have access to quality remedies.[33][34]
",Health
"The World Health Organization describes mental health as ""a state of well-being in which the individual realizes his or her own abilities, can cope with the normal stresses of life, can work productively and fruitfully, and is able to make a contribution to his or her community"".[35] Mental Health is not just the absence of mental illness.[36]
",Health
"Mental illness is described as 'the spectrum of cognitive, emotional, and behavioral conditions that interfere with social and emotional well-being and the lives and productivity of people. Having a mental illness can seriously impair, temporarily or permanently, the mental functioning of a person. Other terms include: 'mental health problem', 'illness', 'disorder', 'dysfunction'.[37]
",Health
"Roughly a quarter of all adults 18 and over in the US are considered diagnosable with mental illness. Mental illnesses are the leading cause of disability in the US and Canada. Examples include, schizophrenia, ADHD, major depressive disorder, bipolar disorder, anxiety disorder, post-traumatic stress disorder and autism.[38]
",Health
"Many teens suffer from mental health issues in response to the pressures of society and social problems they encounter. Some of the key mental health issues seen in teens are: depression, eating disorders, and drug abuse. There are many ways to prevent these health issues from occurring such as communicating well with a teen suffering from mental health issues. Mental health can be treated and be attentive to teens' behavior.[39]
",Health
" Many factors contribute to mental health problems, including:
",Health
"Achieving and maintaining health is an ongoing process, shaped by both the evolution of health care knowledge and practices as well as personal strategies and organized interventions for staying healthy.
",Health
"An important way to maintain your personal health is to have a healthy diet. A healthy diet includes a variety of plant-based and animal-based foods that provide nutrients to your body. Such nutrients give you energy and keep your body running. Nutrients help build and strengthen bones, muscles, and tendons and also regulate body processes (i.e. blood pressure). The food guide pyramid is a pyramid-shaped guide of healthy foods divided into sections. Each section shows the recommended intake for each food group (i.e. Protein, Fat, Carbohydrates, and Sugars). Making healthy food choices is important because it can lower your risk of heart disease, developing some types of cancer, and it will contribute to maintaining a healthy weight.[42]
",Health
"The Mediterranean diet is commonly associated with health-promoting effects due to the fact that it contains some bioactive compounds like phenolic compounds, isoprenoids and alkaloids.[43]
",Health
"Physical exercise enhances or maintains physical fitness and overall health and wellness. It strengthens muscles and improves the cardiovascular system. According to the National Institutes of Health, there are four types of exercise: endurance, strength, flexibility, and balance.[44]
",Health
"Sleep is an essential component to maintaining health. In children, sleep is also vital for growth and development. Ongoing sleep deprivation has been linked to an increased risk for some chronic health problems. In addition, sleep deprivation has been shown to correlate with both increased susceptibility to illness and slower recovery times from illness.[45] In one study, people with chronic insufficient sleep, set as six hours of sleep a night or less, were found to be four times more likely to catch a cold compared to those who reported sleeping for seven hours or more a night.[46] Due to the role of sleep in regulating metabolism, insufficient sleep may also play a role in weight gain or, conversely, in impeding weight loss.[47] Additionally, in 2007, the International Agency for Research on Cancer, which is the cancer research agency for the World Health Organization, declared that ""shiftwork that involves circadian disruption is probably carcinogenic to humans,"" speaking to the dangers of long-term nighttime work due to its intrusion on sleep.[48] In 2015, the National Sleep Foundation released updated recommendations for sleep duration requirements based on age and concluded that ""Individuals who habitually sleep outside the normal range may be exhibiting signs or symptoms of serious health problems or, if done volitionally, may be compromising their health and well-being.""[49]
",Health
"Health science is the branch of science focused on health. There are two main approaches to health science: the study and research of the body and health-related issues to understand how humans (and animals) function, and the application of that knowledge to improve health and to prevent and cure diseases and other physical and mental impairments. The science builds on many sub-fields, including biology, biochemistry, physics, epidemiology, pharmacology, medical sociology. Applied health sciences endeavor to better understand and improve human health through applications in areas such as health education, biomedical engineering, biotechnology and public health.
",Health
"Organized interventions to improve health based on the principles and procedures developed through the health sciences are provided by practitioners trained in medicine, nursing, nutrition, pharmacy, social work, psychology, occupational therapy, physical therapy and other health care professions. Clinical practitioners focus mainly on the health of individuals, while public health practitioners consider the overall health of communities and populations. Workplace wellness programs are increasingly adopted by companies for their value in improving the health and well-being of their employees, as are school health services in order to improve the health and well-being of children.
",Health
"Public health has been described as ""the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals.""[50] It is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health has many sub-fields, but typically includes the interdisciplinary categories of epidemiology, biostatistics and health services. Environmental health, community health, behavioral health, and occupational health are also important areas of public health.
",Health
"The focus of public health interventions is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behavior, communities, and (in aspects relevant to human health) environments. Its aim is to prevent health problems from happening or re-occurring by implementing educational programs, developing policies, administering services and conducting research.[51] In many cases, treating a disease or controlling a pathogen can be vital to preventing it in others, such as during an outbreak. Vaccination programs and distribution of condoms to prevent the spread of communicable diseases are examples of common preventive public health measures, as are educational campaigns to promote vaccination and the use of condoms (including overcoming resistance to such).
",Health
"Public health also takes various actions to limit the health disparities between different areas of the country and, in some cases, the continent or world. One issue is the access of individuals and communities to health care in terms of financial, geographical or socio-cultural constraints to accessing and using services.[52] Applications of the public health system include the areas of maternal and child health, health services administration, emergency response, and prevention and control of infectious and chronic diseases.
",Health
"The great positive impact of public health programs is widely acknowledged. Due in part to the policies and actions developed through public health, the 20th century registered a decrease in the mortality rates for infants and children and a continual increase in life expectancy in most parts of the world. For example, it is estimated that life expectancy has increased for Americans by thirty years since 1900,[53] and worldwide by six years since 1990.[54]
",Health
"Personal health depends partially on the active, passive, and assisted cues people observe and adopt about their own health. These include personal actions for preventing or minimizing the effects of a disease, usually a chronic condition, through integrative care. They also include personal hygiene practices to prevent infection and illness, such as bathing and washing hands with soap; brushing and flossing teeth; storing, preparing and handling food safely; and many others. The information gleaned from personal observations of daily living – such as about sleep patterns, exercise behavior, nutritional intake and environmental features – may be used to inform personal decisions and actions (e.g., ""I feel tired in the morning so I am going to try sleeping on a different pillow""), as well as clinical decisions and treatment plans (e.g., a patient who notices his or her shoes are tighter than usual may be having exacerbation of left-sided heart failure, and may require diuretic medication to reduce fluid overload).[55]
",Health
"Personal health also depends partially on the social structure of a person's life. The maintenance of strong social relationships, volunteering, and other social activities have been linked to positive mental health and also increased longevity. One American study among seniors over age 70, found that frequent volunteering was associated with reduced risk of dying compared with older persons who did not volunteer, regardless of physical health status.[56] Another study from Singapore reported that volunteering retirees had significantly better cognitive performance scores, fewer depressive symptoms, and better mental well-being and life satisfaction than non-volunteering retirees.[57]
",Health
"Prolonged psychological stress may negatively impact health, and has been cited as a factor in cognitive impairment with aging, depressive illness, and expression of disease.[58] Stress management is the application of methods to either reduce stress or increase tolerance to stress. Relaxation techniques are physical methods used to relieve stress. Psychological methods include cognitive therapy, meditation, and positive thinking, which work by reducing response to stress. Improving relevant skills, such as problem solving and time management skills, reduces uncertainty and builds confidence, which also reduces the reaction to stress-causing situations where those skills are applicable.
",Health
"In addition to safety risks, many jobs also present risks of disease, illness and other long-term health problems. Among the most common occupational diseases are various forms of pneumoconiosis, including silicosis and coal worker's pneumoconiosis (black lung disease). Asthma is another respiratory illness that many workers are vulnerable to. Workers may also be vulnerable to skin diseases, including eczema, dermatitis, urticaria, sunburn, and skin cancer.[59][60] Other occupational diseases of concern include carpal tunnel syndrome and lead poisoning.
",Health
"As the number of service sector jobs has risen in developed countries, more and more jobs have become sedentary, presenting a different array of health problems than those associated with manufacturing and the primary sector. Contemporary problems, such as the growing rate of obesity and issues relating to stress and overwork in many countries, have further complicated the interaction between work and health.
",Health
"Many governments view occupational health as a social challenge and have formed public organizations to ensure the health and safety of workers. Examples of these include the British Health and Safety Executive and in the United States, the National Institute for Occupational Safety and Health, which conducts research on occupational health and safety, and the Occupational Safety and Health Administration, which handles regulation and policy relating to worker safety and health.[61][62][63]
",Health
"
",Health
"Global health is the health of populations in the global context;[1] it has been defined as ""the area of study, research and practice that places a priority on improving health and achieving equity in health for all people worldwide"".[2] Problems that transcend national borders or have a global political and economic impact are often emphasized.[3] Thus, global health is about worldwide health improvement (including mental health), reduction of disparities, and protection against global threats that disregard national borders.[4] Global health is not to be confused with international health, which is defined as the branch of public health focusing on developing nations and foreign aid efforts by industrialized countries.[5] Global health can be measured as a function of various global diseases and their prevalence in the world and threat to decrease life in the present day.
",Health
"The predominant agency associated with global health (and international health) is the World Health Organization (WHO). Other important agencies impacting global health include UNICEF and World Food Programme. The United Nations system has also played a part with cross-sectoral actions to address global health and its underlying socioeconomic determinants with the declaration of the Millennium Development Goals[6] and the more recent Sustainable Development Goals.
",Health
"Global health employs several perspectives that focus on the determinants and distribution of health in international contexts:
",Health
"Both individuals and organizations working in the domain of global health often face many questions regarding ethical and human rights. Critical examination of the various causes and justifications of health inequities is necessary for the success of proposed solutions. Such issues are discussed at the bi-annual Global Summits of National Ethics/Bioethics Councils, next in March 2016 in Berlin, with experts from WHO and UNESCO, by invitation of the German Ethics Council.
",Health
"The 19th century held major discoveries in medicine and public health.[12] The Broad Street cholera outbreak of 1854 was central to the development of modern epidemiology. The microorganisms responsible for malaria and tuberculosis were identified in 1880 and 1882, respectively. The 20th century saw the development of preventive and curative treatments for many diseases, including the BCG vaccine (for tuberculosis) and penicillin in the 1920s. The eradication of smallpox, with the last naturally occurring case recorded in 1977, raised hope that other diseases could be eradicated as well.
",Health
"Important steps were taken towards global cooperation in health with the formation of the United Nations (UN) and the World Bank Group in 1945, after World War II. In 1948, the member states of the newly formed United Nations gathered to create the World Health Organization. A cholera epidemic that took 20,000 lives in Egypt in 1947 and 1948 helped spur the international community to action.[13] The WHO published its Model List of Essential Medicines, and the 1978 Alma Ata declaration underlined the importance of primary health care.[14]
",Health
"At a United Nations Summit in 2000, member nations declared eight Millennium Development Goals (MDGs), which reflected the major challenges facing human development globally, to be achieved by 2015.[15] The declaration was matched by unprecedented global investment by donor and recipient countries. According to the UN, these MDGs provided an important framework for development and significant progress has been made in a number of areas.[16][17]  However, progress has been uneven and some of the MDGs were not fully realized including maternal, newborn and child health and reproductive health.[16] Building on the MDGs, a new Sustainable Development Agenda with 17 Sustainable Development Goals (SDGs) has been established for the years 2016-2030.[16]  The first goal being an ambitious and historic pledge to end poverty.[18] On 25 September 2015, the 193 countries of the UN General Assembly adopted the 2030 Development Agenda titled Transforming our world: the 2030 Agenda for Sustainable Development.[18]
",Health
"In 2015 a book titled ""To Save Humanity"" was published, with nearly 100 essays regarding today's most pressing global health issues.[19] The essays were authored by global figures in politics, science, and advocacy ranging from Bill Clinton to Peter Piot, and addressed a wide range of issues including vaccinations, antimicrobial resistance, health coverage, tobacco use, research methodology, climate change, equity, access to medicine, and media coverage of health research.
",Health
"Measures of global health include disability-adjusted life year (DALY), quality-adjusted life years (QALYs), and mortality rate.[20]
",Health
"The DALY is a summary measure that combines the impact of illness, disability, and mortality by measuring the time lived with disability and the time lost due to premature mortality. One DALY can be thought of as one lost year of ""healthy"" life. The DALY for a disease is the sum of the years of life lost due to premature mortality and the years lost due to disability for incident cases of the health condition.
",Health
"QALYs combine expected survival with expected quality of life into a single number: if an additional year of healthy life is worth a value of one (year), then a year of less healthy life is worth less than one (year). QALY calculations are based on measurements of the value that individuals place on expected years of survival. Measurements can be made in several ways: by techniques that simulate gambles about preferences for alternative states of health, with surveys or analyses that infer willingness to pay for alternative states of health, or through instruments that are based on trading off some or all likely survival time that a medical intervention might provide in order to gain less survival time of higher quality.[20]
",Health
"Infant mortality and child mortality for children under age 5 are more specific than DALYs or QALYs in representing the health in the poorest sections of a population, and are thus especially useful when focusing on health equity.[21]
",Health
"Morbidity measures include incidence rate, prevalence, and cumulative incidence, with incidence rate referring to the risk of developing a new health condition within a specified period of time. Although sometimes loosely expressed simply as the number of new cases during a time period, morbidity is better expressed as a proportion or a rate.
",Health
"The diseases and health conditions targeted by global health initiatives are sometimes grouped under ""diseases of poverty"" versus ""diseases of affluence"", although the impact of globalization is increasingly blurring the lines between the two.
",Health
"Infections of the respiratory tract and middle ear are major causes of morbidity and mortality worldwide.[22] Some respiratory infections of global significance include tuberculosis, measles, influenza, and pneumonias caused by pneumococci and Haemophilus influenzae. The spread of respiratory infections is exacerbated by crowded conditions, and poverty is associated with more than a 20-fold increase in the relative burden of lung infections.[23]
",Health
"Diarrhea is the second most common cause of child mortality worldwide, responsible for 17% of deaths of children under age 5.[24] Poor sanitation can increase transmission of bacteria and viruses through water, food, utensils, hands, and flies. Dehydration due to diarrhea can be effectively treated through oral rehydration therapy with dramatic reductions in mortality.[25][26] Important nutritional measures include the promotion of breastfeeding and zinc supplementation. While hygienic measures alone may be insufficient for the prevention of rotavirus diarrhea,[27] it can be prevented by a safe and potentially cost-effective vaccine.[28]
",Health
"Complications of pregnancy and childbirth are the leading causes of death among women of reproductive age in many developing countries: a woman dies from complications from childbirth approximately every minute.[29] According to the World Health Organization's 2005 World Health Report, poor maternal conditions are the fourth leading cause of death for women worldwide, after HIV/AIDS, malaria, and tuberculosis.[30] Most maternal deaths and injuries can be prevented, and such deaths have been largely eradicated in the developed world.[31] Targets for improving maternal health include increasing the number of  deliveries accompanied by skilled birth attendants.[32]
",Health
"68 low-income countries tracked by the WHO- and UNICEF-led collaboration Countdown to 2015 are estimated to hold for 97% of worldwide maternal and child deaths.[33]
",Health
"The HIV/AIDS epidemic has highlighted the global nature of human health and welfare and globalisation has given rise to a trend toward finding common solutions to global health challenges. Numerous international funds have been set up in recent times to address global health challenges such as HIV. [34]Since the beginning of the epidemic, more than 70 million people have been infected with the HIV virus and about 35 million people have died of HIV. Globally, 36.9 million [31.1–43.9 million] people were living with HIV at the end of 2017. An estimated 0.8% [0.6-0.9%] of adults aged 15–49 years worldwide are living with HIV, although the burden of the epidemic continues to vary considerably between countries and regions. The WHO African region remains most severely affected, with nearly 1 in every 25 adults (4.1%) living with HIV and accounting for nearly two-thirds of the people living with HIV worldwide.[35] Human immunodeficiency virus (HIV) is transmitted through unprotected sex, unclean needles, blood transfusions, and from mother to child during birth or lactation. Globally, HIV is primarily spread through sexual intercourse. The risk-per-exposure with vaginal sex in low-income countries from female to male is 0.38% and male to female is 0.3%.[36]The infection damages the immune system, leading to acquired immunodeficiency syndrome (AIDS) and eventually, death. Antiretroviral drugs prolong life and delay the onset of AIDS by minimizing the amount of HIV in the body.
",Health
"Malaria is a mosquito-borne infectious disease caused by the parasites of the genus Plasmodium. Symptoms may include fever, headaches, chills, and nausea. Each year, there are approximately 500 million cases of malaria worldwide, most commonly among children and pregnant women in developing countries.[37] The WHO African Region carries a disproportionately high share of the global malaria burden. In 2016, the region was home to 90% of malaria cases and 91% of malaria deaths. [38]The use of insecticide-treated bednets is a cost-effective way to reduce deaths from malaria, as is prompt artemisinin-based combination therapy, supported by intermittent preventive therapy in pregnancy. International travellers to endemic zones are advised chemoprophylaxis with antimalarial drugs like Atovaquone-proguanil, doxycycline, or mefloquine[39]
",Health
"In 2010, about 104 million children were underweight, and undernutrition contributes to about one third of child deaths around the world.[40] (Undernutrition is not to be confused with malnutrition, which refers to poor proportion of food intake and can thus refer to obesity.)[41] Undernutrition impairs the immune system, increasing the frequency, severity, and duration of infections (including measles, pneumonia, and diarrhea). Infection can further contribute to malnutrition.[42] Deficiencies of micronutrient, such as vitamin A, iron, iodine, and zinc, are common worldwide and can compromise intellectual potential, growth, development, and adult productivity.[43][44][45][46][47][48] Interventions to prevent malnutrition include micronutrient supplementation, fortification of basic grocery foods, dietary diversification, hygienic measures to reduce spread of infections, and the promotion of breastfeeding.
",Health
"Violence against women has been defined as: ""physical, sexual and psychological violence occurring in the family and in the general community, including battering, sexual abuse of children, dowry-related violence, rape, female genital mutilation and other traditional practices harmful to women, non-spousal violence and violence related to exploitation, sexual harassment and intimidation at work, in educational institutions and elsewhere, trafficking in women, forced prostitution and violence perpetrated or condoned by the state.""[49] In addition to causing injury, violence may increase ""women’s long-term risk of a number of other health problems, including chronic pain, physical disability, drug and alcohol abuse, and depression"".[50]
",Health
"Although statistics can be difficult to obtain as many cases go unreported, it is estimated that one in every five women faces some form of violence during her lifetime, in some cases leading to serious injury or even death.[51] Risk factors for being a perpetrator include low education, past exposure to child maltreatment or witnessing violence between parents, harmful use of alcohol, attitudes accepting of violence and gender inequality.[52] Equality of women has been addressed in the Millennium development goals.
",Health
"Approximately 80% of deaths linked to non-communicable diseases occur in developing countries.[53]For instance, urbanization and aging have led to increasing poor health conditions related to non-communicable diseases in India. The fastest-growing causes of disease burden over the last 26 years were diabetes (rate increased by 80%) and ischemic heart disease (up 34%). More than 60% of deaths, about 6.1 million, in 2016 were due to NCDs, up from about 38% in 1990.[54] Increases in refugee urbanization, has led to a growing number of people diagnosed with chronic noncommunicable diseases.[55]
",Health
"In September 2011, the United Nations is hosting its first General Assembly Special Summit on the issue of non-communicable diseases.[56] Noting that non-communicable diseases are the cause of some 35 million deaths each year, the international community is being increasingly called to take measures for the prevention and control of chronic diseases and mitigate their impacts on the world population, especially on women, who are usually the primary caregivers.
",Health
"For example, the rate of type 2 diabetes, associated with obesity, has been on the rise in countries previously plagued by hunger. In low-income countries, the number of individuals with diabetes is expected to increase from 84 million to 228 million by 2030.[57] Obesity, a preventable condition, is associated with numerous chronic diseases, including cardiovascular conditions, stroke, certain cancers, and respiratory disease. About 16% of the global burden of disease, measured as DALYs, has been accounted for by obesity.[57]
",Health
"More than one billion people were treated for at least one neglected tropical disease in 2015.[58] Neglected tropical diseases are a diverse group of infectious diseases that are endemic in tropical and subtropical regions of 149 countries, primarily effecting low and middle income populations in Africa, Asia, and Latin America. They are variously caused by bacteria (Trachoma, Leprosy), viruses (Dengue,[59] Rabies), protozoa (Human African trypanosomiasis, Chagas), and helminths (Schistosomiasis, Onchocerciasis, Soil transmitted helminths).[60] The Global Burden of Disease Study concluded that neglected tropical diseases comprehensively contributed to approximately 26.06 million disability-adjusted life years in 2010, as well as significant deleterious economic effects.[61] In 2011, the World Health Organization launched a 2020 Roadmap for neglected tropical diseases, aiming for the control or elimination of 10 common diseases.[62] The 2012 London Declaration builds on this initiative, and called on endemic countries and the international community to improve access to clean water and basic sanitation, improved living conditions, vector control, and health education, to reach the 2020 goals.[63] In 2017, a WHO report cited 'unprecedented progress' against neglected tropical diseases since 2007, especially due to mass drug administration of drugs donated by pharmaceutical companies.[64]
",Health
"Global interventions for improved child health and survival include the promotion of breastfeeding, zinc supplementation, vitamin A fortification, salt iodization, hygiene interventions such as hand-washing, vaccinations, and treatments of severe acute malnutrition.[32][65][66] The Global Health Council suggests a list of 32 treatments and health interventions that could potentially save several million lives each year.[67]
",Health
"Many populations face an ""outcome gap"", which refers to the gap between members of a population who have access to medical treatment versus those who do not. Countries facing outcome gaps lack sustainable infrastructure.[68] In Guatemala, a subset of the public sector, the Programa de Accessibilidad a los Medicamentos (""Program for Access to Medicines""), had the lowest average availability (25%) compared to the private sector (35%). In the private sector, highest- and lowest-priced medicines were 22.7 and 10.7 times more expensive than international reference prices respectively. Treatments were generally unaffordable, costing as much as 15 days wages for a course of the antibiotic ceftriaxone.[69] The public sector in Pakistan, while having access to medicines at a lower price than international reference prices, has a chronic shortage of and lack of access to basic medicines.[70]
",Health
"Journalist Laurie Garrett argues that the field of global health is not plagued by a lack of funds, but that more funds do not always translate into positive outcomes. The problem lies in the way these funds are allocated, as they are often disproportionately allocated to alleviating a single disease.[71]
",Health
"In its 2006 World Health Report, the WHO estimated a shortage of almost 4.3 million doctors, midwives, nurses, and support workers worldwide, especially in sub-Saharan Africa.[72]
",Health
"The Global Health Security Agenda (GHSA) is ""a multilateral, multi-sector effort that includes 60 participating countries and numerous private and public international organizations focused on building up worldwide health security capabilities toward meeting such threats"" as the spread of infectious disease. On March 26-28, 2018, the GHSA held its last high-level meeting which was located in Tbilisi, Georgia on biosurveillance of infectious disease threats, ""which include such modern-day examples as HIV/AIDS, severe acute respiratory syndrome (SARS), H1N1 influenza, multi-drug resistant tuberculosis — any emerging or reemerging disease that threatens human health and global economic stability.""[73] This event brought together GHSA partner countries, contributing countries of Real-Time Surveillance Action Package, and international partner organizations supporting the strengthening of capacities to detect infectious disease threats within the Real-Time Surveillance Action Package and other cross-cutting packages. Georgia is  the lead country for the Real-Time Surveillance Action Package.[74]
",Health
"
",Health
"Public health is ""the science and art of preventing disease, prolonging life and promoting human health through organized efforts and informed choices of society, organizations, public and private, communities and individuals"".[1]  Analyzing the health of a population and the threats is the basis for public health.[2] The ""public"" in question can be as small as a handful of people, an entire village or it can be as large as several continents, in the case of a pandemic. ""Health"" takes into account physical, mental and social well-being. It is not merely the absence of disease or infirmity, according to the World Health Organization.[3] Public health is interdisciplinary. For example, epidemiology, biostatistics and health services are all relevant. Environmental health, community health, behavioral health, health economics, public policy, mental health and occupational safety, gender issues in health, sexual and reproductive health are other important subfields.
",Health
"Public health aims to improve the quality of life through prevention and treatment of disease, including mental health. This is done through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promoting handwashing and breastfeeding, delivery of vaccinations, suicide prevention and distribution of condoms to control the spread of sexually transmitted diseases.
",Health
"Modern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, medical assistants, public health nurses, midwives, medical microbiologists, economists, sociologists, geneticists and data managers.  Depending on the need environmental health officers or public health inspectors, bioethicists, and even veterinarians, gender experts, sexual and reproductive health specialists might be called on.[4]
",Health
"Access to health care and public health initiatives are difficult challenges in developing countries. Public health infrastructures are still forming in those countries.
",Health
"The focus of a public health intervention is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behaviors, communities and environments. Many diseases are preventable through simple, nonmedical methods. For example, research has shown that the simple act of handwashing with soap can prevent the spread of many contagious diseases.[5] In other cases, treating a disease or controlling a pathogen can be vital to preventing its spread to others, either during an outbreak of infectious disease or through contamination of food or water supplies. Public health communications programs, vaccination programs and distribution of condoms are examples of common preventive public health measures. Measures such as these have contributed greatly to the health of populations and increases in life expectancy.
",Health
"Public health plays an important role in disease prevention efforts in both the developing world and in developed countries through local health systems and non-governmental organizations. The World Health Organization (WHO) is the international agency that coordinates and acts on global public health issues. Most countries have their own government public health agencies, sometimes known as ministries of health, to respond to domestic health issues. For example, in the United States, the front line of public health initiatives are state and local health departments. The United States Public Health Service (PHS), led by the Surgeon General of the United States, and the Centers for Disease Control and Prevention, headquartered in Atlanta, are involved with several international health activities, in addition to their national duties. In Canada, the Public Health Agency of Canada is the national agency responsible for public health, emergency preparedness and response, and infectious and chronic disease control and prevention. The Public health system in India is managed by the Ministry of Health & Family Welfare of the government of India with state-owned health care facilities.
",Health
"Most governments recognize the importance of public health programs in reducing the incidence of disease, disability, and the effects of aging and other physical and mental health conditions. However, public health generally receives significantly less government funding compared with medicine.[7] Public health programs providing vaccinations have made strides in promoting health, including the eradication of smallpox, a disease that plagued humanity for thousands of years.
",Health
"The World Health Organization (WHO) identifies core functions of public health programs including:[8]
",Health
"In particular, public health surveillance programs can:[9]
",Health
"Public health surveillance has led to the identification and prioritization of many public health issues facing the world today, including HIV/AIDS, diabetes, waterborne diseases, zoonotic diseases, and antibiotic resistance leading to the reemergence of infectious diseases such as tuberculosis. Antibiotic resistance, also known as drug resistance, was the theme of World Health Day 2011. Although the prioritization of pressing public health issues is important, Laurie Garrett argues that there are following consequences.[10] When foreign aid is funnelled into disease-specific programs, the importance of public health in general is disregarded. This public health problem of stovepiping is thought to create a lack of funds to combat other existing diseases in a given country.
",Health
"For example, the WHO reports that at least 220 million people worldwide suffer from diabetes. Its incidence is increasing rapidly, and it is projected that the number of diabetes deaths will double by the year 2030.[11] In a June 2010 editorial in the medical journal The Lancet, the authors opined that ""The fact that type 2 diabetes, a largely preventable disorder, has reached epidemic proportion is a public health humiliation.""[12] The risk of type 2 diabetes is closely linked with the growing problem of obesity. The WHO’s latest estimates as of June 2016 highlighted that globally approximately 1.9 billion adults were overweight in 2014, and 41 million children under the age of five were overweight in 2014.[13] The United States is the leading country with 30.6% of its population being obese. Mexico follows behind with 24.2% and the United Kingdom with 23%. Once considered a problem in high-income countries, it is now on the rise in low-income countries, especially in urban settings. Many public health programs are increasingly dedicating attention and resources to the issue of obesity, with objectives to address the underlying causes including healthy diet and physical exercise.
",Health
"Some programs and policies associated with public health promotion and prevention can be controversial. One such example is programs focusing on the prevention of HIV transmission through safe sex campaigns and needle-exchange programmes. Another is the control of tobacco smoking. Changing smoking behavior requires long-term strategies, unlike the fight against communicable diseases, which usually takes a shorter period for effects to be observed. Many nations have implemented major initiatives to cut smoking, such as increased taxation and bans on smoking in some or all public places. Proponents[who?] argue by presenting evidence that smoking is one of the major killers, and that therefore governments have a duty to reduce the death rate, both through limiting passive (second-hand) smoking and by providing fewer opportunities for people to smoke. Opponents[who?] say that this undermines individual freedom and personal responsibility, and worry that the state may be emboldened to remove more and more choice in the name of better population health overall.
",Health
"Simultaneously, while communicable diseases have historically ranged uppermost as a global health priority, non-communicable diseases and the underlying behavior-related risk factors have been at the bottom. This is changing, however, as illustrated by the United Nations hosting its first General Assembly Special Summit on the issue of non-communicable diseases in September 2011.[14]
",Health
"Many health problems are due to maladaptive personal behaviors. From an evolutionary psychology perspective, over consumption of novel substances that are harmful is due to the activation of an evolved reward system for substances such as drugs, tobacco, alcohol, refined salt, fat, and carbohydrates. New technologies such as modern transportation also cause reduced physical activity. Research has found that behavior is more effectively changed by taking evolutionary motivations into consideration instead of only presenting information about health effects. The marketing industry has long known the importance of associating products with high status and attractiveness to others. Films are increasingly being recognized as a public health tool[15]. In fact, film festivals and competitions have been established to specifically promote films about health.[16] Conversely, it has been argued that emphasizing the harmful and undesirable effects of tobacco smoking on other persons and imposing smoking bans in public places have been particularly effective in reducing tobacco smoking.[17]
",Health
"As well as seeking to improve population health through the implementation of specific population-level interventions, public health contributes to medical care by identifying and assessing population needs for health care services, including:[18][19][20][21]
",Health
"To improve public health, one important strategy is to promote modern medicine and scientific neutrality to drive the public health policy and campaign, which is recommended by Armanda Solorzana, through a case study of the Rockefeller Foundation's hookworm campaign in Mexico in the 1920s.  Soloranza argues that public health policy can't concern only politics or economics.  Political concerns can lead government officials to hide the real numbers of people affected by disease in their regions, such as upcoming elections. Therefore, scientific neutrality in making public health policy is critical; it can ensure treatment needs are met regardless of political and economic conditions.[22]
",Health
"The history of public health care clearly shows the global effort to improve health care for all.[citation needed] However, in modern-day medicine, real, measurable change has not been clearly seen, and critics argue that this lack of improvement is due to ineffective methods that are being implemented. As argued by Paul E. Farmer, structural interventions could possibly have a large impact, and yet there are numerous problems as to why this strategy has yet to be incorporated into the health system. One of the main reasons that he suggests could be the fact that physicians are not properly trained to carry out structural interventions, meaning that the ground level health care professionals cannot implement these improvements. While structural interventions can not be the only area for improvement, the lack of coordination between socioeconomic factors and health care for the poor could be counterproductive, and end up causing greater inequity between the health care services received by the rich and by the poor. Unless health care is no longer treated as a commodity, global public health will ultimately not be achieved.[citation needed] This being the case, without changing the way in which health care is delivered to those who have less access to it, the universal goal of public health care cannot be achieved.[23]
",Health
"Another reason why measurable changes may not be noticed in public health is because agencies themselves may not be measuring their programs' efficacy. Perrault et al.[24] analyzed over 4,000 published objectives from Community Health Improvement Plans (CHIPs) of 280 local accredited and non-accredited public health agencies in the U.S., and found that the majority of objectives - around two-thirds - were focused on achieving agency outputs (e.g., developing communication plans, installing sidewalks, disseminating data to the community).  Only about one-third focused on seeking measurable changes in the populations they serve (i.e., changing people's knowledge, attitudes, behaviors). What this research showcases is that if agencies are only focused on accomplishing tasks (i.e., outputs) and do not have a focus on measuring actual changes in their populations with the activities they perform, it should not be surprising when measurable changes are not reported. Perrault et al.[24] advocate for public health agencies to work with those in the discipline of Health Communication to craft objectives that are measurable outcomes, and to assist agencies in developing tools and methods to be able to track more proximal changes in their target populations (e.g., knowledge and attitude shifts) that may be influenced by the activities the agencies are performing.
",Health
"Public Health 2.0 is a movement within public health that aims to make the field more accessible to the general public and more user-driven. The term is used in three senses.  In the first sense, ""Public Health 2.0"" is similar to ""Health 2.0"" and describes the ways in which traditional public health practitioners and institutions are reaching out (or could reach out) to the public through social media and health blogs.[25][26]
",Health
"In the second sense, ""Public Health 2.0"" describes public health research that uses data gathered from social networking sites, search engine queries, cell phones, or other technologies.[27] A recent example is the proposal of statistical framework that utilizes online user-generated content (from social media or search engine queries) to estimate the impact of an influenza vaccination campaign in the UK.[28]
",Health
"In the third sense, ""Public Health 2.0"" is used to describe public health activities that are completely user-driven.[29] An example is the collection and sharing of information about environmental radiation levels after the March 2011 tsunami in Japan.[30] In all cases, Public Health 2.0 draws on ideas from Web 2.0, such as crowdsourcing, information sharing, and user-centred design.[31] While many individual healthcare providers have started making their own personal contributions to ""Public Health 2.0"" through personal blogs, social profiles, and websites, other larger organizations, such as the American Heart Association (AHA) and United Medical Education (UME), have a larger team of employees centered around online driven health education, research, and training. These private organizations recognize the need for free and easy to access health materials often building libraries of educational articles.[citation needed]
",Health
"There is a great disparity in access to health care and public health initiatives between developed nations and developing nations. In the developing world, public health infrastructures are still forming. There may not be enough trained health workers, monetary resources or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention.[32][33] As a result, a large majority of disease and mortality in the developing world results from and contributes to extreme poverty. For example, many African governments spend less than US$10 per person per year on health care, while, in the United States, the federal government spent approximately US$4,500 per capita in 2000. However, expenditures on health care should not be confused with spending on public health. Public health measures may not generally be considered ""health care"" in the strictest sense. For example, mandating the use of seat belts in cars can save countless lives and contribute to the health of a population, but typically money spent enforcing this rule would not count as money spent on health care.
",Health
"Large parts of the world remained plagued by largely preventable or treatable infectious diseases.  In addition to this however, many low- and middle-income countries are also experiencing an epidemiological shift and polarization in which populations are now experiencing more of the effects of chronic diseases as life expectancy increases with, the poorer communities being heavily affected by both chronic and infectious diseases.[33] Another major public health concern in the developing world is poor maternal and child health, exacerbated by malnutrition and poverty.  The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year.[34] Intermittent preventive therapy aimed at treating and preventing malaria episodes among pregnant women and young children is one public health measure in endemic countries.
",Health
"Each day brings new front-page headlines about public health: emerging infectious diseases such as SARS, rapidly making its way from China (see Public health in China) to Canada, the United States and other geographically distant countries; reducing inequities in health care access through publicly funded health insurance programs; the HIV/AIDS pandemic and its spread from certain high-risk groups to the general population in many countries, such as in South Africa; the increase of childhood obesity and the concomitant increase in type II diabetes among children; the social, economic and health effects of adolescent pregnancy; and the public health challenges related to natural disasters such as the 2004 Indian Ocean tsunami, 2005's Hurricane Katrina in the United States and the 2010 Haiti earthquake.
",Health
"Since the 1980s, the growing field of population health has broadened the focus of public health from individual behaviors and risk factors to population-level issues such as inequality, poverty, and education. Modern public health is often concerned with addressing determinants of health across a population. There is a recognition that our health is affected by many factors including where we live, genetics, our income, our educational status and our social relationships; these are known as ""social determinants of health"". The upstream drivers such as environment, education, employment, income, food security, housing, social inclusion and many others effect the distribution of health between and within populations and are often shaped by policy.[35] A social gradient in health runs through society. The poorest generally suffer the worst health, but even the middle classes will generally have worse health outcomes than those of a higher social stratum.[36] The new public health advocates for population-based policies that improve health in an equitable manner.
",Health
"Health aid to developing countries is an important source of public health funding for many low- and middle-income countries.[37] Health aid to developing countries has shown a significant increase after World War II as concerns over the spread of disease as a result of globalization increased and the HIV/AIDS epidemic in sub-Saharan Africa surfaced.[38][39] From 1990 to 2010, total health aid from developed countries increased from 5.5 billion to 26.87 billion with wealthy countries continuously donating billions of dollars every year with the goal of improving population health.[39] Some efforts, however, receive a significantly larger proportion of funds such as HIV which received an increase in funds of over $6 billion dollars between 2000 and 2010 which was more than twice the increase seen in any other sector during those years.[37] Health aid has seen an expansion through multiple channels including private philanthropy, non-governmental organizations, private foundations such as the Bill & Melinda Gates Foundation, bilateral donors, and multilateral donors such as the World Bank or UNICEF.[39] In 2009 health aid from the OECD amounted to $12.47 billion which amounted to 11.4% of its total bilateral aid.[40] In 2009, Multilateral donors were found to spend 15.3% of their total aid on bettering public healthcare.[40] Recent data, however, shows that international health aid has plateaued and may begin to decrease.[37]
",Health
"Debates exist questioning the efficacy of international health aid. Proponents of aid claim that health aid from wealthy countries is necessary in order for developing countries to escape the poverty trap. Opponents of health aid claim that international health aid actually disrupts developing countries' course of development, causes dependence on aid, and in many cases the aid fails to reach its recipients.[37] For example, recently, health aid was funneled towards initiatives such as financing new technologies like antiretroviral medication, insecticide-treated mosquito nets, and new vaccines. The positive impacts of these initiatives can be seen in the eradication of smallpox and polio; however, critics claim that misuse or misplacement of funds may cause many of these efforts to never come into fruition.[37]
",Health
"Economic modeling based on the Institute for Health Metrics and Evaluation and the World Health Organization has shown a link between international health aid in developing countries and a reduction in adult mortality rates.[39] However, a 2014-2016 study suggests that a potential confounding variable for this outcome is the possibility that aid was directed at countries once they were already on track for improvement.[37] That same study, however, also suggests that 1 billion dollars in health aid was associated with 364,000 fewer deaths occurring between ages 0 and 5 in 2011.[37]
",Health
"To address current and future challenges in addressing health issues in the world, the United Nations have developed the Sustainable Development Goals building off of the Millennium Development Goals of 2000 to be completed by 2030.[41] These goals in their entirety encompass the entire spectrum of development across nations, however Goals 1-6 directly address health disparities, primarily in developing countries.[42] These six goals address key issues in global public health: Poverty, Hunger and food security, Health, Education, Gender equality and women's empowerment, and water and sanitation.[42] Public health officials can use these goals to set their own agenda and plan for smaller scale initiatives for their organizations. These goals hope to lessen the burden of disease and inequality faced by developing countries and lead to a healthier future.
",Health
"The links between the various sustainable development goals and public health are numerous and well established:
",Health
"The U.S. Global Health Initiative was created in 2009 by President Obama in an attempt to have a more holistic, comprehensive approach to improving global health as opposed to previous, disease-specific interventions.[48] The Global Health Initiative is a six-year plan, ""to develop a comprehensive U.S. government strategy for global health, building on the President's Emergency Plan for AIDS Relief (PEPFAR) to combat HIV as well as U.S. efforts to address tuberculosis (TB) and malaria, and augmenting the focus on other global health priorities, including neglected tropical diseases (NTDs), maternal, newborn and child health (MNCH), family planning and reproductive health (FP/RH), nutrition, and health systems strengthening (HSS)"".[48] The GHI programs are being implemented in more than 80 countries around the world and works closely with the United States Agency for International Development, the Centers for Disease Control and Prevention, the United States Deputy Secretary of State.[48]
",Health
"There are seven core principles:
",Health
"The aid effectiveness agenda is a useful tool for measuring the impact of these large scale programs such as The Global Fund to Fight AIDS, Tuberculosis and Malaria and the Global Alliance for Vaccines and Immunization  (GAVI) which have been successful in achieving rapid and visible results.[40] The Global Fund claims that its efforts have provided antiretroviral treatment for over three million people worldwide.[40] GAVI claims that its vaccination programs have prevented over 5 million deaths since it began in 2000.[40]
",Health
"Education and training of public health professionals is available throughout the world in Schools of Public Health, Medical Schools, Veterinary Schools, Schools of Nursing, and Schools of Public Affairs. The training typically requires a university degree with a focus on core disciplines of biostatistics, epidemiology, health services administration, health policy, health education, behavioral science, gender issues, sexual and reproductive health, public health nutrition and environmental and occupational health.[49][50] In the global context, the field of public health education has evolved enormously in recent decades, supported by institutions such as the World Health Organization and the World Bank, among others. Operational structures are formulated by strategic principles, with educational and career pathways guided by competency frameworks, all requiring modulation according to local, national and global realities. It is critically important for the health of populations that nations assess their public health human resource needs and develop their ability to deliver this capacity, and not depend on other countries to supply it.[51]
",Health
"In the United States, the Welch-Rose Report of 1915[52] has been viewed as the basis for the critical movement in the history of the institutional schism between public health and medicine because it led to the establishment of schools of public health supported by the Rockefeller Foundation.[53] The report was authored by William Welch, founding dean of the Johns Hopkins Bloomberg School of Public Health, and Wickliffe Rose of the Rockefeller Foundation. The report focused more on research than practical education.[53][54] Some have blamed the Rockefeller Foundation's 1916 decision to support the establishment of schools of public health for creating the schism between public health and medicine and legitimizing the rift between medicine's laboratory investigation of the mechanisms of disease and public health's nonclinical concern with environmental and social influences on health and wellness.[53][55]
",Health
"Even though schools of public health had already been established in Canada, Europe and North Africa, the United States had still maintained the traditional system of housing faculties of public health within their medical institutions.  A $25,000 donation from businessman Samuel Zemurray instituted the School of Public Health and Tropical Medicine at Tulane University in 1912 conferring its first doctor of public health degree in 1914.[56][57]  The Yale School of Public Health was founded by Charles-Edward Avory Winslow in 1915.[58] The Johns Hopkins School of Hygiene and Public Health became an independent, degree-granting institution for research and training in public health, and the largest public health training facility in the United States,[59][60][61][62] when it was founded in 1916. By 1922, schools of public health were established at Columbia and Harvard on the Hopkins model. By 1999 there were twenty nine schools of public health in the US, enrolling around fifteen thousand students.[49][53]
",Health
"Over the years, the types of students and training provided have also changed. In the beginning, students who enrolled in public health schools typically had already obtained a medical degree; public health school training was largely a second degree for medical professionals. However, in 1978, 69% of American students enrolled in public health schools had only a bachelor's degree.[49]
",Health
"Schools of public health offer a variety of degrees which generally fall into two categories: professional or academic.[63] The two major postgraduate degrees are the Master of Public Health (MPH) or the Master of Science in Public Health (MSPH). Doctoral studies in this field include Doctor of Public Health (DrPH) and Doctor of Philosophy (PhD) in a subspeciality of greater Public Health disciplines. DrPH is regarded as a professional degree and PhD as more of an academic degree.
",Health
"Professional degrees are oriented towards practice in public health settings. The Master of Public Health, Doctor of Public Health, Doctor of Health Science (DHSc) and the Master of Health Care Administration are examples of degrees which are geared towards people who want careers as practitioners of public health in health departments, managed care and community-based organizations, hospitals and consulting firms, among others. Master of Public Health degrees broadly fall into two categories, those that put more emphasis on an understanding of epidemiology and statistics as the scientific basis of public health practice and those that include a more eclectic range of methodologies.  A Master of Science of Public Health is similar to an MPH but is considered an academic degree (as opposed to a professional degree) and places more emphasis on scientific methods and research.  The same distinction can be made between the DrPH and the DHSc. The DrPH is considered a professional degree and the DHSc is an academic degree.[citation needed]
",Health
"Academic degrees are more oriented towards those with interests in the scientific basis of public health and preventive medicine who wish to pursue careers in research, university teaching in graduate programs, policy analysis and development, and other high-level public health positions. Examples of academic degrees are the Master of Science, Doctor of Philosophy, Doctor of Science (ScD), and Doctor of Health Science (DHSc). The doctoral programs are distinct from the MPH and other professional programs by the addition of advanced coursework and the nature and scope of a dissertation research project.
",Health
"In the United States, the Association of Schools of Public Health[64] represents Council on Education for Public Health (CEPH) accredited schools of public health.[65] Delta Omega is the honor society for graduate studies in public health. The society was founded in 1924 at the Johns Hopkins School of Hygiene and Public Health. Currently, there are approximately 68 chapters throughout the United States and Puerto Rico.[66]
",Health
"Public health has early roots in antiquity. From the beginnings of human civilization, it was recognized that polluted water and lack of proper waste disposal spread communicable diseases (theory of miasma). Early religions attempted to regulate behavior that specifically related to health, from types of food eaten, to regulating certain indulgent behaviors, such as drinking alcohol or sexual relations. Leaders were responsible for the health of their subjects to ensure social stability, prosperity, and maintain order.
",Health
"By Roman times, it was well understood that proper diversion of human waste was a necessary tenet of public health in urban areas. The ancient Chinese medical doctors developed the practice of variolation following a smallpox epidemic around 1000 BC. An individual without the disease could gain some measure of immunity against it by inhaling the dried crusts that formed around lesions of infected individuals. Also, children were protected by inoculating a scratch on their forearms with the pus from a lesion.
",Health
"In 1485 the Republic of Venice established a permanent Venetian Magistrate for Health comprising supervisors of health with special attention to the prevention of the spread of epidemics in the territory from abroad. The three supervisors were initially appointed by the Venetian Senate. In 1537 it was assumed by the Grand Council, and in 1556 added two judges, with the task of control, on behalf of the Republic, the efforts of the supervisors.
",Health
"However, according to Michel Foucault, the plague model of governmentality was later controverted by the cholera model. A Cholera pandemic devastated Europe between 1829 and 1851, and was first fought by the use of what Foucault called ""social medicine"", which focused on flux, circulation of air, location of cemeteries, etc. All those concerns, born of the miasma theory of disease, were mixed with urbanistic concerns for the management of populations, which Foucault designated as the concept of ""biopower"". The German conceptualized this in the Polizeiwissenschaft (""Police science"").
",Health
"The 18th century saw rapid growth in voluntary hospitals in England.[67] The latter part of the century brought the establishment of the basic pattern of improvements in public health over the next two centuries: a social evil was identified, private philanthropists brought attention to it, and changing public opinion led to government action.[68]
",Health
"The practice of vaccination became prevalent in the 1800s, following the pioneering work of Edward Jenner in treating smallpox. James Lind's discovery of the causes of scurvy amongst sailors and its mitigation via the introduction of fruit on lengthy voyages was published in 1754 and led to the adoption of this idea by the Royal Navy.[69] Efforts were also made to promulgate health matters to the broader public; in 1752 the British physician Sir John Pringle published Observations on the Diseases of the Army in Camp and Garrison, in which he advocated for the importance of adequate ventilation in the military barracks and the provision of latrines for the soldiers.[70]
",Health
"With the onset of the Industrial Revolution, living standards amongst the working population began to worsen, with cramped and unsanitary urban conditions. In the first four decades of the 19th century alone, London's population doubled and even greater growth rates were recorded in the new industrial towns, such as Leeds and Manchester. This rapid urbanisation exacerbated the spread of disease in the large conurbations that built up around the workhouses and factories. These settlements were cramped and primitive with no organized sanitation. Disease was inevitable and its incubation in these areas was encouraged by the poor lifestyle of the inhabitants. Unavailable housing led to the rapid growth of slums and the per capita death rate began to rise alarmingly, almost doubling in Birmingham and Liverpool. Thomas Malthus warned of the dangers of overpopulation in 1798. His ideas, as well as those of Jeremy Bentham, became very influential in government circles in the early years of the 19th century.[68]
",Health
"The first attempts at sanitary reform and the establishment of public health institutions were made in the 1840s. Thomas Southwood Smith, physician at the London Fever Hospital, began to write papers on the importance of public health, and was one of the first physicians brought in to give evidence before the Poor Law Commission in the 1830s, along with Neil Arnott and James Phillips Kay.[71] Smith advised the government on the importance of quarantine and sanitary improvement for limiting the spread of infectious diseases such as cholera and yellow fever.[72][73]
",Health
"The Poor Law Commission reported in 1838 that ""the expenditures necessary to the adoption and maintenance of measures of prevention would ultimately amount to less than the cost of the disease now constantly engendered"". It recommended the implementation of large scale government engineering projects to alleviate the conditions that allowed for the propagation of disease.[68] The Health of Towns Association was formed in Exeter on 11 December 1844, and vigorously campaigned for the development of public health in the United Kingdom.[74] Its formation followed the 1843 establishment of the Health of Towns Commission, chaired by Sir Edwin Chadwick, which produced a series of reports on poor and insanitary conditions in British cities.[74]
",Health
"These national and local movements led to the Public Health Act, finally passed in 1848. It aimed to improve the sanitary condition of towns and populous places in England and Wales by placing the supply of water, sewerage, drainage, cleansing and paving under a single local body with the General Board of Health as a central authority. The Act was passed by the Liberal government of Lord John Russell, in response to the urging of Edwin Chadwick. Chadwick's seminal report on The Sanitary Condition of the Labouring Population was published in 1842[75] and was followed up with a supplementary report a year later.[76]
",Health
"Vaccination for various diseases was made compulsory in the United Kingdom in 1851, and by 1871 legislation required a comprehensive system of registration run by appointed vaccination officers.[77]
",Health
"Further interventions were made by a series of subsequent Public Health Acts, notably the 1875 Act. Reforms included latrinization, the building of sewers, the regular collection of garbage followed by incineration or disposal in a landfill, the provision of clean water and the draining of standing water to prevent the breeding of mosquitoes.
",Health
"The Infectious Disease (Notification) Act 1889 mandated the reporting of infectious diseases to the local sanitary authority, which could then pursue measures such as the removal of the patient to hospital and the disinfection of homes and properties.[78]
",Health
"In the United States, the first public health organization based on a state health department and local boards of health was founded in New York City in 1866.[79]
",Health
"The science of epidemiology was founded by John Snow's identification of a polluted public water well as the source of an 1854 cholera outbreak in London. Dr. Snow believed in the germ theory of disease as opposed to the prevailing miasma theory. He first publicized his theory in an essay, On the Mode of Communication of Cholera, in 1849, followed by a more detailed treatise in 1855 incorporating the results of his investigation of the role of the water supply in the Soho epidemic of 1854.[80]
",Health
"By talking to local residents (with the help of Reverend Henry Whitehead), he identified the source of the outbreak as the public water pump on Broad Street (now Broadwick Street).  Although Snow's chemical and microscope examination of a water sample from the Broad Street pump did not conclusively prove its danger, his studies of the pattern of the disease were convincing enough to persuade the local council to disable the well pump by removing its handle.[81]
",Health
"Snow later used a dot map to illustrate the cluster of cholera cases around the pump.  He also used statistics to illustrate the connection between the quality of the water source and cholera cases.  He showed that the Southwark and Vauxhall Waterworks Company was taking water from sewage-polluted sections of the Thames and delivering the water to homes, leading to an increased incidence of cholera. Snow's study was a major event in the history of public health and geography.  It is regarded as the founding event of the science of epidemiology.[82][83]
",Health
"With the pioneering work in bacteriology of French chemist Louis Pasteur and German scientist Robert Koch, methods for isolating the bacteria responsible for a given disease and vaccines for remedy were developed at the turn of the 20th century. British physician Ronald Ross identified the mosquito as the carrier of malaria and laid the foundations for combating the disease.[84] Joseph Lister revolutionized surgery by the introduction of antiseptic surgery to eliminate infection. French epidemiologist Paul-Louis Simond proved that plague was carried by fleas on the back of rats,[85] and Cuban scientist Carlos J. Finlay and U.S. Americans Walter Reed and James Carroll demonstrated that mosquitoes carry the virus responsible for yellow fever.[86][87] Brazilian scientist Carlos Chagas identified a tropical disease and its vector.[88]
",Health
"With onset of the epidemiological transition and as the prevalence of infectious diseases decreased through the 20th century, public health began to put more focus on chronic diseases such as cancer and heart disease. Previous efforts in many developed countries had already led to dramatic reductions in the infant mortality rate using preventative methods. In Britain, the infant mortality rate fell from over 15% in 1870 to 7% by 1930.[89]
",Health
"France 1871-1914  followed well behind Bismarckian Germany, as well as Great Britain, in developing the welfare state including public health.  Tuberculosis was the most dreaded disease of the day, especially striking young people in their 20s.  Germany set up vigorous measures of public hygiene and public sanatoria, but France let private physicians handle the problem, which left it with a much higher death rate.[90]  The French medical profession jealously guarded its prerogatives, and public health activists were not as well organized or as influential as in Germany, Britain or the United States.[91][92]  For example, there was a long battle over a public health law which began in the 1880s as a campaign to reorganize the nation's health services, to require the registration of infectious diseases, to mandate quarantines, and to improve the deficient health and housing legislation of 1850. However the reformers met opposition from bureaucrats, politicians, and physicians. Because it was so threatening to so many interests, the proposal was debated and postponed for 20 years before becoming law in 1902. Success finally came when the government realized that contagious diseases had a national security impact in weakening military recruits, and keeping the population growth rate well below Germany's.[93]
",Health
"Modern public health began developing in the 19th century, as a response to advances in science that led to the understanding of, the source and spread of disease. As the knowledge of contagious diseases increased, means to control them and prevent infection were soon developed. Once it became understood that these strategies would require community-wide participation, disease control began being viewed as a public responsibility. Various organizations and agencies were then created to implement these disease preventing strategies.[94]
",Health
"Most of the Public health activity in the United States took place at the municipal level before the mid-20th century. There was some activity at the national and state level as well.[95]
",Health
"In the administration of the second president of the United States John Adams, the Congress authorized the creation of hospitals for mariners.  As the U.S. expanded, the scope of the governmental health agency expanded.
In the United States, public health worker Sara Josephine Baker, M.D. established many programs to help the poor in New York City keep their infants healthy, leading teams of nurses into the crowded neighborhoods of Hell's Kitchen and teaching mothers how to dress, feed, and bathe their babies.
",Health
"Another key pioneer of public health in the U.S. was Lillian Wald, who founded the Henry Street Settlement house in New York.  The Visiting Nurse Service of New York was a significant organization for bringing health care to the urban poor.
",Health
"Dramatic increases in average life span in the late 19th century and 20th century, is widely credited to public health achievements, such as vaccination programs and control of many infectious diseases including polio, diphtheria, yellow fever and smallpox; effective health and safety policies such as road traffic safety and occupational safety; improved family planning; tobacco control measures; and programs designed to decrease non-communicable diseases by acting on known risk factors such as a person's background, lifestyle and environment.
",Health
"Another major public health improvement was the decline in the ""urban penalty"" brought about by improvements in sanitation. These improvements included chlorination of drinking water, filtration and sewage treatment which led to the decline in deaths caused by infectious waterborne diseases such as cholera and intestinal diseases.[96]
The federal Office of Indian Affairs (OIA) operated a large-scale field nursing program. Field nurses targeted native women for health education, emphasizing personal hygiene and infant care and nutrition.[97]
",Health
"Public health issues were important for the Spanish empire during the colonial era.  Epidemic disease was the main factor in the decline of indigenous populations in the era immediately following the sixteenth-century conquest era and was a problem during the colonial era. The Spanish crown took steps in eighteenth-century Mexico to bring in regulations to make populations healthier.[98]
",Health
"In the late nineteenth century, Mexico was in the process of modernization, and public health issues were again tackled from a scientific point of view.[99][100][101][102][103][104]  Even during the Mexican Revolution (1910–20), public health was an important concern, with a text on hygiene published in 1916.[105] During the Mexican Revolution, feminist and trained nurse Elena Arizmendi Mejia founded the Neutral White Cross, treating wounded soldiers no matter for what faction they fought.
",Health
"In the post-revolutionary period after 1920, improved public health was a revolutionary goal of the Mexican government.[106][107]
The Mexican state promoted the health of the Mexican population, with most resources going to cities.[108][109] Concern about disease conditions and social impediments to the improvement of Mexicans' health were important in the formation of the Mexican Society for Eugenics. The movement flourished from the 1920s to the 1940s.[110] Mexico was not alone in Latin America or the world in promoting eugenics.[111]  Government campaigns against disease and alcoholism were also seen as promoting public health.[112][113]
",Health
"The Mexican Social Security Institute was established in 1943, during the administration of President Manuel Avila Camacho to deal with public health, pensions, and social security.
",Health
"Since the 1959 Cuban Revolution the Cuban government has devoted extensive resources to the improvement of health conditions for its entire population via universal access to health care. Infant mortality has plummeted.[114] Cuban medical internationalism as a policy has seen the Cuban government sent doctors as a form of aid and export to countries in need in Latin America, especially Venezuela, as well as Oceania and Africa countries.
",Health
"Public health was important elsewhere in Latin America in consolidating state power and integrating marginalized populations into the nation-state.  In Colombia, public health was a means for creating and implementing ideas of citizenship.[115] In Bolivia, a similar push came after their 1952 revolution.[116]
",Health
"Though curable and preventative, malaria remains a huge public health problem and is the third leading cause of death in Ghana.[117] In the absence of a vaccine, mosquito control, or access to anti-malaria medication, public health methods become the main strategy for reducing the prevalence and severity of malaria.[118] These methods include reducing breeding sites, screening doors and windows, insecticide sprays, prompt treatment following infection, and usage of insecticide treated mosquito nets.[118] Distribution and sale of insecticide-treated mosquito nets is a common, cost-effective anti-malaria public health intervention; however, barriers to use exist including cost, hosehold and family organization, access to resources, and social and behavioral determinants which have not only been shown to affect malaria prevalence rates but also mosquito net use.[119][118]
",Health
"International public health strategies and programs
",Health
"
",Health
"The following outline is provided as an overview of and topical guide to health sciences:
",Health
"Health sciences – are those sciences which focus on health, or health care, as core parts of their subject matter. Because these two subject matter relate to multiple academic disciplines, both STEM disciplines as well as emerging patient safety disciplines (such as social care research) are relevant to current health scientific knowledge.
",Health
"Health sciences knowledge bases are currently diverse, with intellectual foundations which are sometimes mutually-inconsistent. There is currently an existing bias in the field, towards high valuation of knowledge deriving from controlling views on human agency (as epitomized by the epistemological basis of Randomized Control Trial designs); compare this against the more naturalistic views on human agency taken by research based on Ethnography for example).
",Health
"Mental health
",Health
"Social health
",Health
"Physical health
",Health
"Medicine – applied science or practice of the diagnosis, treatment, and prevention of disease. It encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Some of its branches are:
",Health
"Health care or healthcare is the maintenance or improvement of health via the prevention, diagnosis, and  treatment of disease, illness, injury, and other physical and mental impairments in human beings. Health care is delivered by health professionals (providers or practitioners) in allied health fields. Physicians and physician associates are a part of these health professionals. Dentistry, midwifery, nursing, medicine, optometry, audiology, pharmacy, psychology, occupational therapy, physical therapy and other health professions are all part of health care. It includes work done in providing primary care, secondary care, and tertiary care, as well as in public health.
",Health
"Access to health care may vary across countries, communities, and individuals, largely influenced by social and economic conditions as well as the health policies in place. Countries and jurisdictions have different policies and plans in relation to the personal and population-based health care goals within their societies. Health care systems are organizations established to meet the health needs of targeted populations. Their exact configuration varies between national and subnational entities. In some countries and jurisdictions, health care planning is distributed among market participants, whereas in others, planning occurs more centrally among governments or other coordinating bodies. In all cases, according to the World Health Organization (WHO), a well-functioning health care system requires a robust financing mechanism; a well-trained and adequately paid workforce; reliable information on which to base decisions and policies; and well maintained health facilities and logistics to deliver quality medicines and technologies.[1]
",Health
"Health care can contribute to a significant part of a country's economy. In 2011, the health care industry consumed an average of 9.3 percent of the GDP or US$ 3,322 (PPP-adjusted) per capita across the 34 members of OECD countries. The US (17.7%, or US$ PPP 8,508), the Netherlands (11.9%, 5,099), France (11.6%, 4,118), Germany (11.3%, 4,495), Canada (11.2%, 5669), and Switzerland (11%, 5,634) were the top spenders, however life expectancy in total population at birth was highest in Switzerland (82.8 years), Japan and Italy (82.7), Spain and Iceland (82.4), France (82.2) and Australia (82.0), while OECD's average exceeds 80 years for the first time ever in 2011: 80.1 years, a gain of 10 years since 1970. The US (78.7 years) ranges only on place 26 among the 34 OECD member countries, but has the highest costs by far. All OECD countries have achieved universal (or almost universal) health coverage, except the US and Mexico.[2][3] (see also international comparisons.)
",Health
"Health care is conventionally regarded as an important determinant in promoting the general physical and mental health and well-being of people around the world. An example of this was the worldwide eradication of smallpox in 1980, declared by the WHO as the first disease in human history to be completely eliminated by deliberate health care interventions.[4]
",Health
"The delivery of modern health care depends on groups of trained professionals and paraprofessionals coming together as interdisciplinary teams.[5] This includes professionals in medicine, psychology, physiotherapy, nursing, dentistry, midwifery and allied health, along with many others such as public health practitioners, community health workers and assistive personnel, who systematically provide personal and population-based preventive, curative and rehabilitative care services.
",Health
"While the definitions of the various types of health care vary depending on the different cultural, political, organizational and disciplinary perspectives, there appears to be some consensus that primary care constitutes the first element of a continuing health care process and may also include the provision of secondary and tertiary levels of care.[6] Health care can be defined as either public or private.
",Health
"Primary care refers to the work of health professionals who act as a first point of consultation for all patients within the health care system.[6][8] Such a professional would usually be a primary care physician, such as a general practitioner or family physician. Another professional would be a licensed independent practitioner such as a physiotherapist, or a non-physician primary care provider such as a physician assistant or nurse practitioner. Depending on the locality, health system organization the patient may see another health care professional first, such as a pharmacist or nurse. Depending on the nature of the health condition, patients may be referred for secondary or tertiary care.
",Health
"Primary care is often used as the term for the health care services that play a role in the local community. It can be provided in different settings, such as Urgent care centers which provide same day appointments or services on a walk-in basis.
",Health
"Primary care involves the widest scope of health care, including all ages of patients, patients of all socioeconomic and geographic origins, patients seeking to maintain optimal health, and patients with all types of acute and chronic physical, mental and social health issues, including multiple chronic diseases. Consequently, a primary care practitioner must possess a wide breadth of knowledge in many areas. Continuity is a key characteristic of primary care, as patients usually prefer to consult the same practitioner for routine check-ups and preventive care, health education, and every time they require an initial consultation about a new health problem. The International Classification of Primary Care (ICPC) is a standardized tool for understanding and analyzing information on interventions in primary care based on the reason for the patient's visit.[9]
",Health
"Common chronic illnesses usually treated in primary care may include, for example: hypertension, diabetes, asthma, COPD, depression and anxiety, back pain, arthritis or thyroid dysfunction. Primary care also includes many basic maternal and child health care services, such as family planning services and vaccinations. In the United States, the 2013 National Health Interview Survey found that skin disorders (42.7%), osteoarthritis and joint disorders (33.6%), back problems (23.9%), disorders of lipid metabolism (22.4%), and upper respiratory tract disease (22.1%, excluding asthma) were the most common reasons for accessing a physician.[10]
",Health
"In the United States, primary care physicians have begun to deliver primary care outside of the managed care (insurance-billing) system through direct primary care which is a subset of the more familiar concierge medicine. Physicians in this model bill patients directly for services, either on a pre-paid monthly, quarterly, or annual basis, or bill for each service in the office. Examples of direct primary care practices include Foundation Health in Colorado and Qliance in Washington.
",Health
"In context of global population aging, with increasing numbers of older adults at greater risk of chronic non-communicable diseases, rapidly increasing demand for primary care services is expected in both developed and developing countries.[11][12] The World Health Organization attributes the provision of essential primary care as an integral component of an inclusive primary health care strategy.[6]
",Health
"Secondary care includes acute care: necessary treatment for a short period of time for a brief but serious illness, injury, or other health condition. This care is often found in a hospital emergency department. Secondary care also includes skilled attendance during childbirth, intensive care, and medical imaging services.
",Health
"The term ""secondary care"" is sometimes used synonymously with ""hospital care."" However, many secondary care providers, such as psychiatrists, clinical psychologists, occupational therapists, most dental specialties or physiotherapists do not necessarily work in hospitals. Some primary care services are delivered within hospitals. Depending on the organization and policies of the national health system, patients may be required to see a primary care provider for a referral before they can access secondary care. 
",Health
"In countries which operate under a mixed market health care system, some physicians limit their practice to secondary care by requiring patients to see a primary care provider first. This restriction may be imposed under the terms of the payment agreements in private or group health insurance plans. In other cases, medical specialists may see patients without a referral, and patients may decide whether self-referral is preferred.
",Health
"In other countries patient self-referral to a medical specialist for secondary care is rare as prior referral from another physician (either a primary care physician or another specialist) is considered necessary, regardless of whether the funding is from private insurance schemes or national health insurance.
",Health
"Allied health professionals, such as physical therapists, respiratory therapists, occupational therapists, speech therapists, and dietitians, also generally work in secondary care, accessed through either patient self-referral or through physician referral.
",Health
"Tertiary care is specialized consultative health care, usually for inpatients and on referral from a primary or secondary health professional, in a facility that has personnel and facilities for advanced medical investigation and treatment, such as a tertiary referral hospital.[13]
",Health
"Examples of tertiary care services are cancer management, neurosurgery, cardiac surgery, plastic surgery, treatment for severe burns, advanced neonatology services, palliative, and other complex medical and surgical interventions.[14]
",Health
"The term quaternary care is sometimes used as an extension of tertiary care in reference to advanced levels of medicine which are highly specialized and not widely accessed. Experimental medicine and some types of uncommon diagnostic or surgical procedures are considered quaternary care. These services are usually only offered in a limited number of regional or national health care centers.[14][15] Quaternary care is more prevalent in the United Kingdom.
",Health
"Many types of health care interventions are delivered outside of health facilities. They include many interventions of public health interest, such as food safety surveillance, distribution of condoms and needle-exchange programs for the prevention of transmissible diseases.
",Health
"They also include the services of professionals in residential and community settings in support of self care, home care, long-term care, assisted living, treatment for substance use disorders among other types of health and social care services.
",Health
"Community rehabilitation services can assist with mobility and independence after loss of limbs or loss of function. This can include prosthesis, orthotics or wheelchairs.
",Health
"Many countries, especially in the west are dealing with aging populations, so one of the priorities of the health care system is to help seniors live full, independent lives in the comfort of their own homes. There is an entire section of health care geared to providing seniors with help in day-to-day activities at home such as transportation to and from doctor's appointments along with many other activities that are essential for their health and well-being. Although they provide home care for older adults in cooperation, family members and care workers may harbor diverging attitudes and values towards their joint efforts. This state of affairs presents a challenge for the design of ICT (information and communication technology) for home care.[16]
",Health
"Because statistics show that over 80 million Americans have taken time off of their primary employment to care for a loved one,[17] many countries have begun offering programs such as Consumer Directed Personal Assistant Program to allow family members to take care of their loved ones without giving up their entire income.[citation needed]
",Health
"With obesity in children rapidly becoming a major concern, health services often set up programs in schools aimed at educating children about nutritional eating habits, making physical education a requirement and teaching young adolescents to have positive self-image.
",Health
"Health care ratings are ratings or evaluations of health care used to evaluate the process of care and health care structures and/or outcomes of health care services. This information is translated into report cards that are generated by quality organizations, nonprofit, consumer groups and media. This evaluation of quality is based on measures of:
",Health
"Health care extends beyond the delivery of services to patients, encompassing many related sectors, and is set within a bigger picture of financing and governance structures.
",Health
"A health system, also sometimes referred to as health care system or healthcare system is the organization of people, institutions, and resources that deliver health care services to populations in need.
",Health
"The health care industry incorporates several sectors that are dedicated to providing health care services and products. As a basic framework for defining the sector, the United Nations' International Standard Industrial Classification categorizes health care as generally consisting of hospital activities, medical and dental practice activities, and ""other human health activities."" The last class involves activities of, or under the supervision of, nurses, midwives, physiotherapists, scientific or diagnostic laboratories, pathology clinics, residential health facilities, patient advocates[18] or other allied health professions.
",Health
"In addition, according to industry and market classifications, such as the Global Industry Classification Standard and the Industry Classification Benchmark, health care includes many categories of medical equipment, instruments and services including biotechnology, diagnostic laboratories and substances, drug manufacturing and delivery.
",Health
"For example, pharmaceuticals and other medical devices are the leading high technology exports of Europe and the United States.[19][20] The United States dominates the biopharmaceutical field, accounting for three-quarters of the world's biotechnology revenues.[19][21]
",Health
"The quantity and quality of many health care interventions are improved through the results of science, such as advanced through the medical model of health which focuses on the eradication of illness through diagnosis and effective treatment. Many important advances have been made through health research, biomedical research and pharmaceutical research, which form the basis for evidence-based medicine and evidence-based practice in health care delivery.
",Health
"Health services research can lead to greater efficiency and equitable delivery of health care interventions, as advanced through the social model of health and disability, which emphasizes the societal changes that can be made to make populations healthier.[22] Results from health services research often form the basis of evidence-based policy in health care systems. Health services research is also aided by initiatives in the field of artificial intelligence for the development of systems of health assessment that are clinically useful, timely, sensitive to change, culturally sensitive, low burden, low cost, built into standard procedures, and involve the patient.[23]
",Health
"There are generally five primary methods of funding health care systems:[24]
",Health
"In most countries there is a mix of all five models, but this varies across countries and over time within countries.
",Health
"The management and administration of health care is vital to the delivery of health care services. In particular, the practice of health professionals and operation of health care institutions is typically regulated by national or state/provincial authorities through appropriate regulatory bodies for purposes of quality assurance.[25] Most countries have credentialing staff in regulatory boards or health departments who document the certification or licensing of health workers and their work history.[26]
",Health
"Health information technology (HIT) is ""the application of information processing involving both computer hardware and software that deals with the storage, retrieval, sharing, and use of health care information, data, and knowledge for communication and decision making.""[27]
",Health
"Health information technology components:
",Health
"Mental health is a level of psychological well-being or an absence of mental illness - the ""psychological state of someone who is functioning at a satisfactory level of emotional and behavioural adjustment"".[1] From the perspectives of positive psychology or of holism, mental health may include an individual's ability to enjoy  life, and to create a balance between life activities and efforts to achieve psychological resilience.[citation needed]
According to the World Health Organization (WHO), mental health includes ""subjective well-being, perceived self-efficacy, autonomy, competence, inter-generational dependence, and self-actualization of one's intellectual and emotional potential, among others.""[2] The WHO further states that the well-being of an individual is encompassed in the realization of their abilities, coping with normal stresses of life, productive work and contribution to their community.[3]
Cultural differences, subjective assessments, and competing professional theories all affect how one defines ""mental health"".[2]
",Health
"According to the U.K. surgeon general (1999), mental health is the successful performance of mental function, resulting in productive activities, fulfilling relationships with other people, and providing the ability to adapt to change and cope with adversity. The term mental illness refers collectively to all diagnosable mental disorders—health conditions characterized by alterations in thinking, mood, or behavior associated with distress or impaired functioning.[4]
",Health
"A person struggling with their mental health may experience this because of stress, loneliness, depression, anxiety, relationship problems, death of a loved one, suicidal thoughts, grief, addiction, ADHD,     Cutting, Self-harm, Self-Injury, burning, various mood disorders, or other mental illnesses of varying degrees, as well as learning disabilities.[5][6] Therapists, psychiatrists, psychologists, social workers, nurse practitioners or physicians can help manage mental illness with treatments such as therapy, counseling, or medication.
",Health
"In the mid-19th century, William Sweetser was the first to coin the term ""mental hygiene"", which can be seen as the precursor to contemporary approaches to work on promoting positive mental health.[7][8] Isaac Ray, one of the founders and the fourth president [9] of the American Psychiatric Association, further defined mental hygiene as ""the art of preserving the mind against all incidents and influences calculated to deteriorate its qualities, impair its energies, or derange its movements.""[8]
",Health
"Dorothea Dix (1802–1887) was an important figure in the development of the ""mental hygiene"" movement. Dix was a school teacher who endeavored throughout her life to help people with mental disorders, and to bring to light the deplorable conditions into which they were put.[10] This was known as the ""mental hygiene movement"".[10] Before this movement, it was not uncommon that people affected by mental illness in the 19th century would be considerably neglected, often left alone in deplorable conditions, barely even having sufficient clothing.[10] Dix's efforts were so great that there was a rise in the number of patients in mental health facilities, which sadly resulted in these patients receiving less attention and care, as these institutions were largely understaffed.[10]
",Health
"Emil Kraepelin in 1896 developed the taxonomy of mental disorders which has dominated the field for nearly 80 years. Later the proposed disease model of abnormality was subjected to analysis and considered normality to be relative to the physical, geographical and cultural aspects of the defining group.
",Health
"At the beginning of the 20th century, Clifford Beers founded ""Mental Health America – National Committee for Mental Hygiene"", after publication of his accounts from lived experience in lunatic asylums, A Mind That Found Itself, in 1908[11] and opened the first outpatient mental health clinic in the United States.[12]
",Health
"The mental hygiene movement, related to the social hygiene movement, had at times been associated with advocating eugenics and sterilisation of those considered too mentally deficient to be assisted into productive work and contented family life.[13][14] In the post-WWII years, references to mental hygiene were gradually replaced by the term 'mental health' due to its positive aspect that evolves from the treatment of illness to preventive and promotive areas of healthcare.[15]
",Health
"Marie Jahoda described six major, fundamental categories that can be used to categorize mentally healthy individuals: a positive attitude towards the self, personal growth, integration, autonomy, a true perception of reality, and environmental mastery, which include adaptability and healthy interpersonal relationships.[16]
",Health
"Mental illnesses are more common than cancer, diabetes, or heart disease. Over 26 percent of all Americans over the age of 18 meet the criteria for having a mental illness.[17] A WHO report estimates the global cost of mental illness at nearly $2.5 trillion (two-thirds in indirect costs) in 2010, with a projected increase to over $6 trillion by 2030.
",Health
"Evidence from the World Health Organization suggests that nearly half of the world's population are affected by mental illness with an impact on their self-esteem, relationships and ability to function in everyday life.[18]  An individual's emotional health can also impact physical health and poor mental health can lead to problems such as substance abuse.[19]
",Health
"Maintaining good mental health is crucial to living a long and healthy life. Good mental health can enhance one's life, while poor mental health can prevent someone from living an enriching life. According to Richards, Campania, & Muse-Burke, ""There is growing evidence that is showing emotional abilities are associated with prosocial behaviors such as stress management and physical health.""[19] Their research also concluded that people who lack emotional expression are inclined to anti-social behaviors (e.g., drug and alcohol abuse, physical fights, vandalism), which are a direct reflection of their mental health and suppress emotions.[19] Adults and children with mental illness may experience social stigma, which can exacerbate the issues.[20]
",Health
"Mental health can be seen as an unstable continuum, where an individual's mental health may have many different possible values.[21] Mental wellness is generally viewed as a positive attribute, even if the person does not have any diagnosed mental health condition. This definition of mental health highlights emotional well-being, the capacity to live a full and creative life, and the flexibility to deal with life's inevitable challenges. Some discussions are formulated in terms of contentment or happiness.[22] Many therapeutic systems and self-help books offer methods and philosophies espousing strategies and techniques vaunted as effective for further improving the mental wellness. Positive psychology is increasingly prominent in mental health.
",Health
"A holistic model of mental health generally includes concepts based upon anthropological, educational, psychological, religious and sociological perspectives, as well as theoretical perspectives from personality, social, clinical, health and developmental psychology.[23][24]
",Health
"The tripartite model of mental well-being[21][25] views mental well-being as encompassing three components of emotional well-being, social well-being, and psychological well-being. Emotional well-being is defined as having high levels of positive emotions, whereas social and psychological well-being are defined as the presence of psychological and social skills and abilities that contribute to optimal functioning in daily life. The model has received empirical support across cultures.[25][26][27] The Mental Health Continuum-Short Form (MHC-SF) is the most widely used scale to measure the tripartite model of mental well-being.[28][29][30]
",Health
"Mental health and stability is a very important factor in a person’s everyday life. Social skills, behavioral skills, and someone’s way of thinking are just some of the things that the human brain develops at an early age. Learning how to interact with others and how to focus on certain subjects are essential lessons to learn from the time we can talk all the way to when we are so old that we can barely walk. However, there are some people out there who have difficulty with these kind of skills and behaving like an average person. This is a most likely the cause of having a mental illness. A mental illness is a wide range of conditions that affect a person’s mood, thinking, and behavior. About 26% of people in the United States, ages 18 and older, have been diagnosed with some kind of mental disorder. However, not much is said about children with mental illnesses even though there are many that will develop one, even as early as age three.
",Health
"The most common mental illnesses in children include, but are not limited to, ADHD, autism and anxiety disorder, as well as depression in older children and teens. Having a mental illness at a younger age is much different from having one in your thirties. Children's brains are still developing and will continue to develop until around the age of twenty-five.[31]  When a mental illness is thrown into the mix, it becomes significantly harder for a child to acquire the necessary skills and habits that people use throughout the day. For example, behavioral skills don’t develop as fast as motor or sensory skills do.[31] So when a child has an anxiety disorder, they begin to lack proper social interaction and associate many ordinary things with intense fear.[32] This can be scary for the child because they don’t necessarily understand why they act and think the way that they do. Many researchers say that parents should keep an eye on their child if they have any reason to believe that something is slightly off.[31] If the children are evaluated earlier, they become more acquainted to their disorder and treating it becomes part of their daily routine.[31] This is opposed to adults who might not recover as quickly because it is more difficult for them to adapt.
",Health
"Mental illness affects not only the person themselves, but the people around them. Friends and family also play an important role in the child’s mental health stability and treatment. If the child is young, parents are the ones who evaluate their child and decide whether or not they need some form of help.[33] Friends are a support system for the child and family as a whole. Living with a mental disorder is never easy, so it’s always important to have people around to make the days a little easier. However, there are negative factors that come with the social aspect of mental illness as well. Parents are sometimes held responsible for their child’s own illness.[33] People also say that the parents raised their children in a certain way or they acquired their behavior from them. Family and friends are sometimes so ashamed of the idea of being close to someone with a disorder that the child feels isolated and thinks that they have to hide their illness from others.[33] When in reality, hiding it from people prevents the child from getting the right amount of social interaction and treatment in order to thrive in today’s society.
",Health
"Stigma is also a well-known factor in mental illness. Stigma is defined as “a mark of disgrace associated with a particular circumstance, quality, or person.” Stigma is used especially when it comes to the mentally disabled. People have this assumption that everyone with a mental problem, no matter how mild or severe, is automatically considered destructive or a criminal person. Thanks to the media, this idea has been planted in our brains from a young age.[34] Watching movies about teens with depression or children with Autism makes us think that all of the people that have a mental illness are like the ones on TV. In reality, the media displays an exaggerated version of most illnesses. Unfortunately, not many people know that, so they continue to belittle those with disorders. In a recent study, a majority of young people associate mental illness with extreme sadness or violence.[35] Now that children are becoming more and more open to technology and the media itself, future generations will then continue to pair mental illness with negative thoughts. The media should be explaining that many people with disorders like ADHD and anxiety, with the right treatment, can live ordinary lives and should not be punished for something they cannot help.
",Health
"Sueki, (2013) carried out a study titled “The effect of suicide–related internet use on users’ mental health: A longitudinal Study”. This study investigated the effects of suicide-related internet use on user’s suicidal thoughts, predisposition to depression and anxiety and loneliness. The study consisted of 850 internet users; the data was obtained by carrying out a questionnaire amongst the participants. This study found that browsing websites related to suicide, and methods used to commit suicide, had a negative effect on suicidal thoughts and increased depression and anxiety tendencies. The study concluded that as suicide-related internet use adversely affected the mental health of certain age groups it may be prudent to reduce or control their exposure to these websites. These findings certainly suggest that the internet can indeed have a profoundly negative impact on our mental health.[36]
",Health
"Psychiatrist Thomas Szasz compared that 50 years ago children were either categorized as good or bad, and today ""all children are good, but some are mentally healthy and others are mentally ill"". The social control and forced identity creation is the cause of many mental health problems among today's children.[37] A behaviour or misbehaviour might not be an illness but exercise of their free will and today's immediacy in drug administration for every problem along with the legal over-guarding and regard of a child's status as a dependent shakes their personal self and invades their internal growth.
",Health
"Mental health is conventionally defined as a hybrid of absence of a mental disorder and presence of well-being. Focus is increasing on preventing mental disorders.
Prevention is beginning to appear in mental health strategies, including the 2004 WHO report ""Prevention of Mental Disorders"", the 2008 EU ""Pact for Mental Health"" and the 2011 US National Prevention Strategy.[38][page needed] Some commentators have argued that a pragmatic and practical approach to mental disorder prevention at work would be to treat it the same way as physical injury prevention.[39]
",Health
"Prevention of a disorder at a young age may significantly decrease the chances that a child will suffer from a disorder later in life, and shall be the most efficient and effective measure from a public health perspective.[40] Prevention may require the regular consultation of a physician for at least twice a year to detect any signs that reveal any mental health concerns.[citation needed]
",Health
"Mental health is a socially constructed and socially defined concept; that is, different societies, groups, cultures, institutions and professions have very different ways of conceptualizing its nature and causes, determining what is mentally healthy, and deciding what interventions, if any, are appropriate.[41] Thus, different professionals will have different cultural, class, political and religious backgrounds, which will impact the methodology applied during treatment.
",Health
"Research has shown that there is stigma attached to mental illness.[42] In the United Kingdom, the Royal College of Psychiatrists organized the campaign Changing Minds (1998–2003) to help reduce stigma.[43] Due to this stigma, responses to a positive diagnosis may be a display of denialism.[44]
",Health
"Family caregivers of individuals with mental disorders may also suffer discrimination or stigma[45].
",Health
"Addressing and eliminating the social stigma and perceived stigma attached to mental illness has been recognized as a crucial part to addressing the education of mental health issues. In the United States, the National Alliance of Mental Illness is an institution that was founded in 1979 to represent and advocate for victims struggling with mental health issues. NAMI also helps to educate about mental illnesses and health issues, while also working to eliminate the stigma[46] attached to these disorders such as anxiety and depression.
",Health
"Many mental health professionals are beginning to, or already understand, the importance of competency in religious diversity and spirituality. The American Psychological Association explicitly states that religion must be respected. Education in spiritual and religious matters is also required by the American Psychiatric Association,[47] however, far less attention is paid to the damage that more rigid, fundamentalist faiths commonly practiced in the United States can cause[48]. This theme has been widely politicized in 2018 such as with the creation of the Religious Liberty Task Force in July of that year[49]. In addition, many providers and practioners in the United States are only beginning to realize that the institution of mental healthcare lacks knowledge and competence of many non-Western cultures, leaving providers in the United States ill-equipped to treat patients from different cultures.[50]
",Health
"Unemployment has been shown to have a negative impact on an individual's emotional well-being, self-esteem and more broadly their mental health. Increasing unemployment has been show to have a significant impact on mental health, predominantly depressive disorders.[51] This is an important consideration when reviewing the triggers for mental health disorders in any population survey.[52] In order to improve your emotional mental health, the root of the issue has to be resolved. ""Prevention emphasizes the avoidance of risk factors; promotion aims to enhance an individual's ability to achieve a positive sense of self-esteem, mastery, well-being, and social inclusion.""[53] It is very important to improve your emotional mental health by surrounding yourself with positive relationships. We as humans, feed off companionships and interaction with other people. Another way to improve your emotional mental health is participating in activities that can allow you to relax and take time for yourself. Yoga is a great example of an activity that calms your entire body and nerves. According to a study on well-being by Richards, Campania and Muse-Burke, ""mindfulness is considered to be a purposeful state, it may be that those who practice it believe in its importance and value being mindful, so that valuing of self-care activities may influence the intentional component of mindfulness.""[19]
",Health
"Mental health care navigation helps to guide patients and families through the fragmented, often confusing mental health industries. Care navigators work closely with patients and families through discussion and collaboration to provide information on best therapies as well as referrals to practitioners and facilities specializing in particular forms of emotional improvement. The difference between therapy and care navigation is that the care navigation process provides information and directs patients to therapy rather than providing therapy. Still, care navigators may offer diagnosis and treatment planning. Though many care navigators are also trained therapists and doctors. Care navigation is the link between the patient and the below therapies. A clear recognition that mental health requires medical intervention was demonstrated in a study by Kessler et al. of the prevalence and treatment of mental disorders from 1990 to 2003 in the United States. Despite the prevalence of mental health disorders remaining unchanged during this period, the number of patients seeking treatment for mental disorders increased threefold.[54]
",Health
"Emotional mental disorders are a leading cause of disabilities worldwide. Investigating the degree and severity of untreated emotional mental disorders throughout the world is a top priority of the World Mental Health (WMH) survey initiative,[55] which was created in 1998 by the World Health Organization (WHO).[56] ""Neuropsychiatric disorders are the leading causes of disability worldwide, accounting for 37% of all healthy life years lost through disease.These disorders are most destructive to low and middle-income countries due to their inability to provide their citizens with proper aid. Despite modern treatment and rehabilitation for emotional mental health disorders, ""even economically advantaged societies have competing priorities and budgetary constraints"".
",Health
"The World Mental Health survey initiative has suggested a plan for countries to redesign their mental health care systems to best allocate resources. 
""A first step is documentation of services being used and the extent and nature of unmet needs for treatment. A second step could be to do a cross-national comparison of service use and unmet needs in countries with different mental health care systems. Such comparisons can help to uncover optimum financing, national policies, and delivery systems for mental health care.""
",Health
"Knowledge of how to provide effective emotional mental health care has become imperative worldwide. Unfortunately, most countries have insufficient data to guide decisions, absent or competing visions for resources, and near constant pressures to cut insurance and entitlements. WMH surveys were done in Africa (Nigeria, South Africa), the Americas (Colombia, Mexico, United States), Asia and the Pacific (Japan, New Zealand, Beijing and Shanghai in the People's Republic of China), Europe (Belgium, France, Germany, Italy, Netherlands, Spain, Ukraine), and the middle east (Israel, Lebanon). Countries were classified with World Bank criteria as low-income (Nigeria), lower middle-income (China, Colombia, South Africa, Ukraine), higher middle-income (Lebanon, Mexico), and high-income.
",Health
"The coordinated surveys on emotional mental health disorders, their severity, and treatments were implemented in the aforementioned countries. These surveys assessed the frequency, types, and adequacy of mental health service use in 17 countries in which WMH surveys are complete. The WMH also examined unmet needs for treatment in strata defined by the seriousness of mental disorders. Their research showed that ""the number of respondents using any 12-month mental health service was generally lower in developing than in developed countries, and the proportion receiving services tended to correspond to countries' percentages of gross domestic product spent on health care"".
""High levels of unmet need worldwide are not surprising, since WHO Project ATLAS' findings of much lower mental health expenditures than was suggested by the magnitude of burdens from mental illnesses. Generally, unmet needs in low-income and middle-income countries might be attributable to these nations spending reduced amounts (usually <1%) of already diminished health budgets on mental health care, and they rely heavily on out-of-pocket spending by citizens who are ill equipped for it"".
",Health
"Archaeological records have shown that trepanation was a procedure used to treat ""headaches, insanities or epilepsy"" in several parts of the world in the Stone age. It was a surgical process used in the Stone Age. Paul Broca studied trepanation and came up with his own theory on it. He noticed that the fractures on the skulls dug up weren't caused by wounds inflicted due to violence, but because of careful surgical procedures. ""Doctors used sharpened stones to scrape the skull and drill holes into the head of the patient"" to allow evil spirits which plagued the patient to escape. There were several patients that died in these procedures, but those that survived were revered and believed to possess ""properties of a mystical order"".[1] [2]
",Health
"Lobotomy was used in the 20th century as a common practice of alternative treatment for mental illnesses such as schizophrenia and depression. The first ever modern leucotomy meant for the purpose of treating a mental illness occurred in 1935 by a Portuguese neurologist, Antonio Egas Moniz. He received the Nobel Prize in medicine in 1949. [3]. This belief that mental health illnesses could be treated by surgery came from Swiss neurologist, Gottlieb Burckhardt. After conducting experiments on six patients with schizophrenia, he claimed that half of his patients recovered or calmed down.
Psychiatrist Walter Freeman believed that ""an overload of emotions led to mental illness and “that cutting certain nerves in the brain could eliminate excess emotion and stabilize a personality,” according to a National Public Radio article [4].""
",Health
"""Exorcism is the religious or spiritual practice of evicting demons or other spiritual entities from a person, or an area, they are believed to have possessed.""
",Health
"Mental health illnesses such as Huntington’s Disease (HD), Tourette syndrome and schizophrenia were believed to be signs of possession by the Devil. This led to several mentally ill patients being subjected to exorcisms. This practice has been around for a long time, though decreasing steadily until it reached a low in the 18th century. It seldom occurred until the 20th century when the numbers rose due to the attention the media was giving to exorcisms. Different belief systems practice exorcisms in different ways.[57]
",Health
"Pharmacotherapy is therapy that uses pharmaceutical drugs. Pharmacotherapy is used in the treatment of mental illness through the use of antidepressants, benzodiazepines, and the use of elements such as lithium.
",Health
"Physical activity is a very good way to help improve your mental health as well as your physical health. Playing sports, walking, cycling or doing any form of physical activity can trigger the production of endorphins. Endorphins are natural mood enhancers.[58][59]
",Health
"Activity therapies, also called recreation therapy and occupational therapy, promote healing through active engagement.  Making crafts can be a part of occupational therapy.  Walks can be a part of recreation therapy.
In recent years colouring has been recognised as an activity which has been proven to significantly lower the levels of depressive symptoms and anxiety in many studies.[60]
",Health
"Expressive therapies are a form of psychotherapy that involves the arts or art-making. These therapies include music therapy, art therapy, dance therapy, drama therapy, and poetry therapy. It has been proven that Music therapy is an effective way of helping people who suffer from a mental health disorder.[61]
",Health
"Psychotherapy is the general term for scientific based treatment of mental health issues based on modern medicine.  It includes a number of schools, such as gestalt therapy, psychoanalysis, cognitive behavioral therapy and dialectical behavioral therapy. 
Group therapy involves any type of therapy that takes place in a setting involving multiple people.  It can include psychodynamic groups, activity groups for expressive therapy, support groups (including the Twelve-step program), problem-solving and psychoeducation groups.
",Health
"The practice of mindfulness meditation has several mental health benefits, such as bringing about reductions in depression, anxiety and stress.[62][63][64][65] Mindfulness meditation may also be effective in treating substance use disorders.[66][67] Further, mindfulness meditation appears to bring about favorable structural changes in the brain.[68][69][70]
",Health
"The Heartfulness meditation program has proven to show significant improvements in the state of mind of health-care professionals.[71] A study posted on the US National Library of Medicine showed that these professionals of varied stress levels were able to improve their conditions after this meditation program was conducted. They benefited in aspects of burnouts and emotional wellness.
",Health
"People with anxiety disorders participated in a stress-reduction program conducted by researchers from the Mental Health Service Line at the W.G. Hefner Veterans Affairs Medical Center in Salisbury, North Carolina. The participants practiced mindfulness meditation. After the study was over, it was concluded that the ""mindfulness meditation training program can effectively reduce symptoms of anxiety and panic and can help maintain these reductions in patients with generalized anxiety disorder, panic disorder, or panic disorder with agoraphobia.""[72]
",Health
"Spiritual counselors meet with people in need to offer comfort and support and to help them gain a better understanding of their issues and develop a problem-solving relation with spirituality. These types of counselors deliver care based on spiritual, psychological and theological principles.[73][unreliable source?]
",Health
"Social work in mental health, also called psychiatric social work, is a process where an individual in a setting is helped to attain freedom from overlapping internal and external problems (social and economic situations, family and other relationships, the physical and organizational environment, psychiatric symptoms, etc.). It aims for harmony, quality of life, self-actualization and personal adaptation across all systems. Psychiatric social workers are mental health professionals that can assist patients and their family members in coping with both mental health issues and various economic or social problems caused by mental illness or psychiatric dysfunctions and to attain improved mental health and well-being. They are vital members of the treatment teams in Departments of Psychiatry and Behavioral Sciences in hospitals. They are employed in both outpatient and inpatient settings of a hospital, nursing homes, state and local governments, substance abuse clinics, correctional facilities, health care services...etc.[74]
",Health
"In psychiatric social work there are three distinct groups. One made up of the social workers in psychiatric organizations and hospitals. The second group consists members interested with mental hygiene education and holding designations that involve functioning in various mental health services and the third group consist of individuals involved directly with treatment and recovery process.[75]
",Health
"In the United States, social workers provide most of the mental health services. According to government sources, 60 percent of mental health professionals are clinically trained social workers, 10 percent are psychiatrists, 23 percent are psychologists, and 5 percent are psychiatric nurses.[76]
",Health
"Mental health social workers in Japan have professional knowledge of health and welfare and skills essential for person's well-being. Their social work training enables them as a professional to carry out Consultation assistance for mental disabilities and their social reintegration; Consultation regarding the rehabilitation of the victims; Advice and guidance for post-discharge residence and re-employment after hospitalized care, for major life events in regular life, money and self-management and in other relevant matters in order to equip them to adapt in daily life. Social workers provide individual home visits for mentally ill and do welfare services available, with specialized training a range of procedural services are coordinated for home, workplace and school. In an administrative relationship, Psychiatric social workers provides consultation, leadership, conflict management and work direction. Psychiatric social workers who provides assessment and psychosocial interventions function as a clinician, counselor and municipal staff of the health centers.[77]
",Health
"Social workers play many roles in mental health settings, including those of case manager, advocate, administrator, and therapist. The major functions of a psychiatric social worker are promotion and prevention, treatment, and rehabilitation. Social workers may also practice:
",Health
"Psychiatric social workers conduct psychosocial assessments of the patients and work to enhance patient and family communications with the medical team members and ensure the inter-professional cordiality in the team to secure patients with the best possible care and to be active partners in their care planning. Depending upon the requirement, social workers are often involved in illness education, counseling and psychotherapy. In all areas, they are pivotal to the aftercare process to facilitate a careful transition back to family and community.
[78]
",Health
"During the 1840s, Dorothea Lynde Dix, a retired Boston teacher who is considered the founder of the Mental Health Movement, began a crusade that would change the way people with mental disorders were viewed and treated. Dix was not a social worker; the profession was not established until after her death in 1887. However, her life and work were embraced by early psychiatric social workers, and she is considered one of the pioneers of psychiatric social work along with Elizabeth Horton, who in 1907 was the first psychiatric social worker in the New York hospital system, and others.[79] The early twentieth century was a time of progressive change in attitudes towards mental illness. Community Mental Health Centers Act was passed in 1963. This policy encouraged the deinstitutionalisation of people with mental illness. Later, mental health consumer movement came by 1980s. A consumer was defined as a person who has received or is currently receiving services for a psychiatric condition. People with mental disorders and their families became advocates for better care. Building public understanding and awareness through consumer advocacy helped bring mental illness and its treatment into mainstream medicine and social services.[80] In the 2000s focus was on Managed care movement which aimed at a health care delivery system to eliminate unnecessary and inappropriate care in order to reduce costs & Recovery movement in which by principle acknowledges that many people with serious mental illness spontaneously recover and others recover and improve with proper treatment.[81]
",Health
"Role of social workers made an impact with 2003 invasion of Iraq and War in Afghanistan (2001–14) social workers worked out of the NATO hospital in Afghanistan and Iraq bases. They made visits to provide counseling services at forward operating bases. Twenty-two percent of the clients were diagnosed with post-traumatic stress disorder, 17 percent with depression, and 7 percent with alcohol abuse.[82] In 2009, a high level of suicides was reached among active-duty soldiers: 160 confirmed or suspected Army suicides. In 2008, the Marine Corps had a record 52 suicides.[83] The stress of long and repeated deployments to war zones, the dangerous and confusing nature of both wars, wavering public support for the wars, and reduced troop morale have all contributed to the escalating mental health issues.[84] Military and civilian social workers are primary service providers in the veterans’ health care system.
",Health
"Mental health services, is a loose network of services ranging from highly structured inpatient psychiatric units to informal support groups, where psychiatric social workers indulges in the diverse approaches in multiple settings along with other paraprofessional workers.
",Health
"A role for psychiatric social workers was established early in Canada’s history of service delivery in the field of population health. Native North Americans understood mental trouble as an indication of an individual who had lost their equilibrium with the sense of place and belonging in general, and with the rest of the group in particular. In native healing beliefs, health and mental health were inseparable, so similar combinations of natural and spiritual remedies were often employed to relieve both mental and physical illness. These communities and families greatly valued holistic approaches for preventative health care. Indigenous peoples in Canada have faced cultural oppression and social marginalization through the actions of European colonizers and their institutions since the earliest periods of contact. Culture contact brought with it many forms of depredation. Economic, political, and religious institutions of the European settlers all contributed to the displacement and oppression of indigenous people.[85][page needed]
",Health
"The first officially recorded treatment practices were in 1714, when Quebec opened wards for the mentally ill. In the 1830s social services were active through charity organizations and church parishes (Social Gospel Movement). Asylums for the insane were opened in 1835 in Saint John and New Brunswick. In 1841 in Toronto, when care for the mentally ill became institutionally based. Canada became a self-governing dominion in 1867, retaining its ties to the British crown. During this period age of industrial capitalism began, which lead to a social and economic dislocation in many forms. By 1887 asylums were converted to hospitals and nurses and attendants were employed for the care of the mentally ill. The first social work training began at the University of Toronto in 1914. In 1918 Clarence Hincks & Clifford Beers founded the Canadian National Committee for Mental Hygiene, which later became the Canadian Mental Health Association. In the 1930s Dr. Clarence Hincks promoted prevention and of treating sufferers of mental illness before they were incapacitated/early detection.
",Health
"World War II profoundly affected attitudes towards mental health. The medical examinations of recruits revealed that thousands of apparently healthy adults suffered mental difficulties. This knowledge changed public attitudes towards mental health, and stimulated research into preventive measures and methods of treatment.[86] In 1951 Mental Health Week was introduced across Canada. For the first half of the twentieth century, with a period of deinstitutionalisation beginning in the late 1960s psychiatric social work succeeded to the current emphasis on community-based care, psychiatric social work focused beyond the medical model’s aspects on individual diagnosis to identify and address social inequities and structural issues. In the 1980s Mental Health Act was amended to give consumers the right to choose treatment alternatives. Later the focus shifted to workforce mental health issues and environment.[87]
",Health
"The earliest citing of mental disorders in India are from Vedic Era (2000 BC – AD 600).[88] Charaka Samhita, an ayurvedic textbook believed to be from 400–200 BC describes various factors of mental stability. It also has instructions regarding how to set up a care delivery system.[89] In the same era In south India Siddha was a medical system, the great sage Agastya, one of the 18 siddhas contributing to a system of medicine has included the Agastiyar Kirigai Nool, a compendium of psychiatric disorders and their recommended treatments.[90] In Atharva Veda too there are descriptions and resolutions about mental health afflictions. In the Mughal period Unani system of medicine was introduced by an Indian physician Unhammad in 1222.[91] Then existed form of psychotherapy was known then as ilaj-i-nafsani in Unani medicine.
",Health
"The 18th century was a very unstable period in Indian history, which contributed to psychological and social chaos in the Indian subcontinent. In 1745 of lunatic asylums were developed in Bombay (Mumbai) followed by Calcutta (Kolkata) in 1784, and Madras (Chennai) in 1794. The need to establish hospitals became more acute, first to treat and manage Englishmen and Indian ‘sepoys’ (military men) employed by the British East India Company.[92] The First Lunacy Act (also called Act No. 36) that came into effect in 1858 was later modified by a committee appointed in Bengal in 1888. Later, the Indian Lunacy Act, 1912 was brought under this legislation. A rehabilitation programme was initiated between 1870s and 1890s for persons with mental illness at the Mysore Lunatic Asylum, and then an occupational therapy department was established during this period in almost each of the lunatic asylums. The programme in the asylum was called ‘work therapy’. In this programme, persons with mental illness were involved in the field of agriculture for all activities. This programme is considered as the seed of origin of psychosocial rehabilitation in India.
",Health
"Berkeley-Hill, superintendent of the European Hospital (now known as the Central Institute of Psychiatry (CIP), established in 1918), was deeply concerned about the improvement of mental hospitals in those days. The sustained efforts of Berkeley-Hill helped to raise the standard of treatment and care and he also persuaded the government to change the term ‘asylum’ to ‘hospital’ in 1920.[93] Techniques similar to the current token-economy were first started in 1920 and called by the name ‘habit formation chart’ at the CIP, Ranchi. In 1937, the first post of psychiatric social worker was created in the child guidance clinic run by the Dhorabji Tata School of Social Work (established in 1936), It is considered as the first documented evidence of social work practice in Indian mental health field.
",Health
"After Independence in 1947, general hospital psychiatry units (GHPUs) where established to improve conditions in existing hospitals, while at the same time encouraging outpatient care through these units. In Amritsar a Dr. Vidyasagar, instituted active involvement of families in the care of persons with mental illness. This was advanced practice ahead of its times regarding treatment and care. This methodology had a greater impact on social work practice in the mental health field especially in reducing the stigmatisation. In 1948 Gauri Rani Banerjee, trained in the United States, started a master’s course in medical and psychiatric social work at the Dhorabji Tata School of Social Work (Now TISS). Later the first trained psychiatric social worker was appointed in 1949 at the adult psychiatry unit of Yervada mental hospital, Pune.
",Health
"In various parts of the country, in mental health service settings, social workers were employed—in 1956 at a mental hospital in Amritsar, in 1958 at a child guidance clinic of the college of nursing, and in Delhi in 1960 at the All India Institute of Medical Sciences and in 1962 at the Ram Manohar Lohia Hospital. In 1960, the Madras Mental Hospital (Now Institute of Mental Health), employed social workers to bridge the gap between doctors and patients. In 1961 the social work post was created at the NIMHANS. In these settings they took care of the psychosocial aspect of treatment. This had long-term greater impact of social work practice in mental health.[94]
",Health
"In 1966 by the recommendation Mental Health Advisory Committee, Ministry of Health, Government of India, NIMHANS commenced Department of Psychiatric Social Work started and a two-year Postgraduate Diploma in Psychiatric Social Work was introduced in 1968. In 1978, the nomenclature of the course was changed to MPhil in Psychiatric Social Work. Subsequently, a PhD Programme was introduced. By the recommendations Mudaliar committee in 1962, Diploma in Psychiatric Social Work was started in 1970 at the European Mental Hospital at Ranchi (now CIP), upgraded the program and added other higher training courses subsequently.
",Health
"A new initiative to integrate mental health with general health services started in 1975 in India. The Ministry of Health, Government of India formulated the National Mental Health Programme (NMHP) and launched it in 1982. The same was reviewed in 1995 and based on that, the District Mental Health Program (DMHP) launched in 1996 and sought to integrate mental health care with public health care.[95] This model has been implemented in all the states and currently there are 125 DMHP sites in India.
",Health
"National Human Rights Commission (NHRC) in 1998 and 2008 carried out systematic, intensive and critical examinations of mental hospitals in India. This resulted in recognition of the human rights of the persons with mental illness by the NHRC. From the NHRC's report as part of the NMHP, funds were provided for upgrading the facilities of mental hospitals. This is studied to result in positive changes over the past 10 years than in the preceding five decades by the 2008 report of the NHRC and NIMHANS.[96] In 2016 Mental Health Care Bill was passed which ensures and legally entitles access to treatments with coverage from insurance, safeguarding dignity of the afflicted person, improving legal and healthcare access and allows for free medications.[97][98][99] In December 2016, Disabilities Act 1995 was repealed with Rights of Persons with Disabilities Act (RPWD), 2016 from the 2014 Bill which ensures benefits for a wider population with disabilities. The Bill before becoming an Act was pushed for amendments by stakeholders mainly against alarming clauses in the ""Equality and Non discrimination"" section that diminishes the power of the act and allows establishments to overlook or discriminate against persons with disabilities and against the general lack of directives that requires to ensure the proper implementation of the Act.[100][101]
",Health
"Lack of any universally accepted single licensing authority compared to foreign countries puts social workers at general in risk. But general bodies/councils accepts automatically a university-qualified social worker as a professional licensed to practice or as a qualified clinician. Lack of a centralized council in tie-up with Schools of Social Work also makes a decline in promotion for the scope of social workers as mental health professionals. Though in this midst the service of social workers has given a facelift of the mental health sector in the country with other allied professionals.[102]
",Health
"Evidence suggests that 450 million people worldwide are impacted by mental health, major depression ranks fourth among the top 10 leading causes of disease worldwide. Within 20 years, mental illness is predicted to become the leading cause of disease worldwide. Women are more likely to have a mental illness than men. One million people commit suicide every year and 10 to 20 million attempt it.[103]
",Health
"A survey conducted by Australian Bureau of Statistics in 2008 regarding adults with manageable to severe neurosis reveals almost half of the population had a mental disorder at some point of their life and one in five people had a sustained disorder in the preceding 12 months. In neurotic disorders, 14% of the population experienced anxiety disorders, comorbidity disorders were the next common mental disorder with vulnerability to substance abuse and relapses. There were distinct gender differences in disposition to mental health illness. Women were found to have high rate of mental health disorders and Men had higher propensity of risk for substance abuse. The SMHWB survey showed low socioeconomic status and high dysfunctional pattern in the family was proportional to greater risk for mental health disorders. A 2010 survey regarding adults with psychosis revealed 5 persons per 1000 in the population seeks professional mental health services for psychotic disorders and the most common psychotic disorder was schizophrenia.[104][105]
",Health
"According to statistics released by the Centre of Addiction and Mental Health one in five people in Ontario experience a mental health or addiction problem. Young people ages 15 to 25 are particularly vulnerable. Major depression is found to affect 8% and anxiety disorder 12% of the population. Women are 1.5 times more likely to suffer from mood and anxiety disorders. WHO points out that there are distinct gender differences in patterns of mental health and illness. The lack of power and control over their socioeconomic status, gender based violence; low social position and responsibility for the care of others render women vulnerable to mental health risks. Since more women than men seek help regarding a mental health problem, this has led to not only gender stereotyping but also reinforcing social stigma. WHO has found that this stereotyping has led doctors to diagnose depression more often in women than in men even when they display identical symptoms. Often communication between health care providers and women is authoritarian leading to either the under-treatment or over-treatment of these women.[3]
",Health
"Women's College Hospital is specifically dedicated to women's health in Canada. This hospital is located in downtown Toronto where there are several locations available for specific medical conditions. WCH is an organization that helps educate women on mental illness due to its specialization with women and mental health. The organization helps women who have symptoms of mental illnesses such as depression, anxiety, menstruation, pregnancy, childbirth, and menopause. They also focus on psychological issues, abuse, neglect and mental health issues from various medications.[106]
",Health
"The countless aspect about this organization is that WCH is open to women of all ages, including pregnant women that experience poor mental health. WCH not only provides care for good mental health, but they also have a program called the ""Women's Mental Health Program"" where doctors and nurses help treat and educate women regarding mental health collaboratively, individually, and online by answering questions from the public.[106]
",Health
"The second organization is the Centre for Addiction and Mental Health (CAMH). CAMH is one of Canada's largest and most well-known health and addiction facilities, and it has received international recognitions from the Pan American Health Organization and World Health Organization Collaborating Centre. They practice in doing research in areas of addiction and mental health in both men and women. In order to help both men and women, CAMH provides ""clinical care, research, education, policy development and health promotion to help transform the lives of people affected by mental health and addiction issues.""[107] CAMH is different from Women's College Hospital due to its widely known rehab centre for women who have minor addiction issues, to severe ones. This organization provides care for mental health issues by assessments, interventions, residential programs, treatments, and doctor and family support.[107]
",Health
"According to the World Health Organization in 2004, depression is the leading cause of disability in the United States for individuals ages 15 to 44.[108] Absence from work in the U.S. due to depression is estimated to be in excess of $31 billion per year. Depression frequently co-occurs with a variety of medical illnesses such as heart disease, cancer, and chronic pain and is associated with poorer health status and prognosis.[109] Each year, roughly 30,000 Americans take their lives, while hundreds of thousands make suicide attempts (Centers for Disease Control and Prevention).[110] In 2004, suicide was the 11th leading cause of death in the United States (Centers for Disease Control and Prevention), third among individuals ages 15–24.  Despite the increasingly availability of effectual depression treatment, the level of unmet need for treatment remains high.[citation needed] By way of comparison, a study conducted in Australia during 2006 to 2007 reported that one-third (34.9%) of patients diagnosed with a mental health disorder had presented to medical health services for treatment.[111]
",Health
"There are many factors that influence mental health including:
",Health
"Emotional mental illnesses should be a particular concern in the United States since the U.S. has the highest annual prevalence rates (26 percent) for mental illnesses among a comparison of 14 developing and developed countries.[112] While approximately 80 percent of all people in the United States with a mental disorder eventually receive some form of treatment, on the average persons do not access care until nearly a decade following the development of their illness, and less than one-third of people who seek help receive minimally adequate care.[113] The government offers everyone programs and services, but veterans receive the most help, there is certain eligibility criteria that has to be met.[114]
",Health
"The mental health policies in the United States have experienced four major reforms: the American asylum movement led by Dorothea Dix in 1843; the ""mental hygiene"" movement inspired by Clifford Beers in 1908; the deinstitutionalization started by Action for Mental Health in 1961; and the community support movement called for by The CMCH Act Amendments of 1975.[115]
",Health
"In 1843, Dorothea Dix submitted a Memorial to the Legislature of Massachusetts, describing the abusive treatment and horrible conditions received by the mentally ill patients in jails, cages, and almshouses. She revealed in her Memorial: ""I proceed, gentlemen, briefly to call your attention to the present state of insane persons confined within this Commonwealth, in cages, closets, cellars, stalls, pens! Chained, naked, beaten with rods, and lashed into obedience….""[116] Many asylums were built in that period, with high fences or walls separating the patients from other community members and strict rules regarding the entrance and exit. In those asylums, traditional treatments were well implemented: drugs were not used as a cure for a disease, but a way to reset equilibrium in a person's body, along with other essential elements such as healthy diets, fresh air, middle class culture, and the visits by their neighboring residents.[citation needed] In 1866, a recommendation came to the New York State Legislature to establish a separate asylum for chronic mentally ill patients. Some hospitals placed the chronic patients into separate wings or wards, or different buildings.[117]
",Health
"In A Mind That Found Itself (1908) Clifford Whittingham Beers described the humiliating treatment he received and the deplorable conditions in the mental hospital.[118] One year later, the National Committee for Mental Hygiene (NCMH) was founded by a small group of reform-minded scholars and scientists – including Beer himself – which marked the beginning of the ""mental hygiene"" movement. The movement emphasized the importance of childhood prevention. World War I catalyzed this idea with an additional emphasis on the impact of maladjustment, which convinced the hygienists that prevention was the only practical approach to handle mental health issues.[119] However, prevention was not successful, especially for chronic illness; the condemnable conditions in the hospitals were even more prevalent, especially under the pressure of the increasing number of chronically ill and the influence of the depression.[115]
",Health
"In 1961, the Joint Commission on Mental Health published a report called Action for Mental Health, whose goal was for community clinic care to take on the burden of prevention and early intervention of the mental illness, therefore to leave space in the hospitals for severe and chronic patients. The court started to rule in favor of the patients' will on whether they should be forced to treatment. By 1977, 650 community mental health centers were built to cover 43 percent of the population and serve 1.9 million individuals a year, and the lengths of treatment decreased from 6 months to only 23 days.[120] However, issues still existed. Due to inflation, especially in the 1970s, the community nursing homes received less money to support the care and treatment provided. Fewer than half of the planned centers were created, and new methods did not fully replace the old approaches to carry out its full capacity of treating power.[120] Besides, the community helping system was not fully established to support the patients' housing, vocational opportunities, income supports, and other benefits.[115] Many patients returned to welfare and criminal justice institutions, and more became homeless. The movement of deinstitutionalization was facing great challenges.[121]
",Health
"After realizing that simply changing the location of mental health care from the state hospitals to nursing houses was insufficient to implement the idea of deinstitutionalization, the National Institute of Mental Health in 1975 created the Community Support Program (CSP) to provide funds for communities to set up a comprehensive mental health service and supports to help the mentally ill patients integrate successfully in the society. The program stressed the importance of other supports in addition to medical care, including housing, living expenses, employment, transportation, and education; and set up new national priority for people with serious mental disorders. In addition, the Congress enacted the Mental Health Systems Act of 1980 to prioritize the service to the mentally ill and emphasize the expansion of services beyond just clinical care alone.[122] Later in the 1980s, under the influence from the Congress and the Supreme Court, many programs started to help the patients regain their benefits. A new Medicaid service was also established to serve people who were diagnosed with a ""chronic mental illness."" People who were temporally hospitalized were also provided aid and care and a pre-release program was created to enable people to apply for reinstatement prior to discharge.[120] Not until 1990, around 35 years after the start of the deinstitutionalization, did the first state hospital begin to close. The number of hospitals dropped from around 300 by over 40 in the 1990s, and finally a Report on Mental Health showed the efficacy of mental health treatment, giving a range of treatments available for patients to choose.[122]
",Health
"However, several critics maintain that deinstitutionalization has, from a mental health point of view, been a thoroughgoing failure. The seriously mentally ill are either homeless, or in prison; in either case (especially the latter), they are getting little or no mental health care. This failure is attributed to a number of reasons over which there is some degree of contention, although there is general agreement that community support programs have been ineffective at best, due to a lack of funding.[121]
",Health
"The 2011 National Prevention Strategy included mental and emotional well-being, with recommendations including better parenting and early intervention programs, which increase the likelihood of prevention programs being included in future US mental health policies.[123][page needed] The NIMH is researching only suicide and HIV/AIDS prevention, but the National Prevention Strategy could lead to it focusing more broadly on longitudinal prevention studies.[124][not in citation given]
",Health
"In 2013, United States Representative Tim Murphy introduced the Helping Families in Mental Health Crisis Act, HR2646. The bipartisan bill went through substantial revision and was reintroduced in 2015 by Murphy and Congresswoman Eddie Bernice Johnson. In November 2015, it passed the Health Subcommittee by an 18–12 vote.[citation needed]
",Health
"Community health is a major field of study within the medical and clinical sciences which focuses on the maintenance, protection, and improvement of the health status of population groups and communities. It is a distinct field of study that may be taught within a separate school of public health or environmental health. The WHO defines community health as:",Health
"environmental, social, and economic resources to sustain emotional and physical well being among people in ways that advance their aspirations and satisfy their needs in their unique environment.[1]",Health
"Community health tends to focus on a defined geographical community. The health characteristics of a community are often examined using geographic information system (GIS) software and public health datasets. Some projects, such as InfoShare or GEOPROJ combine GIS with existing datasets, allowing the general public to examine the characteristics of any given community in participating countries.
",Health
"Medical interventions that occur in communities can be classified as three categories: primary healthcare, secondary healthcare, and tertiary healthcare. Each category focuses on a different level and approach towards the community or population group. In the United States, community health is rooted within primary healthcare achievements.[2] Primary healthcare programs aim to reduce risk factors and increase health promotion and prevention. Secondary healthcare is related to ""hospital care"" where acute care is administered in a hospital department setting. Tertiary healthcare refers to highly specialized care usually involving disease or disability management.
",Health
"The success of community health programmes relies upon the transfer of information from health professionals to the general public using one-to-one or one to many communication (mass communication). The latest shift is towards health marketing.
",Health
"Community health is generally measured by geographical information systems and demographic data. Geographic information systems can be used to define sub-communities when neighborhood location data is not enough.[3] Traditionally community health has been measured using sampling data which was then compared to well-known data sets, like the National Health Interview Survey or National Health and Nutrition Examination Survey.[4] With technological development, information systems could store more data for small scale communities, cities, and towns; as opposed to census data that only generalizes information about small populations based on the overall population. Geographical information systems (GIS) can give more precise information of community resources, even at neighborhood levels.[5] The ease of use of geographic information systems (GIS), advances in multilevel statistics, and spatial analysis methods makes it easier for researchers to procure and generate data related to the built environment.[6]
",Health
"Social media can also play a big role in health information analytics.[7] Studies have found social media being capable of influencing people to change their unhealthy behaviors and encourage interventions capable of improving health status.[7]  Social media statistics combined with geographical information systems (GIS) may provide researchers with a more complete image of community standards for health and well being.[8][9]
",Health
"Community based health promotion emphasizes primary prevention and population based perspective(traditional prevention).[10]  It is the goal of community health to have individuals in a certain community improve their lifestyle or seek medical attention. Primary healthcare is provided by health professionals, specifically the ones a patient sees first that may refer them to secondary or tertiary care.
",Health
"Primary prevention refers to the early avoidance and identification of risk factors that may lead to certain diseases and disabilities. Community focused efforts including immunizations, classroom teaching, and awareness campaigns are all good examples of how primary prevention techniques are utilized by communities to change certain health behaviors. Prevention programs, if carefully designed and drafted, can effectively prevent problems that children and adolescents face as they grow up.[11]  This finding also applies to all groups and classes of people. Prevention programs are one of the most effective tools health professionals can use to greatly impact individual, population, and community health.[11]
",Health
"Community health can also be improved with improvements in individuals' environments. Community health status is determined by the environmental characteristics, behavioral characteristics, social cohesion in the environment of that community.[12]  Appropriate modifications in the environment can help to prevent unhealthy behaviors and negative health outcomes.
",Health
"Secondary prevention refers to improvements made in a patient's lifestyle or environment after the onset of disease or disability. This sort of prevention works to make life easier for the patient, since it's too late to prevent them from their current disease or disability. An example of secondary prevention is when those with occupational low back pain are provided with strategies to stop their health status from worsening; the prospects of secondary prevention may even hold more promise than primary prevention in this case.[13]
",Health
"Chronic diseases has been a growing phenomena within recent decades, affecting nearly 50% of adults within the US in 2012.[14] Such diseases include asthma, arthritis, diabetes, and hypertension. While they are not directly life-threatening, they place a significant burden on daily lives, affecting quality of life for the individual, their families, and the communities they live in, both socially and financially. Chronic diseases are responsible for an estimated 70% of healthcare expenditures within the US, spending nearly $650 billion per year.
",Health
"With steadily growing numbers, many community healthcare providers have developed self-management programs to assist patients in properly managing their own behavior as well as making adequate decisions about their lifestyle.[15] Separate from clinical patient care, these programs are facilitated to further educate patients about their health conditions as a means to adopt health-promoting behaviors into their own lifestyle.[16] Characteristics of these programs include:
",Health
"Chronic Disease self-management programs are structured to help improve overall patient health and quality of life as well as utilize less healthcare resources, such as physician visits and emergency care.[17] Furthermore, better self-monitoring skills can help patients effectively and efficiently make better use of healthcare professionals' time, which can result in better care.[18] Many self-management programs either are conducted through a health professional or a peer diagnosed with a certain chronic disease trained by health professionals to conduct the program. No significant differences have been reported comparing the effectiveness of both peer-led versus professional led self-management programs.[17]
",Health
"There has been a lot of debate regarding the effectiveness of these programs and how well they influence patient behavior and understanding their own health conditions. Some studies argue that self-management programs are effective in improving patient quality of life and decreasing healthcare expenditures and hospital visits. A 2001 study assessed health statuses through healthcare resource utilizations and self-management outcomes after 1 and 2 years to determine the effectiveness of chronic disease self-management programs. After analyzing 800 patients diagnosed with various types of chronic conditions, including heart disease, stroke, and arthritis, the study found that after the 2 years, there was a significant improvement in health status and fewer emergency department and physician visits (also significant after 1 year). They concluded that these low-cost self-management programs allowed for less healthcare utilization as well as an improvement in overall patient health.[19] Another study in 2003 by the National Institute for Health Research analyzed a 7-week chronic disease self-management program in its cost-effectiveness and health efficacy within a population over 18 years of age experiencing one or more chronic diseases. They observed similar patterns, such as an improvement in health status, reduced number of visits to the emergency department and to physicians, shorter hospital visits. They also noticed that after measuring unit costs for both hospital stays ($1000) and emergency department visits ($100), the study found the overall savings after the self-management program resulted in nearly $489 per person.[20] Lastly, a meta-analysis study in 2005 analyzed multiple chronic disease self-management programs focusing specifically on hypertension, osteoarthritis, and diabetes mellitus, comparing and contrasting different intervention groups. They concluded that self-management programs for both diabetes and hypertension produced clinically significant benefits to overall health.[15]
",Health
"On the other hand, there are a few studies measuring little significance of the effectiveness of chronic disease self-management programs. In the previous 2005 study in Australia, there was no clinical significance in the health benefits of osteoarthritis self-management programs and cost-effectiveness of all of these programs.[15] Furthermore, in a 2004 literature review analyzing the variability of chronic disease self-management education programs by disease and their overlapping similarities, researchers found ""small to moderate effects for selected chronic diseases,"" recommending further research being conducted.[16]
",Health
"Some programs are looking to integrate self-management programs into the traditional healthcare system, specifically primary care, as a way to incorporate behavioral improvements and decrease the increased patient visits with chronic diseases.[21] However, they have argued that severe limitations hinder these programs from acting its full potential. Possible limitations of chronic disease self-management education programs include the following:[18]
",Health
"In tertiary healthcare, community health can only be affected with professional medical care involving the entire population. Patients need to be referred to specialists and undergo advanced medical treatment. In some countries, there are more sub-specialties of medical professions than there are primary care specialists.[12] Health inequalities are directly related to social advantage and social resources.[12]
",Health
"The complexity of community health and its various problems can make it difficult for researchers to assess and identify solutions. Community-based participatory research (CBPR) is a unique alternative that combines community participation, inquiry, and action.[24] Community-based participatory research (CBPR) helps researchers address community issues with a broader lens and also works with the people in the community to find culturally sensitive, valid, and reliable methods and approaches.[24]
",Health
"Other issues involve access and cost of medical care. A great majority of the world does not have adequate health insurance.[25] In low-income countries, less than 40% of total health expenditures are paid for by the public/government.[25] Community health, even population health, is not encouraged as health sectors in developing countries are not able to link the national authorities with the local government and community action.[25]
",Health
"In the United States, the Affordable Care Act (ACA) changed the way community health centers operate and the policies that were in place, greatly influencing community health.[26] The ACA directly affected community health centers by increasing funding, expanding insurance coverage for Medicaid, reforming the Medicaid payment system, appropriating $1.5 billion to increase the workforce and promote training.[26] The impact, importance, and success of the Affordable Care Act is still being studied and will have a large impact on how ensuring health can affect community standards on health and also individual health.
",Health
"
",Health
"Population health has been defined as ""the health outcomes of a group of individuals, including the distribution of such outcomes within the group"".[1] It is an approach to health that aims to improve the health of an entire human population. This concept does not refer to animal or plant populations. It has been described as consisting of three components. These are ""health outcomes, patterns of health determinants, and policies and interventions"".[1]  A priority considered important in achieving the aim of Population Health is to reduce health inequities or disparities among different population groups due to, among other factors, the social determinants of health, SDOH. The SDOH include all the factors (social, environmental, cultural and physical) that the different populations are born into, grow up and function with throughout their lifetimes which potentially have a measurable impact on the health of human populations.[2] The Population Health concept represents a change in the focus from the individual-level, characteristic of most mainstream medicine. It also seeks to complement the classic efforts of public health agencies by addressing a broader range of factors shown to impact the health of different populations. The World Health Organization's Commission on Social Determinants of Health, reported in 2008, that the SDOH factors were responsible for the bulk of diseases and injuries and these were the major causes of health inequities in all countries.[3]  In the US, SDOH were estimated to account for 70% of avoidable mortality.[4]
",Health
"From a population health perspective, health has been defined not simply as a state free from disease but as ""the capacity of people to adapt to, respond to, or control life's challenges and changes"".[5] The World Health Organization (WHO) defined health in its broader sense in 1946 as ""a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity.""[6][7]
",Health
"Healthy People 2020 is a web site sponsored by the US Department of Health and Human Services, representing the cumulative effort of 34 years of interest by the Surgeon General's office and others. It identifies 42 topics considered social determinants of health and approximately 1200 specific goals considered to improve population health. It provides links to the current research available for selected topics and identifies and supports the need for community involvement considered essential to address these problems realistically.[8]
",Health
"Recently, human role has been encouraged by the influence of population growth there has been increasing interest from epidemiologists on the subject of economic inequality and its relation to the health of populations. There is a very robust correlation between socioeconomic status and health. This correlation suggests that it is not only the poor who tend to be sick when everyone else is healthy,  heart disease, ulcers, type 2 diabetes, rheumatoid arthritis, certain types of cancer, and premature aging. Despite the reality of the SES Gradient, there is debate as to its cause. A number of researchers (A. Leigh, C. Jencks, A. Clarkwest—see also Russell Sage working papers) see a definite link between economic status and mortality due to the greater economic resources of the better-off, but they find little correlation due to social status differences.
",Health
"Other researchers such as Richard G. Wilkinson, J. Lynch, and G.A. Kaplan have found that socioeconomic status strongly affects health even when controlling for economic resources and access to health care. Most famous for linking social status with health are the Whitehall studies—a series of studies conducted on civil servants in London. The studies found that, despite the fact that all civil servants in England have the same access to health care, there was a strong correlation between social status and health. The studies found that this relationship stayed strong even when controlling for health-affecting habits such as exercise, smoking and drinking. Furthermore, it has been noted that no amount of medical attention will help decrease the likelihood of someone getting type 1 diabetes or rheumatoid arthritis—yet both are more common among populations with lower socioeconomic status. Lastly, it has been found that amongst the wealthiest quarter of countries on earth (a set stretching from Luxembourg to Slovakia) there is no relation between a country's wealth and general population health[1]—suggesting that past a certain level, absolute levels of wealth have little impact on population health, but relative levels within a country do.
The concept of psychosocial stress attempts to explain how psychosocial phenomenon such as status and social stratification can lead to the many diseases associated with the SES gradient. Higher levels of economic inequality tend to intensify social hierarchies and generally degrades the quality of social relations—leading to greater levels of stress and stress related diseases. Richard Wilkinson found this to be true not only for the poorest members of society, but also for the wealthiest. Economic inequality is bad for everyone's health. 
Inequality does not only affect the health of human populations. David H. Abbott at the Wisconsin National Primate Research Center found that among many primate species, less egalitarian social structures correlated with higher levels of stress hormones among socially subordinate individuals. Research by Robert Sapolsky of Stanford University provides similar findings.
",Health
"There is well-documented variation in health outcomes and health care utilization & costs by geographic variation in the U.S., down to the level of Hospital Referral Regions (defined as a regional health care market, which may cross state boundaries, of which there are 306 in the U.S.).[9][10] There is ongoing debate as to the relative contributions of race, gender, poverty, education level and place to these variations. The Office of Epidemiology of the Maternal and Child Health Bureau recommends using an analytic approach (Fixed Effects or hybrid Fixed Effects) to research on health disparities to reduce the confounding effects of neighborhood (geographic) variables on the outcomes.[11]
",Health
"Family planning programs (including contraceptives, sexuality education, and promotion of safe sex) play a major role in population health. Family planning is one of the most highly cost-effective interventions in medicine.[12]  Family planning saves lives and money by reducing unintended pregnancy and the transmission of sexually transmitted infections.[12]
",Health
"For example, the United States Agency for International Development lists as benefits of its international family planning program:[13]
",Health
"One method to improve population health is population health management (PHM), which has been defined as ""the technical field of endeavor which utilizes a variety of individual, organizational and cultural interventions to help improve the morbidity patterns (i.e., the illness and injury burden) and the health care use behavior of defined populations"".[14] PHM is distinguished from disease management by including more chronic conditions and diseases, by use of ""a single point of contact and coordination"", and by ""predictive modeling across multiple clinical conditions"".[15] PHM is considered broader than disease management in that it also includes ""intensive care management for individuals at the highest level of risk"" and ""personal health management... for those at lower levels of predicted health risk"".[16] Many PHM-related articles are published in Population Health Management, the official journal of DMAA: The Care Continuum Alliance.[17]
",Health
"The following road map has been suggested for helping healthcare organizations navigate the path toward implementing effective population health management:[18]
",Health
"Healthcare reform is driving change to traditional hospital reimbursement models. Prior to the introduction of the Patient Protection and Affordable Care Act (PPACA), hospitals were reimbursed based on the volume of procedures through fee-for-service models. Under the PPACA, reimbursement models are shifting from volume to value. New reimbursement models are built around pay for performance, a value-based reimbursement approach, which places financial incentives around patient outcomes and has drastically changed the way US hospitals must conduct business to remain financially viable.[19] In addition to focusing on improving patient experience of care and reducing costs, hospitals must also focus on improving the health of populations (IHI Triple Aim[20]).
",Health
"As participation in value-based reimbursement models such as accountable care organizations (ACOs) increases, these initiatives will help drive population health.[21] Within the ACO model, hospitals have to meet specific quality benchmarks, focus on prevention, and carefully manage patients with chronic diseases.[22] Providers get paid more for keeping their patients healthy and out of the hospital.[22] Studies have shown that inpatient admission rates have dropped over the past ten years in communities that were early adopters of the ACO model and implemented population health measures to treat ""less sick"" patients in the outpatient setting.[23] A study conducted in the Chicago area showed a decline in inpatient utilization rates across all age groups, which was an average of a 5% overall drop in inpatient admissions.[24]
",Health
"Hospitals are finding it financially advantageous to focus on population health management and keeping people in the community well.[25] The goal of population health management is to improve patient outcomes and increase health capital. Other goals include preventing disease, closing care gaps, and cost savings for providers.[26] In the last few years, more effort has been directed towards developing telehealth services, community-based clinics in areas with high proportion of residents using the emergency department as primary care, and patient care coordinator roles to coordinate healthcare services across the care continuum.[25]
",Health
"Health can be considered a capital good; health capital is part of human capital as defined by the Grossman model.[27] Health can be considered both an investment good and consumption good.[28] Factors such as obesity and smoking have negative effects on health capital, while education, wage rate, and age may also impact health capital.[28] When people are healthier through preventative care, they have the potential to live a longer and healthier life, work more and participate in the economy, and produce more based on the work done. These factors all have the potential to increase earnings. Some states, like New York, have implemented statewide initiatives to address population health. In New York state there are 11 such programs. One example is the Mohawk Valley Population Health Improvement Program (http://www.mvphip.org/). These programs work to address the needs of the people in their region, as well as assist their local community based organizations and social services to gather data, address health disparities, and explore evidence-based interventions that will ultimately lead to better health for everyone. Following a similar approach, Cullati et al. developed a theoretical framework for the development and onset of vulnerability in later life based on the concept of ""reserves"".[29] The advantages to use the concept of reserves in interdisciplinary studies, as compared with related concepts such as resources and capital, is to strengthen the importance of constitution and sustainability of reserves (the “use it or lose it” paradigm) and the presence of thresholds, below which functioning becomes challenging. 
",Health
